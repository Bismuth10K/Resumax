(155.95999145507812, 54.62837219238281, 456.0304870605469, 113.63127899169922, 'Summary Evaluation\nwith and without References\n', 0, 0)
(58.90699005126953, 118.60984802246094, 549.2501220703125, 131.8694610595703, 'Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Vel´azquez-Morales\n', 1, 0)
(48.95896911621094, 165.23977661132812, 300.01812744140625, 296.492431640625, 'Abstract—We\nstudy\na\nnew\ncontent-based\nmethod\nfor\nthe\nevaluation\nof\ntext\nsummarization\nsystems\nwithout\nhuman models which is used to produce system rankings.\nThe\nresearch\nis\ncarried\nout\nusing\na\nnew\ncontent-based\nevaluation framework called FRESA to compute a variety of\ndivergences among probability distributions. We apply our\ncomparison framework to various well-established content-based\nevaluation measures in text summarization such as COVERAGE,\nRESPONSIVENESS,\nPYRAMIDS\nand\nROUGE\nstudying\ntheir\nassociations in various text summarization tasks including\ngeneric multi-document summarization in English and French,\nfocus-based\nmulti-document\nsummarization\nin\nEnglish\nand\ngeneric single-document summarization in French and Spanish.\n', 2, 0)
(48.95896911621094, 300.2418212890625, 300.00830078125, 321.90545654296875, 'Index Terms—Text summarization evaluation, content-based\nevaluation measures, divergences.\n', 3, 0)
(48.958953857421875, 336.9942321777344, 300.0267028808594, 579.1834716796875, 'I. INTRODUCTION\nT\nEXT summarization evaluation has always been a\ncomplex\nand\ncontroversial\nissue\nin\ncomputational\nlinguistics. In the last decade, signiﬁcant advances have been\nmade in this ﬁeld as well as various evaluation measures have\nbeen designed. Two evaluation campaigns have been led by\nthe U.S. agence DARPA. The ﬁrst one, SUMMAC, ran from\n1996 to 1998 under the auspices of the Tipster program [1],\nand the second one, entitled DUC (Document Understanding\nConference) [2], was the main evaluation forum from 2000\nuntil 2007. Nowadays, the Text Analysis Conference (TAC)\n[3] provides a forum for assessment of different information\naccess technologies including text summarization.\nEvaluation in text summarization can be extrinsic or\nintrinsic [4]. In an extrinsic evaluation, the summaries are\nassessed in the context of an speciﬁc task carried out by a\nhuman or a machine. In an intrinsic evaluation, the summaries\nare evaluated in reference to some ideal model. SUMMAC\nwas mainly extrinsic while DUC and TAC followed an\nintrinsic evaluation paradigm. In an intrinsic evaluation, an\n', 4, 0)
(48.95893859863281, 587.9385986328125, 300.0180358886719, 714.1055297851562, 'Manuscript received June 8, 2010. Manuscript accepted for publication July\n25, 2010.\nJuan-Manuel\nTorres-Moreno\nis\nwith\nLIA/Universit´e\nd’Avignon,\nFrance\nand\n´Ecole\nPolytechnique\nde\nMontr´eal,\nCanada\n(juan-manuel.torres@univ-avignon.fr).\nEric\nSanJuan\nis\nwith\nLIA/Universit´e\nd’Avignon,\nFrance\n(eric.sanjuan@univ-avignon.fr).\nHoracio\nSaggion\nis\nwith\nDTIC/Universitat\nPompeu\nFabra,\nSpain\n(horacio.saggion@upf.edu).\nIria\nda\nCunha\nis\nwith\nIULA/Universitat\nPompeu\nFabra,\nSpain;\nLIA/Universit´e d’Avignon, France and Instituto de Ingenier´ıa/UNAM, Mexico\n(iria.dacunha@upf.edu).\nPatricia\nVel´azquez-Morales\nis\nwith\nVM\nLabs,\nFrance\n(patricia velazquez@yahoo.com).\n', 5, 0)
(311.9728088378906, 164.67724609375, 563.0427856445312, 714.66552734375, 'automatically generated summary (peer) has to be compared\nwith one or more reference summaries (models). DUC used\nan interface called SEE to allow human judges to compare\na peer with a model. Thus, judges give a COVERAGE score\nto each peer produced by a system and the ﬁnal system\nCOVERAGE score is the average of the COVERAGE’s scores\nasigned. These system’s COVERAGE scores can then be used\nto rank summarization systems. In the case of query-focused\nsummarization (e.g. when the summary should answer a\nquestion or series of questions) a RESPONSIVENESS score\nis also assigned to each summary, which indicates how\nresponsive the summary is to the question(s).\nBecause manual comparison of peer summaries with model\nsummaries is an arduous and costly process, a body of\nresearch has been produced in the last decade on automatic\ncontent-based evaluation procedures. Early studies used text\nsimilarity measures such as cosine similarity (with or without\nweighting schema) to compare peer and model summaries\n[5]. Various vocabulary overlap measures such as n-grams\noverlap or longest common subsequence between peer and\nmodel have also been proposed [6], [7]. The BLEU machine\ntranslation evaluation measure [8] has also been tested in\nsummarization [9]. The DUC conferences adopted the ROUGE\npackage for content-based evaluation [10]. ROUGE implements\na series of recall measures based on n-gram co-occurrence\nbetween a peer summary and a set of model summaries. These\nmeasures are used to produce systems’ rank. It has been shown\nthat system rankings, produced by some ROUGE measures\n(e.g., ROUGE-2, which uses 2-grams), have a correlation with\nrankings produced using COVERAGE.\nIn recent years the PYRAMIDS evaluation method [11] has\nbeen introduced. It is based on the distribution of “content”\nof a set of model summaries. Summary Content Units (SCUs)\nare ﬁrst identiﬁed in the model summaries, then each SCU\nreceives a weight which is the number of models containing\nor expressing the same unit. Peer SCUs are identiﬁed in the\npeer, matched against model SCUs, and weighted accordingly.\nThe PYRAMIDS score given to a peer is the ratio of the sum\nof the weights of its units and the sum of the weights of the\nbest possible ideal summary with the same number of SCUs as\nthe peer. The PYRAMIDS scores can be also used for ranking\nsummarization systems. [11] showed that PYRAMIDS scores\nproduced reliable system rankings when multiple (4 or more)\nmodels were used and that PYRAMIDS rankings correlate with\nrankings produced by ROUGE-2 and ROUGE-SU2 (i.e. ROUGE\nwith skip 2-grams). However, this method requires the creation\n', 6, 0)
(302.0, 762.5703125, 565.2230224609375, 771.4296875, '13\nPolibits (42) 2010\n', 7, 0)
