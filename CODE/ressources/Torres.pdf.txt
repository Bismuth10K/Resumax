(155.95999145507812, 54.62837219238281, 456.0304870605469, 113.63127899169922, 'Summary Evaluation\nwith and without References\n', 0, 0)
(58.90699005126953, 118.60984802246094, 549.2501220703125, 131.8694610595703, 'Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Vel´azquez-Morales\n', 1, 0)
(48.95896911621094, 165.23977661132812, 300.01812744140625, 296.492431640625, 'Abstract—We\nstudy\na\nnew\ncontent-based\nmethod\nfor\nthe\nevaluation\nof\ntext\nsummarization\nsystems\nwithout\nhuman models which is used to produce system rankings.\nThe\nresearch\nis\ncarried\nout\nusing\na\nnew\ncontent-based\nevaluation framework called FRESA to compute a variety of\ndivergences among probability distributions. We apply our\ncomparison framework to various well-established content-based\nevaluation measures in text summarization such as COVERAGE,\nRESPONSIVENESS,\nPYRAMIDS\nand\nROUGE\nstudying\ntheir\nassociations in various text summarization tasks including\ngeneric multi-document summarization in English and French,\nfocus-based\nmulti-document\nsummarization\nin\nEnglish\nand\ngeneric single-document summarization in French and Spanish.\n', 2, 0)
(48.95896911621094, 300.2418212890625, 300.00830078125, 321.90545654296875, 'Index Terms—Text summarization evaluation, content-based\nevaluation measures, divergences.\n', 3, 0)
(48.958953857421875, 336.9942321777344, 300.0267028808594, 579.1834716796875, 'I. INTRODUCTION\nT\nEXT summarization evaluation has always been a\ncomplex\nand\ncontroversial\nissue\nin\ncomputational\nlinguistics. In the last decade, signiﬁcant advances have been\nmade in this ﬁeld as well as various evaluation measures have\nbeen designed. Two evaluation campaigns have been led by\nthe U.S. agence DARPA. The ﬁrst one, SUMMAC, ran from\n1996 to 1998 under the auspices of the Tipster program [1],\nand the second one, entitled DUC (Document Understanding\nConference) [2], was the main evaluation forum from 2000\nuntil 2007. Nowadays, the Text Analysis Conference (TAC)\n[3] provides a forum for assessment of different information\naccess technologies including text summarization.\nEvaluation in text summarization can be extrinsic or\nintrinsic [4]. In an extrinsic evaluation, the summaries are\nassessed in the context of an speciﬁc task carried out by a\nhuman or a machine. In an intrinsic evaluation, the summaries\nare evaluated in reference to some ideal model. SUMMAC\nwas mainly extrinsic while DUC and TAC followed an\nintrinsic evaluation paradigm. In an intrinsic evaluation, an\n', 4, 0)
(48.95893859863281, 587.9385986328125, 300.0180358886719, 714.1055297851562, 'Manuscript received June 8, 2010. Manuscript accepted for publication July\n25, 2010.\nJuan-Manuel\nTorres-Moreno\nis\nwith\nLIA/Universit´e\nd’Avignon,\nFrance\nand\n´Ecole\nPolytechnique\nde\nMontr´eal,\nCanada\n(juan-manuel.torres@univ-avignon.fr).\nEric\nSanJuan\nis\nwith\nLIA/Universit´e\nd’Avignon,\nFrance\n(eric.sanjuan@univ-avignon.fr).\nHoracio\nSaggion\nis\nwith\nDTIC/Universitat\nPompeu\nFabra,\nSpain\n(horacio.saggion@upf.edu).\nIria\nda\nCunha\nis\nwith\nIULA/Universitat\nPompeu\nFabra,\nSpain;\nLIA/Universit´e d’Avignon, France and Instituto de Ingenier´ıa/UNAM, Mexico\n(iria.dacunha@upf.edu).\nPatricia\nVel´azquez-Morales\nis\nwith\nVM\nLabs,\nFrance\n(patricia velazquez@yahoo.com).\n', 5, 0)
(311.9728088378906, 164.67724609375, 563.0427856445312, 714.66552734375, 'automatically generated summary (peer) has to be compared\nwith one or more reference summaries (models). DUC used\nan interface called SEE to allow human judges to compare\na peer with a model. Thus, judges give a COVERAGE score\nto each peer produced by a system and the ﬁnal system\nCOVERAGE score is the average of the COVERAGE’s scores\nasigned. These system’s COVERAGE scores can then be used\nto rank summarization systems. In the case of query-focused\nsummarization (e.g. when the summary should answer a\nquestion or series of questions) a RESPONSIVENESS score\nis also assigned to each summary, which indicates how\nresponsive the summary is to the question(s).\nBecause manual comparison of peer summaries with model\nsummaries is an arduous and costly process, a body of\nresearch has been produced in the last decade on automatic\ncontent-based evaluation procedures. Early studies used text\nsimilarity measures such as cosine similarity (with or without\nweighting schema) to compare peer and model summaries\n[5]. Various vocabulary overlap measures such as n-grams\noverlap or longest common subsequence between peer and\nmodel have also been proposed [6], [7]. The BLEU machine\ntranslation evaluation measure [8] has also been tested in\nsummarization [9]. The DUC conferences adopted the ROUGE\npackage for content-based evaluation [10]. ROUGE implements\na series of recall measures based on n-gram co-occurrence\nbetween a peer summary and a set of model summaries. These\nmeasures are used to produce systems’ rank. It has been shown\nthat system rankings, produced by some ROUGE measures\n(e.g., ROUGE-2, which uses 2-grams), have a correlation with\nrankings produced using COVERAGE.\nIn recent years the PYRAMIDS evaluation method [11] has\nbeen introduced. It is based on the distribution of “content”\nof a set of model summaries. Summary Content Units (SCUs)\nare ﬁrst identiﬁed in the model summaries, then each SCU\nreceives a weight which is the number of models containing\nor expressing the same unit. Peer SCUs are identiﬁed in the\npeer, matched against model SCUs, and weighted accordingly.\nThe PYRAMIDS score given to a peer is the ratio of the sum\nof the weights of its units and the sum of the weights of the\nbest possible ideal summary with the same number of SCUs as\nthe peer. The PYRAMIDS scores can be also used for ranking\nsummarization systems. [11] showed that PYRAMIDS scores\nproduced reliable system rankings when multiple (4 or more)\nmodels were used and that PYRAMIDS rankings correlate with\nrankings produced by ROUGE-2 and ROUGE-SU2 (i.e. ROUGE\nwith skip 2-grams). However, this method requires the creation\n', 6, 0)
(302.0, 762.5703125, 565.2230224609375, 771.4296875, '13\nPolibits (42) 2010\n', 7, 0)

page suivante
(48.958984375, 55.88720703125, 300.0271301269531, 485.8086242675781, 'of models and the identiﬁcation, matching, and weighting of\nSCUs in both: models and peers.\n[12] evaluated the effectiveness of the Jensen-Shannon\n(J S) [13] theoretic measure in predicting systems ranks\nin\ntwo\nsummarization\ntasks:\nquery-focused\nand\nupdate\nsummarization.\nThey\nhave\nshown\nthat\nranks\nproduced\nby\nPYRAMIDS\nand\nthose\nproduced\nby\nJ S\nmeasure\ncorrelate.\nHowever,\nthey\ndid\nnot\ninvestigate\nthe\neffect\nof the measure in summarization tasks such as generic\nmulti-document\nsummarization\n(DUC\n2004\nTask\n2),\nbiographical summarization (DUC 2004 Task 5), opinion\nsummarization\n(TAC\n2008\nOS),\nand\nsummarization\nin\nlanguages other than English.\nIn this paper we present a series of experiments aimed at\na better understanding of the value of the J S divergence\nfor ranking summarization systems. We have carried out\nexperimentation with the proposed measure and we have\nveriﬁed that in certain tasks (such as those studied by\n[12])\nthere\nis\na\nstrong\ncorrelation\namong\nPYRAMIDS,\nRESPONSIVENESS and the J S divergence, but as we will\nshow in this paper, there are datasets in which the correlation\nis not so strong. We also present experiments in Spanish\nand French showing positive correlation between the J S\nand ROUGE which is the de facto evaluation measure used\nin evaluation of non-English summarization. To the best of\nour knowledge this is the more extensive set of experiments\ninterpreting the value of evaluation without human models.\nThe rest of the paper is organized in the following way:\nFirst in Section II we introduce related work in the area of\ncontent-based evaluation identifying the departing point for\nour inquiry; then in Section III we explain the methodology\nadopted in our work and the tools and resources used for\nexperimentation. In Section IV we present the experiments\ncarried out together with the results. Section V discusses the\nresults and Section VI concludes the paper and identiﬁes future\nwork.\n', 0, 0)
(48.95899200439453, 496.45623779296875, 300.0286560058594, 714.6656494140625, 'II. RELATED WORK\nOne of the ﬁrst works to use content-based measures in\ntext summarization evaluation is due to [5], who presented an\nevaluation framework to compare rankings of summarization\nsystems produced by recall and cosine-based measures. They\nshowed that there was weak correlation among rankings\nproduced by recall, but that content-based measures produce\nrankings which were strongly correlated. This put forward\nthe idea of using directly the full document for comparison\npurposes in text summarization evaluation. [6] presented a\nset of evaluation measures based on the notion of vocabulary\noverlap including n-gram overlap, cosine similarity, and\nlongest common subsequence, and they applied them to\nmulti-document\nsummarization\nin\nEnglish\nand\nChinese.\nHowever, they did not evaluate the performance of the\nmeasures in different summarization tasks. [7] also compared\nvarious evaluation measures based on vocabulary overlap.\nAlthough these measures were able to separate random from\n', 1, 0)
(311.9729919433594, 55.88824462890625, 563.0430908203125, 271.8778381347656, 'non-random systems, no clear conclusion was reached on the\nvalue of each of the studied measures.\nNowadays,\na\nwidespread\nsummarization\nevaluation\nframework is ROUGE [14], which offers a set of statistics\nthat\ncompare\npeer\nsummaries\nwith\nmodels.\nIt\ncounts\nco-occurrences of n-grams in peer and models to derive a\nscore. There are several statistics depending on the used\nn-grams and the text processing applied to the input texts\n(e.g., lemmatization, stop-word removal).\n[15] proposed a method of evaluation based on the\nuse of “distances” or divergences between two probability\ndistributions (the distribution of units in the automatic\nsummary\nand\nthe\ndistribution\nof\nunits\nin\nthe\nmodel\nsummary). They studied two different Information Theoretic\nmeasures of divergence: the Kullback-Leibler (KL) [16] and\nJensen-Shannon (J S) [13] divergences. KL computes the\ndivergence between probability distributions P and Q in the\nfollowing way:\n', 2, 0)
(368.56103515625, 278.9549865722656, 437.3165283203125, 302.7317199707031, 'DKL(P||Q) = 1\n', 3, 0)
(432.3349914550781, 292.5277404785156, 437.3164978027344, 302.4907531738281, '2\n', 4, 0)
(440.1730041503906, 276.01055908203125, 454.5595703125, 313.1924743652344, 'X\n', 5, 0)
(444.3919982910156, 278.9547424316406, 563.0319213867188, 306.3664855957031, 'w\nPw log2\nPw\nQw\n(1)\n', 6, 0)
(311.9730224609375, 311.6061706542969, 492.4314880371094, 330.37646484375, 'While J S divergence is deﬁned as follows:\n', 7, 0)
(311.9730224609375, 330.687744140625, 381.2715148925781, 354.4644775390625, 'DJ S(P||Q) = 1\n', 8, 0)
(376.2900085449219, 344.2617492675781, 381.2715148925781, 354.2247619628906, '2\n', 9, 0)
(384.1280212402344, 327.74456787109375, 398.51458740234375, 364.9264831542969, 'X\n', 10, 0)
(388.3470153808594, 330.687744140625, 462.7765197753906, 358.1004943847656, 'w\nPw log2\n2Pw\n', 11, 0)
(434.87799072265625, 330.687744140625, 551.12744140625, 355.71875, 'Pw + Qw\n+ Qw log2\n2Qw\n', 12, 0)
(311.9730224609375, 344.2617492675781, 563.0410766601562, 547.021484375, 'Pw + Qw\n(2)\nThese measures can be applied to the distribution of units in\nsystem summaries P and reference summaries Q. The value\nobtained may be used as a score for the system summary. The\nmethod has been tested by [15] over the DUC 2002 corpus for\nsingle and multi-document summarization tasks showing good\ncorrelation among divergence measures and both coverage and\nROUGE rankings.\n[12] went even further and, as in [5], they proposed to\ncompare directly the distribution of words in full documents\nwith the distribution of words in automatic summaries to\nderive a content-based evaluation measure. They found a\nhigh correlation between rankings produced using models\nand rankings produced without models. This last work is the\ndeparting point for our inquiry into the value of measures that\ndo not rely on human models.\n', 13, 0)
(393.4630126953125, 563.5940551757812, 481.5476989746094, 575.5994873046875, 'III. METHODOLOGY\n', 14, 0)
(311.9728088378906, 580.9281005859375, 563.0409545898438, 688.5745239257812, 'The followed methodology in this paper mirrors the one\nadopted in past work (e.g. [5], [7], [12]). Given a particular\nsummarization task T, p data points to be summarized\nwith input material {Ii}p−1\ni=0 (e.g. document(s), question(s),\ntopic(s)), s peer summaries {SUMi,k}s−1\nm model summaries {MODELi,j}m−1k=0 for input i, and\nj=0\nfor input i, we will\ncompare rankings of the s peer summaries produced by various\nevaluation measures. Some measures that we use compare\nsummaries with n of the m models:\n', 15, 0)
(352.16290283203125, 702.0022583007812, 563.03173828125, 721.2823486328125, 'MEASUREM(SUMi,k, {MODELi,j}n−1\nj=0 )\n(3)\n', 16, 0)
(46.77693176269531, 20.7718505859375, 397.5782470703125, 29.6312255859375, 'Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales\n', 17, 0)
(46.77693176269531, 762.5703125, 310.0, 771.4296875, '14\nPolibits (42) 2010\n', 18, 0)

page suivante
(48.95899963378906, 55.88720703125, 300.02667236328125, 79.84861755371094, 'while other measures compare peers with all or some of the\ninput material:\n', 0, 0)
(120.36199951171875, 86.90436553955078, 300.0188903808594, 100.67752075195312, 'MEASUREM(SUMi,k, I′\ni)\n(4)\n', 1, 0)
(48.959014892578125, 106.855224609375, 300.0276794433594, 322.0987548828125, 'where I′\ni is some subset of input Ii. The values produced\nby the measures for each summary SUMi,k are averaged\nfor each system k = 0, . . . , s − 1 and these averages are\nused to produce a ranking. Rankings are then compared\nusing Spearman Rank correlation [17] which is used to\nmeasure the degree of association between two variables\nwhose values are used to rank objects. We have chosen\nto use this correlation to compare directly results to those\npresented in [12]. Computation of correlations is done using\nthe Statistics-RankCorrelation-0.12 package1, which computes\nthe rank correlation between two vectors. We also veriﬁed\nthe good conformity of the results with the correlation test\nof Kendall τ calculated with the statistical software R. The\ntwo nonparametric tests of Spearman and Kendall do not\nreally stand out as the treatment of ex-æquo. The good\ncorrespondence between the two tests shows that they do not\nintroduce bias in our analysis. Subsequently will mention only\nthe ρ of Sperman more widely used in this ﬁeld.\n', 2, 0)
(48.95903396606445, 339.16033935546875, 83.74983215332031, 351.05615234375, 'A. Tools\n', 3, 0)
(48.959014892578125, 354.8383483886719, 300.028076171875, 498.3506164550781, 'We carry out experimentation using a new summarization\nevaluation framework: FRESA –FRamework for Evaluating\nSummaries Automatically–, which includes document-based\nsummary\nevaluation\nmeasures\nbased\non\nprobabilities\ndistribution2. As in the ROUGE package, FRESA supports\ndifferent n-grams and skip n-grams probability distributions.\nThe FRESA environment can be used in the evaluation of\nsummaries in English, French, Spanish and Catalan, and it\nintegrates ﬁltering and lemmatization in the treatment of\nsummaries and documents. It is developed in Perl and will\nbe made publicly available. We also use the ROUGE package\n[10] to compute various ROUGE statistics in new datasets.\n', 4, 0)
(48.959014892578125, 515.4122314453125, 209.50283813476562, 527.3080444335938, 'B. Summarization Tasks and Data Sets\n', 5, 0)
(48.959014892578125, 531.0902099609375, 300.0267333984375, 664.9926147460938, 'We have conducted our experimentation with the following\nsummarization tasks and data sets:\n1) Generic\nmulti-document-summarization\nin\nEnglish\n(production of a short summary of a cluster of related\ndocuments) using data from DUC’043, task 2: 50\nclusters, 10 documents each – 294,636 words.\n2) Focused-based summarization in English (production of\na short focused multi-document summary focused on the\nquestion “who is X?”, where X is a person’s name) using\ndata from the DUC’04 task 5: 50 clusters, 10 documents\neach plus a target person name – 284,440 words.\n', 6, 0)
(48.959007263183594, 674.8373413085938, 300.0124816894531, 714.1055908203125, '1http://search.cpan.org/∼gene/Statistics-RankCorrelation-0.12/\n2FRESA is available at: http://lia.univavignon.fr/ﬁleadmin/axes/TALNE/\nRessources.html\n3http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html\n', 7, 0)
(321.9359436035156, 55.88824462890625, 563.0408935546875, 414.5937194824219, '3) Update-summarization task that consists of creating a\nsummary out of a cluster of documents and a topic. Two\nsub-tasks are considered here: A) an initial summary has\nto be produced based on an initial set of documents and\ntopic; B) an update summary has to be produced from\na different (but related) cluster assuming documents\nused in A) are known. The English TAC’08 Update\nSummarization dataset is used, which consists of 48\ntopics with 20 documents each – 36,911 words.\n4) Opinion summarization where systems have to analyze\na set of blog articles and summarize the opinions\nabout a target in the articles. The TAC’08 Opinion\nSummarization in English4 data set (taken from the\nBlogs06 Text Collection) is used: 25 clusters and targets\n(i.e., target entity and questions) were used – 1,167,735\nwords.\n5) Generic single-document summarization in Spanish\nusing the Medicina Cl´ınica5 corpus, which is composed\nof 50 medical articles in Spanish, each one with its\ncorresponding author abstract – 124,929 words.\n6) Generic single document summarization in French using\nthe “Canadien French Sociological Articles” corpus\nfrom the journal Perspectives interdisciplinaires sur le\ntravail et la sant´e (PISTES)6. It contains 50 sociological\narticles in French, each one with its corresponding\nauthor abstract – 381,039 words.\n7) Generic multi-document-summarization in French using\ndata from the RPM27 corpus [18], 20 different themes\nconsisting of 10 articles and 4 abstracts by reference\nthematic – 185,223 words.\n', 8, 0)
(311.9729309082031, 418.1493225097656, 563.0409545898438, 489.9306640625, 'For experimentation in the TAC and the DUC datasets we use\ndirectly the peer summaries produced by systems participating\nin the evaluations. For experimentation in Spanish and French\n(single and multi-document summarization) we have created\nsummaries at a similar ratio to those of reference using the\nfollowing systems:\n', 9, 0)
(321.9359436035156, 493.4872741699219, 563.0406494140625, 660.90966796875, '– ENERTEX [19], a summarizer based on a theory of\ntextual energy;\n– CORTEX [20], a single-document sentence extraction\nsystem for Spanish and French that combines various\nstatistical measures of relevance (angle between sentence\nand topic, various Hamming weights for sentences, etc.)\nand applies an optimal decision algorithm for sentence\nselection;\n– SUMMTERM [21], a terminology-based summarizer that\nis used for summarization of medical articles and\nuses specialized terminology for scoring and ranking\nsentences;\n– REG [22], summarization system based on an greedy\nalgorithm;\n', 10, 0)
(319.9429626464844, 673.9403686523438, 528.3231201171875, 714.1055908203125, '4http://www.nist.gov/tac/data/index.html\n5http://www.elsevier.es/revistas/ctl servlet? f=7032&revistaid=2\n6http://www.pistes.uqam.ca/\n7http://www-labs.sinequa.com/rpm2\n', 11, 0)
(403.68701171875, 20.7718505859375, 565.2243041992188, 29.6312255859375, 'Summary Evaluation with and without References\n', 12, 0)
(302.0, 762.5703125, 565.2230224609375, 771.4296875, '15\nPolibits (42) 2010\n', 13, 0)

page suivante
(58.9219970703125, 55.88720703125, 300.0283508300781, 175.48973083496094, '– J S summarizer, a summarization system that scores\nand ranks sentences according to their Jensen-Shannon\ndivergence to the source document;\n– a lead-based summarization system that selects the lead\nsentences of the document;\n– a\nrandom-based\nsummarization\nsystem\nthat\nselects\nsentences at random;\n– Open Text Summarizer [23], a multi-lingual summarizer\nbased on the frequency and\n– commercial systems: Word, SSSummarizer8, Pertinence9\n', 0, 0)
(69.88102722167969, 174.586181640625, 134.79779052734375, 187.4447479248047, 'and Copernic10.\n', 1, 0)
(48.95903778076172, 203.6072998046875, 148.6587677001953, 215.50311279296875, 'C. Evaluation Measures\n', 2, 0)
(48.95903778076172, 218.91632080078125, 300.0267639160156, 560.0775756835938, 'The following measures derived from human assessment of\nthe content of the summaries are used in our experiments:\n– COVERAGE is understood as the degree to which one\npeer summary conveys the same information as a model\nsummary [2]. COVERAGE was used in DUC evaluations.\nThis measure is used as indicated in equation 3 using\nhuman references or models.\n– RESPONSIVENESS ranks summaries in a 5-point scale\nindicating how well the summary satisﬁed a given\ninformation need [2]. It is used in focused-based\nsummarization tasks. This measure is used as indicated\nin equation 4 since a human judges the summary\nwith respect to a given input “user need” (e.g., a\nquestion). RESPONSIVENESS was used in DUC and TAC\nevaluations.\n– PYRAMIDS [11] is a content assessment measure which\ncompares content units in a peer summary to weighted\ncontent units in a set of model summaries. This\nmeasure is used as indicated in equation 3 using human\nreferences or models. PYRAMIDS is the adopted metric\nfor content-based evaluation in the TAC evaluations.\nFor DUC and TAC datasets the values of these measures are\navailable and we used them directly. We used the following\nautomatic evaluation measures in our experiments:\n– ROUGE [14], which is a recall metric that takes into\naccount n-grams as units of content for comparing peer\nand model summaries. The ROUGE formula speciﬁed in\n[10] is as follows:\n', 3, 0)
(79.06405639648438, 581.0411376953125, 270.90167236328125, 632.6884155273438, 'ROUGE-n(R, M) =\nP\nm ∈ M P\nn−gram∈P countmatch(n − gram)\n', 4, 0)
(118.79100036621094, 602.3851928710938, 300.0188903808594, 648.553466796875, 'P\nm ∈ M P count(n-gram)\n(5)\n', 5, 0)
(69.88101196289062, 626.6511840820312, 300.0302429199219, 674.5226440429688, 'where R is the summary to be evaluated, M is the set of\nmodel (human) summaries, countmatch is the number of\ncommon n-grams in m and P, and count is the number\nof n-grams in the model summaries. For the experiments\n', 6, 0)
(56.930015563964844, 683.8033447265625, 223.5940399169922, 714.1055908203125, '8http://www.kryltech.com/summarizer.htm\n9http://www.pertinence.net\n10http://www.copernic.com/en/products/summarizer\n', 7, 0)
(321.93603515625, 55.88720703125, 563.0409545898438, 163.5347137451172, 'presented here we used uni-grams, 2-grams, and the skip\n2-grams with maximum skip distance of 4 (ROUGE-1,\nROUGE-2 and ROUGE-SU4). ROUGE is used to compare\na peer summary to a set of model summaries in our\nframework (as indicated in equation 3).\n– Jensen-Shannon divergence formula given in Equation 2\nis implemented in our FRESA package with the following\nspeciﬁcation (Equation 6) for the probability distribution\nof words w.\n', 8, 0)
(469.76300048828125, 166.45663452148438, 509.967529296875, 191.36672973632812, 'Pw = CT\nw\nN\n', 9, 0)
(383.5350036621094, 202.7257537841797, 408.3692626953125, 214.18276977539062, 'Qw =\n', 10, 0)
(411.1440124511719, 185.47055053710938, 452.20281982421875, 222.6524658203125, '(\nCS\nw\n', 11, 0)
(441.4859924316406, 194.15020751953125, 505.6383361816406, 212.92050170898438, 'NS\nif w ∈ S\n', 12, 0)
(428.239990234375, 207.46127319335938, 449.6864929199219, 217.44129943847656, 'CT\nw+δ\n', 13, 0)
(425.34600830078125, 200.9921875, 563.0319213867188, 229.3522491455078, 'N+δ∗B\notherwise\n(6)\n', 14, 0)
(332.89501953125, 227.19921875, 563.0407104492188, 478.3083801269531, 'Where P is the probability distribution of words w in\ntext T and Q is the probability distribution of words w\nin summary S; N is the number of words in text and\nsummary N = NT +NS, B = 1.5|V |, CT\nw is the number\nof words in the text and CS\nw is the number of words in\nthe summary. For smoothing the summary’s probabilities\nwe have used δ = 0.005. We have also implemented\nother smoothing approaches (e.g. Good-Turing [24], that\nuses the CPAN Perl’s Statistics-Smoothing-SGT-2.1.2\npackage11) in FRESA, but we do not use them in\nthe experiments reported here. Following the ROUGE\napproach, in addition to word uni-grams we use 2-grams\nand skip n-grams computing divergences such as J S\n(using uni-grams) J S2 (using 2-grams), J S4 (using the\nskip n-grams of ROUGE-SU4), and J SM which is an\naverage of the J Si. J Ss measures are used to compare a\npeer summary to its source document(s) in our framework\n(as indicated in equation 4). In the case of summarization\nof multiple documents, these are concatenated (in the\ngiven input order) to form a single input from which\nprobabilities are computed.\n', 15, 0)
(311.9730224609375, 489.2679748535156, 563.0410766601562, 695.389404296875, 'IV. EXPERIMENTS AND RESULTS\nWe ﬁrst replicated the experiments presented in [12] to\nverify that our implementation of J S produced correlation\nresults compatible with that work. We used the TAC’08\nUpdate Summarization data set and computed J S and\nROUGE measures for each peer summary. We produced\ntwo system rankings (one for each measure), which were\ncompared to rankings produced using the manual PYRAMIDS\nand RESPONSIVENESS scores. Spearman correlations were\ncomputed among the different rankings. The results are\npresented in Table I. These results conﬁrm a high correlation\namong PYRAMIDS, RESPONSIVENESS and J S. We also\nveriﬁed high correlation between J S and ROUGE-2 (0.83\nSpearman correlation, not shown in the table) in this task and\ndataset.\nThen, we experimented with data from DUC’04, TAC’08\nOpinion Summarization pilot task as well as single and\n', 16, 0)
(319.9430236816406, 703.5291137695312, 537.347412109375, 715.9188842773438, '11http://search.cpan.org/∼bjoernw/Statistics-Smoothing-SGT-2.1.2/\n', 17, 0)
(46.77693176269531, 20.7718505859375, 397.5782470703125, 29.6312255859375, 'Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales\n', 18, 0)
(46.77693176269531, 762.5703125, 310.0, 771.4296875, '16\nPolibits (42) 2010\n', 19, 0)

page suivante
(55.504005432128906, 51.097694396972656, 293.47247314453125, 78.63452911376953, 'TABLE I\nSPEARMAN CORRELATION OF CONTENT-BASED MEASURES IN TAC’08\nUPDATE SUMMARIZATION TASK\n', 0, 0)
(56.35200119018555, 96.33982849121094, 297.5127258300781, 106.70880126953125, 'Mesure\nPYRAMIDS\np-value\nRESPONSIVENESS\np-value\n', 1, 0)
(55.13600158691406, 105.99173736572266, 301.54315185546875, 115.59558868408203, 'ROUGE-2\n0.96\np < 0.005\n0.92\np < 0.005\n', 2, 0)
(64.1449966430664, 115.35672760009766, 301.54315185546875, 130.3323516845703, 'J S\n0.85\np < 0.005\n0.74\np < 0.005\n', 3, 0)
(48.95899963378906, 155.54620361328125, 300.02911376953125, 658.4845581054688, 'multi-document summarization in Spanish and French. In spite\nof the fact that the experiments for French and Spanish corpora\nuse less data points (i.e., less summarizers per task) than\nfor English, results are still quite signiﬁcant. For DUC’04,\nwe computed the J S measure for each peer summary in\ntasks 2 and 5 and we used J S, ROUGE, COVERAGE and\nRESPONSIVENESS scores to produce systems’ rankings. The\nvarious Spearman’s rank correlation values for DUC’04 are\npresented in Tables II (for task 2) and III (for task 5).\nFor task 2, we have veriﬁed a strong correlation between\nJ S and COVERAGE. For task 5, the correlation between\nJ S and COVERAGE is weak, and that between J S and\nRESPONSIVENESS is weak and negative.\nAlthough the Opinion Summarization (OS) task is a new\ntype of summarization task and its evaluation is a complicated\nissue, we have decided to compare J S rankings with those\nobtained using PYRAMIDS and RESPONSIVENESS in TAC’08.\nSpearman’s correlation values are listed in Table IV. As it can\nbe seen, there is weak and negative correlation of J S with\nboth PYRAMIDS and RESPONSIVENESS. Correlation between\nPYRAMIDS and RESPONSIVENESS rankings is high for this\ntask (0.71 Spearman’s correlation value).\nFor experimentation in mono-document summarization\nin Spanish and French, we have run 11 multi-lingual\nsummarization systems; for experimentation in French, we\nhave run 12 systems. In both cases, we have produced\nsummaries at a compression rate close to the compression rate\nof the authors’ provided abstracts. We have then computed J S\nand ROUGE measures for each summary and we have averaged\nthe measure’s values for each system. These averages were\nused to produce rankings per each measure. We computed\nSpearman’s correlations for all pairs of rankings.\nResults are presented in Tables V, VI and VII. All results\nshow medium to strong correlation between the J S measures\nand ROUGE measures. However the J S measure based on\nuni-grams has lower correlation than J Ss which use n-grams\nof higher order. Note that table VII presents results for\ngeneric multi-document summarization in French, in this\ncase correlation scores are lower than correlation scores for\nsingle-document summarization in French, a result which may\nbe expected given the diversity of input in multi-document\nsummarization.\n', 4, 0)
(140.90005493164062, 674.069091796875, 208.08522033691406, 686.0745239257812, 'V. DISCUSSION\n', 5, 0)
(48.95905685424805, 690.705078125, 300.0267639160156, 714.66552734375, 'The departing point for our inquiry into text summarization\nevaluation has been recent work on the use of content-based\n', 6, 0)
(311.9730529785156, 55.88812255859375, 563.0410766601562, 523.1875610351562, 'evaluation metrics that do not rely on human models but that\ncompare summary content to input content directly [12]. We\nhave some positive and some negative results regarding the\ndirect use of the full document in content-based evaluation.\nWe have veriﬁed that in both generic muti-document\nsummarization\nand\nin\ntopic-based\nmulti-document\nsummarization\nin\nEnglish\ncorrelation\namong\nmeasures\nthat use human models (PYRAMIDS,\nRESPONSIVENESS\nand ROUGE) and a measure that does not use models\n(J S divergence) is strong. We have found that correlation\namong the same measures is weak for summarization of\nbiographical information and summarization of opinions in\nblogs. We believe that in these cases content-based measures\nshould be considered, in addition to the input document, the\nsummarization task (i.e. text-based representation, description)\nto better assess the content of the peers [25], the task being a\ndeterminant factor in the selection of content for the summary.\nOur multi-lingual experiments in generic single-document\nsummarization\nconﬁrm\na\nstrong\ncorrelation\namong\nthe\nJ S divergence and ROUGE measures. It is worth noting\nthat\nROUGE\nis\nin\ngeneral\nthe\nchosen\nframework\nfor\npresenting content-based evaluation results in non-English\nsummarization.\nFor the experiments in Spanish, we are conscious that we\nonly have one model summary to compare with the peers.\nNevertheless, these models are the corresponding abstracts\nwritten by the authors. As the experiments in [26] show, the\nprofessionals of a specialized domain (as, for example, the\nmedical domain) adopt similar strategies to summarize their\ntexts and they tend to choose roughly the same content chunks\nfor their summaries. Previous studies have shown that author\nabstracts are able to reformulate content with ﬁdelity [27] and\nthese abstracts are ideal candidates for comparison purposes.\nBecause of this, the summary of the author of a medical article\ncan be taken as reference for summaries evaluation. It is worth\nnoting that there is still debate on the number of models to be\nused in summarization evaluation [28]. In the French corpus\nPISTES, we suspect the situation is similar to the Spanish\ncase.\n', 7, 0)
(352.0540771484375, 539.4141235351562, 522.9530029296875, 551.4195556640625, 'VI. CONCLUSIONS AND FUTURE WORK\n', 8, 0)
(311.9730224609375, 556.504150390625, 563.0433349609375, 714.6655883789062, 'This paper has presented a series of experiments in\ncontent-based measures that do not rely on the use of model\nsummaries for comparison purposes. We have carried out\nextensive experimentation with different summarization tasks\ndrawing a clearer picture of tasks where the measures could\nbe applied. This paper makes the following contributions:\n– We have shown that if we are only interested in ranking\nsummarization systems according to the content of their\nautomatic summaries, there are tasks were models could\nbe subtituted by the full document in the computation of\nthe J S measure obtaining reliable rankings. However,\nwe have also found that the substitution of models\nby full-documents is not always advisable. We have\n', 9, 0)
(403.68701171875, 20.7718505859375, 565.2243041992188, 29.6312255859375, 'Summary Evaluation with and without References\n', 10, 0)
(302.0, 762.5703125, 565.2230224609375, 771.4296875, '17\nPolibits (42) 2010\n', 11, 0)

page suivante
(164.44000244140625, 51.097694396972656, 447.5509948730469, 69.66852569580078, 'TABLE II\nSPEARMAN ρ OF CONTENT-BASED MEASURES WITH COVERAGE IN DUC’04 TASK 2\n', 0, 0)
(239.67300415039062, 79.95677185058594, 364.78973388671875, 90.32574462890625, 'Mesure\nCOVERAGE\np-value\n', 1, 0)
(238.45700073242188, 89.6087417602539, 370.9352111816406, 99.21259307861328, 'ROUGE-2\n0.79\np < 0.0050\n', 2, 0)
(247.46600341796875, 98.9737319946289, 370.9352111816406, 113.9493637084961, 'J S\n0.68\np < 0.0025\n', 3, 0)
(194.71800231933594, 125.3187026977539, 417.2730407714844, 143.8895263671875, 'TABLE III\nSPEARMAN ρ OF CONTENT-BASED MEASURES IN DUC’04 TASK 5\n', 4, 0)
(184.14500427246094, 148.20079040527344, 424.5517578125, 158.56976318359375, 'Mesure\nCOVERAGE\np-value\nRESPONSIVENESS\np-value\n', 5, 0)
(182.9290008544922, 157.8527069091797, 426.46710205078125, 167.45654296875, 'ROUGE-2\n0.78\np < 0.001\n0.44\np < 0.05\n', 6, 0)
(191.93800354003906, 167.2176971435547, 426.46710205078125, 182.1933135986328, 'J S\n0.40\np < 0.050\n-0.18\np < 0.25\n', 7, 0)
(192.84500122070312, 193.56272888183594, 419.14935302734375, 212.132568359375, 'TABLE IV\nSPEARMAN ρ OF CONTENT-BASED MEASURES IN TAC’08 OS TASK\n', 8, 0)
(186.9429931640625, 222.4217987060547, 420.3387451171875, 232.790771484375, 'Mesure\nPYRAMIDS\np-value\nRESPONSIVENESS\np-value\n', 9, 0)
(194.73599243164062, 232.07371520996094, 422.2540588378906, 247.04933166503906, 'J S\n-0.13\np < 0.25\n-0.14\np < 0.25\n', 10, 0)
(130.3159942626953, 258.4187316894531, 481.6750183105469, 276.98956298828125, 'TABLE V\nSPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE Medicina Cl´ınica CORPUS (SPANISH)\n', 11, 0)
(148.5070037841797, 287.2787780761719, 456.65875244140625, 297.64776611328125, 'Mesure\nROUGE-1\np-value\nROUGE-2\np-value\nROUGE-SU4\np-value\n', 12, 0)
(148.5070037841797, 296.9297180175781, 460.68914794921875, 311.9053649902344, 'J S\n0.56\np < 0.100\n0.46\np < 0.100\n0.45\np < 0.200\n', 13, 0)
(148.5070037841797, 306.2947082519531, 460.68914794921875, 321.2703552246094, 'J S2\n0.88\np < 0.001\n0.80\np < 0.002\n0.81\np < 0.005\n', 14, 0)
(148.5070037841797, 315.6596984863281, 460.68914794921875, 330.6353454589844, 'J S4\n0.88\np < 0.001\n0.80\np < 0.002\n0.81\np < 0.005\n', 15, 0)
(148.5070037841797, 325.02471923828125, 460.68914794921875, 340.0003662109375, 'J SM\n0.82\np < 0.005\n0.71\np < 0.020\n0.71\np < 0.010\n', 16, 0)
(58.92198944091797, 364.5281982421875, 300.0266418457031, 543.9054565429688, 'found weak correlation among different rankings in\ncomplex summarization tasks such as the summarization\nof biographical information and the summarization of\nopinions.\n– We have also carried out large-scale experiments in\nSpanish and French which show positive medium to\nstrong correlation among system’s ranks produced by\nROUGE and divergence measures that do not use the\nmodel summaries.\n– We have also presented a new framework, FRESA, for\nthe computation of measures based on J S divergence.\nFollowing the ROUGE approach, FRESA package use\nword uni-grams, 2-grams and skip n-grams computing\ndivergences. This framework will be available to the\ncommunity for research purposes.\n', 17, 0)
(48.958980560302734, 547.2430419921875, 300.0267639160156, 714.6654663085938, 'Although we have made a number of contributions, this paper\nleaves many open questions than need to be addressed. In\norder to verify correlation between ROUGE and J S, in the\nshort term we intend to extend our investigation to other\nlanguages such as Portuguese and Chinesse for which we\nhave access to data and summarization technology. We also\nplan to apply FRESA to the rest of the DUC and TAC\nsummarization tasks, by using several smoothing techniques.\nAs a novel idea, we contemplate the possibility of adapting\nthe evaluation framework for the phrase compression task\n[29], which, to our knowledge, does not have an efﬁcient\nevaluation measure. The main idea is to calculate J S from\nan automatically-compressed sentence taking the complete\nsentence by reference. In the long term, we plan to incorporate\n', 18, 0)
(311.9730224609375, 364.528076171875, 563.0410766601562, 568.5073852539062, 'a representation of the task/topic in the calculation of\nmeasures. To carry out these comparisons, however, we are\ndependent on the existence of references.\nFRESA will also be used in the new question-answer task\ncampaign INEX’2010 (http://www.inex.otago.ac.nz/tracks/qa/\nqa.asp) for the evaluation of long answers. This task aims\nto answer a question by extraction and agglomeration of\nsentences in Wikipedia. This kind of task corresponds\nto those for which we have found a high correlation\namong the measures J S\nand evaluation methods with\nhuman intervention. Moreover, the J S calculation will be\namong the summaries produced and a representative set of\nrelevant passages from Wikipedia. FRESA will be used to\ncompare three types of systems, although different tasks: the\nmulti-document summarizer guided by a query, the search\nsystems targeted information (focused IR) and the question\nanswering systems.\n', 19, 0)
(394.0560302734375, 587.7799072265625, 480.9524841308594, 599.7853393554688, 'ACKNOWLEDGMENT\n', 20, 0)
(311.9730224609375, 606.9689331054688, 563.0410766601562, 714.6653442382812, 'We are grateful to the Programa Ram´on y Cajal from\nMinisterio de Ciencia e Innovaci´on, Spain. This work is\npartially supported by: a postdoctoral grant from the National\nProgram for Mobility of Research Human Resources (National\nPlan of Scientiﬁc Research, Development and Innovation\n2008-2011, Ministerio de Ciencia e Innovaci´on, Spain); the\nresearch project CONACyT, number 82050, and the research\nproject PAPIIT-DGAPA (Universidad Nacional Aut´onoma de\nM´exico), number IN403108.\n', 21, 0)
(46.77693176269531, 20.7718505859375, 397.5782470703125, 29.6312255859375, 'Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales\n', 22, 0)
(46.77693176269531, 762.5703125, 310.0, 771.4296875, '18\nPolibits (42) 2010\n', 23, 0)

page suivante
(144.4510040283203, 51.097694396972656, 467.5400390625, 69.66852569580078, 'TABLE VI\nSPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE PISTES CORPUS (FRENCH)\n', 0, 0)
(152.01800537109375, 79.95677185058594, 455.9367370605469, 90.32574462890625, 'Mesure\nROUGE-1\np-value\nROUGE-2\np-value\nROUGE-SU4\np-value\n', 1, 0)
(152.01800537109375, 89.6087417602539, 459.9661560058594, 104.5843734741211, 'J S\n0.70\np < 0.050\n0.73\np < 0.05\n0.73\np < 0.500\n', 2, 0)
(152.01800537109375, 98.9737319946289, 459.9661560058594, 113.9493637084961, 'J S2\n0.93\np < 0.002\n0.86\np < 0.01\n0.86\np < 0.005\n', 3, 0)
(152.01800537109375, 108.3387222290039, 459.9661560058594, 123.3143539428711, 'J S4\n0.83\np < 0.020\n0.76\np < 0.05\n0.76\np < 0.050\n', 4, 0)
(152.01800537109375, 117.7037124633789, 459.9661560058594, 132.67933654785156, 'J SM\n0.88\np < 0.010\n0.83\np < 0.02\n0.83\np < 0.010\n', 5, 0)
(147.2840118408203, 153.01573181152344, 464.7070007324219, 171.5855712890625, 'TABLE VII\nSPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE RPM2 CORPUS (FRENCH)\n', 6, 0)
(150.7480010986328, 181.8748016357422, 455.13873291015625, 192.2437744140625, 'Measure\nROUGE-1\np-value\nROUGE-2\np-value\nROUGE-SU4\np-value\n', 7, 0)
(150.7480010986328, 191.52671813964844, 458.4490966796875, 206.50233459472656, 'J S\n0.830\np < 0.002\n0.660\np < 0.05\n0.741\np < 0.01\n', 8, 0)
(150.7480010986328, 200.89170837402344, 458.4490966796875, 215.86732482910156, 'J S2\n0.800\np < 0.005\n0.590\np < 0.05\n0.680\np < 0.02\n', 9, 0)
(150.7480010986328, 210.25572204589844, 458.4490966796875, 225.23133850097656, 'J S4\n0.750\np < 0.010\n0.520\np < 0.10\n0.620\np < 0.05\n', 10, 0)
(150.7480010986328, 219.62071228027344, 458.4490966796875, 234.59632873535156, 'J SM\n0.850\np < 0.002\n0.640\np < 0.05\n0.740\np < 0.01\n', 11, 0)
(146.593994140625, 259.12420654296875, 202.39389038085938, 271.1296081542969, 'REFERENCES\n', 12, 0)
(48.959983825683594, 282.3247375488281, 300.0201721191406, 714.1057739257812, '[1] I.\nMani,\nG.\nKlein,\nD.\nHouse,\nL.\nHirschman,\nT.\nFirmin,\nand\nB. Sundheim, “Summac: a text summarization evaluation,” Natural\nLanguage Engineering, vol. 8, no. 1, pp. 43–68, 2002.\n[2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,\nno. 6, pp. 1506–1520, 2007.\n[3] Proceedings of the Text Analysis Conference.\nGaithesburg, Maryland,\nUSA: NIST, November 17-19 2008.\n[4] K. Sp¨arck Jones and J. Galliers, Evaluating Natural Language\nProcessing Systems, An Analysis and Review, ser. Lecture Notes in\nComputer Science.\nSpringer, 1996, vol. 1083.\n[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison of\nrankings produced by summarization evaluation measures,” in NAACL\nWorkshop on Automatic Summarization, 2000, pp. 69–78.\n[6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluation\nof Summaries in a Cross-lingual Environment using Content-based\nMetrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.\n[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. C¸ elebi,\nD. Liu, and E. Dr´abek, “Evaluation challenges in large-scale document\nsummarization,” in ACL’03, 2003, pp. 375–382.\n[8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a method\nfor automatic evaluation of machine translation,” in ACL’02, 2002, pp.\n311–318.\n[9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in Evaluation\nInitiatives in Natural Language Processing. Budapest, Hungary: EACL,\n14 April 2003.\n[10] C.-Y.\nLin,\n“ROUGE:\nA\nPackage\nfor\nAutomatic\nEvaluation\nof\nSummaries,” in Text Summarization Branches Out: ACL-04 Workshop,\nM.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.\n[11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in\nSummarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.\n145–152.\n[12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selection\nin Summarization without Human Models,” in Empirical Methods in\nNatural Language Processing, Singapore, August 2009, pp. 306–314.\n[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032\n[13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEE\nTransactions on Information Theory, vol. 37, no. 145-151, 1991.\n[14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries Using\nN-gram Co-occurrence Statistics,” in HLT-NAACL.\nMorristown, NJ,\nUSA: Association for Computational Linguistics, 2003, pp. 71–78.\n[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoretic\napproach to automatic evaluation of summaries,” in HLT-NAACL,\nMorristown, USA, 2006, pp. 463–470.\n[16] S. Kullback and R. Leibler, “On information and sufﬁciency,” Ann. of\nMath. Stat., vol. 22, no. 1, pp. 79–86, 1951.\n[17] S. Siegel and N. Castellan, Nonparametric Statistics for the Behavioral\nSciences.\nMcGraw-Hill, 1998.\n', 13, 0)
(311.9729919433594, 260.9259338378906, 563.0343627929688, 602.325927734375, '[18] C. de Loupy, M. Gu´egan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,\n“A\nFrench\nHuman\nReference\nCorpus\nfor\nmulti-documents\nsummarization\nand\nsentence\ncompression,”\nin\nLREC’10,\nvol.\n2,\nMalta, 2010, p. In press.\n[19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, “Textual Energy\nof Associative Memories: performants applications of Enertex algorithm\nin text summarization and topic segmentation,” in MICAI’07, 2007, pp.\n861–871.\n[20] J.-M.\nTorres-Moreno,\nP.\nVel´azquez-Morales,\nand\nJ.-G.\nMeunier,\n“Condens´es de textes par des m´ethodes num´eriques,” in JADT’02, vol. 2,\nSt Malo, France, 2002, pp. 723–734.\n[21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Vel´azquez-Morales,\n“Automatic\nsummarization\nusing\nterminological\nand\nsemantic\nresources,” in LREC’10, vol. 2, Malta, 2010, p. In press.\n[22] J.-M. Torres-Moreno and J. Ramirez, “REG : un algorithme glouton\nappliqu´e au r´esum´e automatique de texte,” in JADT’10.\nRome, 2010,\np. In press.\n[23] V. Yatsko and T. Vishnyakov, “A method for evaluating modern\nsystems of automatic text summarization,” Automatic Documentation\nand Mathematical Linguistics, vol. 41, no. 3, pp. 93–103, 2007.\n[24] C. D. Manning and H. Sch¨utze, Foundations of Statistical Natural\nLanguage Processing.\nCambridge, Massachusetts: The MIT Press,\n1999.\n[25] K. Sp¨arck Jones, “Automatic summarising: The state of the art,” IPM,\nvol. 43, no. 6, pp. 1449–1481, 2007.\n[26] I. da Cunha, L. Wanner, and M. T. Cabr´e, “Summarization of specialized\ndiscourse: The case of medical articles in spanish,” Terminology, vol. 13,\nno. 2, pp. 249–286, 2007.\n[27] C.-K. Chuah, “Types of lexical substitution in abstracting,” in ACL\nStudent Research Workshop.\nToulouse, France: Association for\nComputational Linguistics, 9-11 July 2001 2001, pp. 49–54.\n[28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries:\nMetrics under varying data conditions,” in UCNLG+Sum’09, Suntec,\nSingapore, August 2009, pp. 23–30.\n[29] K. Knight and D. Marcu, “Statistics-based summarization-step one:\nSentence compression,” in Proceedings of the National Conference on\nArtiﬁcial Intelligence.\nMenlo Park, CA; Cambridge, MA; London;\nAAAI Press; MIT Press; 1999, 2000, pp. 703–710.\n', 14, 0)
(403.68701171875, 20.7718505859375, 565.2243041992188, 29.6312255859375, 'Summary Evaluation with and without References\n', 15, 0)
(302.0, 762.5703125, 565.2230224609375, 771.4296875, '19\nPolibits (42) 2010\n', 16, 0)

page suivante
