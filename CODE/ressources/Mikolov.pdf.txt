(129.6300048828125, 106.38220977783203, 482.3736877441406, 148.70443725585938, 'Efﬁcient Estimation of Word Representations in\nVector Space\n', 0, 0)
(149.90499877929688, 189.03892517089844, 282.058837890625, 229.45713806152344, 'Tomas Mikolov\nGoogle Inc., Mountain View, CA\ntmikolov@google.com\n', 1, 0)
(329.94097900390625, 189.03892517089844, 462.0948486328125, 229.45713806152344, 'Kai Chen\nGoogle Inc., Mountain View, CA\nkaichen@google.com\n', 2, 0)
(149.90496826171875, 246.71287536621094, 282.0588073730469, 287.131103515625, 'Greg Corrado\nGoogle Inc., Mountain View, CA\ngcorrado@google.com\n', 3, 0)
(329.94097900390625, 246.71287536621094, 462.0948486328125, 287.131103515625, 'Jeffrey Dean\nGoogle Inc., Mountain View, CA\njeff@google.com\n', 4, 0)
(283.7579650878906, 315.5259704589844, 328.2432556152344, 331.0796813964844, 'Abstract\n', 5, 0)
(143.86495971679688, 342.05950927734375, 468.1376037597656, 430.7763671875, 'We propose two novel model architectures for computing continuous vector repre-\nsentations of words from very large data sets. The quality of these representations\nis measured in a word similarity task, and the results are compared to the previ-\nously best performing techniques based on different types of neural networks. We\nobserve large improvements in accuracy at much lower computational cost, i.e. it\ntakes less than a day to learn high quality word vectors from a 1.6 billion words\ndata set. Furthermore, we show that these vectors provide state-of-the-art perfor-\nmance on our test set for measuring syntactic and semantic word similarities.\n', 6, 0)
(107.99995422363281, 450.6298828125, 190.81362915039062, 466.18359375, '1\nIntroduction\n', 7, 0)
(107.99995422363281, 476.6544189453125, 504.00335693359375, 543.4542846679688, 'Many current NLP systems and techniques treat words as atomic units - there is no notion of similar-\nity between words, as these are represented as indices in a vocabulary. This choice has several good\nreasons - simplicity, robustness and the observation that simple models trained on huge amounts of\ndata outperform complex systems trained on less data. An example is the popular N-gram model\nused for statistical language modeling - today, it is possible to train N-grams on virtually all available\ndata (trillions of words [3]).\n', 8, 0)
(107.99995422363281, 548.3853759765625, 504.00335693359375, 615.185302734375, 'However, the simple techniques are at their limits in many tasks. For example, the amount of\nrelevant in-domain data for automatic speech recognition is limited - the performance is usually\ndominated by the size of high quality transcribed speech data (often just millions of words). In\nmachine translation, the existing corpora for many languages contain only a few billions of words\nor less. Thus, there are situations where simple scaling up of the basic techniques will not result in\nany signiﬁcant progress, and we have to focus on more advanced techniques.\n', 9, 0)
(107.99995422363281, 620.1163330078125, 504.0032653808594, 664.998291015625, 'With progress of machine learning techniques in recent years, it has become possible to train more\ncomplex models on much larger data set, and they typically outperform the simple models. Probably\nthe most successful concept is to use distributed representations of words [10]. For example, neural\nnetwork based language models signiﬁcantly outperform N-gram models [1, 27, 17].\n', 10, 0)
(107.99995422363281, 677.7947387695312, 209.1800994873047, 690.7560424804688, '1.1\nGoals of the Paper\n', 11, 0)
(107.99995422363281, 698.7383422851562, 504.0032043457031, 732.6613159179688, 'The main goal of this paper is to introduce techniques that can be used for learning high-quality word\nvectors from huge data sets with billions of words, and with millions of words in the vocabulary. As\nfar as we know, none of the previously proposed architectures has been successfully trained on more\n', 12, 0)
(303.50897216796875, 750.5443725585938, 308.4902648925781, 762.54931640625, '1\n', 13, 0)
(10.940000534057617, 223.3599853515625, 37.619998931884766, 550.0, 'arXiv:1301.3781v3  [cs.CL]  7 Sep 2013\n', 14, 0)
