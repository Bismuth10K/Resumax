(156.66000366210938, 252.67132568359375, 441.06988525390625, 275.2877197265625, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT \nSYSTEMS*  \n', 0, 0)
(157.860107421875, 287.53094482421875, 442.47802734375, 298.6273193359375, 'M. Teresa Cabré Castellví, Rosa Estopà Bagot, Jordi Vivaldi Palatresi  \n', 1, 0)
(151.9803009033203, 299.05096435546875, 445.7968444824219, 310.1473388671875, 'Institut Universitari de Lingüística Aplicada. Universitat Pompeu Fabra \n', 2, 0)
(261.1200866699219, 310.571044921875, 336.58270263671875, 321.66741943359375, 'La Rambla, 30-32 \n', 3, 0)
(243.60009765625, 322.03094482421875, 354.17486572265625, 333.1273193359375, 'Barcelona – Spain - 08002 \n', 4, 0)
(141.83990478515625, 333.5509338378906, 455.864990234375, 344.6474304199219, 'teresa.cabre@trad.upf.es, \nrosa.estopa@trad.upf.es, jorge.vivaldi@info.upf.es\n \n', 5, 0)
(134.6396942138672, 369.1911315917969, 174.52529907226562, 380.2875061035156, 'Abstract \n', 6, 0)
(155.63999938964844, 416.419677734375, 442.0108642578125, 457.4661865234375, 'In this paper we account for the main characteristics and performance of a \nnumber of recently developed term extraction systems. The analysed tools \nrepresent the main strategies followed by researchers in this area. All systems \nare analysed and compared against a set of technically relevant characteristics. \n', 7, 0)
(134.63999938964844, 482.05133056640625, 212.65673828125, 493.147705078125, '1. \nIntroduction \n', 8, 0)
(134.63958740234375, 493.4510498046875, 463.347900390625, 667.785888671875, 'In the late 80s there was an acute need, from different disciplines and goals, to \nautomatically extract terminological units from specialised texts. In the 90s large \ncomputerised textual corpora have been constructed resulting in the first \nprograms for terminology extraction1 (henceforth TE) which have showed \nencouraging results. \nThroughout the current decade computational linguists, applied linguists, \ntranslators, interpreters, scientific journalists and computer engineers have been \ninterested in automatically isolating terminology from texts. There are many \ngoals that have led these different professional groups to design software tools so \nas to directly extract terminology from texts: building of glossaries, vocabularies \nand terminological dictionaries; text indexing; automatic translation; building of \n                                                           \n* In Bourigault, D.; Jacquemin, C.; L’Homme, M-C. (2001) Recent Advances in Computational \nTerminology, 53-88. \n1 In order to give a broader view of TE we use both extractor and detector to refer to the same notion. \nHowever, we are aware of the fact that some scholars attribute different meanings to these words. \n', 9, 0)

page suivante
(134.63999938964844, 175.99102783203125, 374.29931640625, 187.08740234375, '2 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.63999938964844, 204.3707275390625, 463.2569274902344, 468.4269104003906, 'knowledge databases; construction of hypertext systems; construction of expert \nsystems and corpus analysis.  \nFrom the appearance of TERMINO (the first broadly known term detector) in \n1990 until today a number of projects to design different types of automatic \nterminology detectors have been carried out to assist terminological work. \nHowever, despite the large number of studies in progress, the automatisation of \nthe terminological extraction phase is still fraught with problems. The main \nproblems encountered by term extractors are: (1) identification of complex \nterms, that is, determining where a terminological phrase begins and ends; (2) \nrecognition of complex terms, that is, deciding whether a discursive unit \nconstitutes a terminological phrase or a free unit; (3) identification of the \nterminological nature of a lexical unit, that is, knowing whether in a specialised \ntext a lexical unit has a terminological nature or belongs to general language and \n(4) appropriateness of a terminological unit to a given vocabulary (this has \nscarcely been addressed from the point of view of automatization).  \nSystems for TE are based on three types of knowledge: (a) linguistic; (b) \nstatistical; (c) hybrid (statistical and linguistic). Hence, there are different \napproaches to automatic term detection. All systems analyse a corpus of \nspecialised texts in electronic form and extract lists of word chunks (i.e. \ncandidate terms) that are to be confirmed by the terminologist. To make the \nterminologist’s task easier the candidate term is provided with its context and, \nwhen available, with any other further information (frequency, relationship \nbetween terms, etc.) \n', 1, 0)
(134.6391143798828, 468.8509521484375, 463.2913818359375, 656.9262084960938, 'Two relevant aspects regarding the nature of terms are termhood and \nunithood2; TE systems may be designed based on only one of these two aspects. \nSome practical experiments following each scheme for ranking a set terms \nextracted from Japanese texts are presented in (Nakagawa & Mori, 1998). They \nshow that results in precision and recall are very close but the set of terms \nextracted are a somewhat different. This is still a research issue. \nAlongside term detection we find the task of automatic document indexing (i.e. \ninformation retrieval, IR). This applied field of natural language processing \n(NLP) techniques has an interesting common point with automatic term \ndetection, that is, word chunks that index a given document are often \nterminological units. This same goal explains why many extraction systems are \nrooted on IR as well as on the analysis of a specific IR system with no \napplication whatsoever to TE. \n                                                           \n2 (Kageura & Umino, 1996) refer to unithood as the degree of stability of syntagmatic combinations \n(collocations) and termhood as the degree in that a linguistic unit is related to a domain-specific \nconcept. \n', 2, 0)

page suivante
(134.64065551757812, 175.99102783203125, 463.1714172363281, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  3 \n \n', 0, 0)
(134.63938903808594, 204.3707275390625, 463.29443359375, 670.7260131835938, 'The difference between these two approaches lies in the fact that a tool for TE \nshould extract all terminological units from a text, whereas IR focuses on the \nextraction of only words or word sequences that better describe the contents of \nthe document regardless of their grammatical features. \nThe standard approach to IR consists in processing documents so as to extract \nthe so-called indexing terms. These terms are usually isolated words containing \nenough semantic load to provide information about its goodness when describing \ndocuments. Queries are processed in a similar fashion to extract query terms. \nWith regard to queries the relevance of documents is based exclusively on their \nrepresenting terms. This is the reason why their choice is crucial. \nOften these indexing terms are single words although it is known that isolated \nwords are seldom relevant enough to decide the semantic value of a document \nwith regard to the query. This fact has given rise to the ever-growing appearance, \nin the TREC3 assessments, of word and word-sequence indexing systems using \nNLP techniques. \nStatistically based systems function by detecting two or more lexical units whose \noccurrence is higher than a given level. This is not a random situation, but it is \nrelated to a particular usage of these lexical units. This principle, called Mutual \nInformation, also applies to other science domains such as telecommunications \nand physics. Term detectors based on hybrid knowledge tend to use this idea \nprior to a linguistic-based processing. \nThe problem with this kind of approach is that there are low-frequency terms \ndifficult to be managed by extraction systems. Here it is important to note that \nthese systems use basically numerical information and thus are prone to be \nlanguage-independent. The two most frequently used measures in the assessment \nof these systems are found in IR: recall and precision. Recall is defined as the \nrelationship between the sum of retrieved terms and the sum of existing terms in \nthe document that is being explored. In contrast precision accounts for the \nrelationship between those extracted terms that are really terms and the aggregate \nof candidate terms that are found. These measures can be interpreted as the \ncapacity of the detection system to extract all terms from a document (recall) and \nthe capacity to discriminate between those units detected by the system which \nare terms and those which are not (precision). The fact that recall accounts for all \nterms from a document implies that it is a figure much more difficult to estimate \nand improve than precision. \nIn contrast with this traditional approach, other approaches attempt to solve the \nproblem by using linguistic knowledge, which may include two types of \ninformation: \n                                                           \n3 TREC (Text Retrieval Engineering Conference) refers to a series of conferences supported by NIST \nand DARPA (U.S. agencies). Further information can be found at: http://trec.nist.gov/. \n', 1, 0)

page suivante
(134.63999938964844, 175.99102783203125, 374.29931640625, 187.08740234375, '4 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.63980102539062, 204.3707275390625, 463.26202392578125, 663.9071044921875, 'a) Term specific: it consists in the detection of the recurrent patterns from \ncomplex terminological units such as noun-adjective and noun-preposition-noun. \nThis calls for the use of regular expressions and techniques of finite state \nautomata. \nb) Language generic: it consists in the use of more complex systems of NLP that \nstart with the detection of more basic linguistic structures: noun phrase (NP), \nprepositional phrase (PP), etc. \nIn both approaches each word is associated to a morphological category. In order \nto do so different strategies are proposed: from coarse systems that do not make \nuse of any dictionary to complex systems that have an extremely detailed \nmorphological analysis and a final phase of disambiguation. \nSystems that harness structural information resort to techniques of partial \nanalysis to detect potentially terminological phrasal structures. There are also \nsystems that benefit from their understanding of what is a non-term so they are at \nsome point in between those systems already mentioned. Other systems try to \nreutilize current terminological databases to find terms, variants or new terms. \nSystems based on linguistic knowledge tend to use noise and silence as a \nmeasure of its efficiency. Noise attempts to assess the rate between discarded \ncandidates and accepted ones; silence attempts to assess those terms contained in \nan analysed text that are not detected by the system. Noise is common problem \nof those systems using this approach. Errors in the assignation of morphological \ncategory are also shared by these systems. \nThe type of knowledge used leads to language-specific systems and therefore it \nrequires a prior linguistic analysis and probably a redesign of many parts of the \nsystem. Knowledge in artificial intelligence has been traditionally obtained from \nexperts in each domain. This has yielded several difficulties so that some \nscholars have focused on automatization and systematisation in knowledge \nacquisition. This strategy seems to show the benefits of a terminological \napproach. Thus some researchers (e.g. Condamines, 1995) have proposed the \nconstruction of terminological knowledge databases so as to include linguistic \nknowledge in traditional databases. Although this is a recent approach, there is \nno database yet containing all the features that could be used in TE, i.e. there is \nhardly any semantic information. Thus closed lists of words containing sparse \nsemantic information within a given specialised domain have been proposed.  \nIn this paper we attempt to analyse the main systems of terminology extraction in \norder to describe its current status and thus be able to enrich them. This paper is \ndivided up into two main parts: firstly, the largest part is devoted to describe \nvarious systems of terminology extraction together with a short evaluation in \nwhich weak and strong points have been outlined. Secondly, the terminology \nextraction systems have been classified according to some parameters.  \n', 1, 0)

page suivante
(134.64065551757812, 175.99102783203125, 463.1714172363281, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  5 \n \n', 0, 0)
(134.6396942138672, 204.490966796875, 378.6828308105469, 215.58734130859375, '2. \nDescription of some terminology extraction systems \n', 1, 0)
(134.6396942138672, 215.83056640625, 463.2364807128906, 307.447021484375, 'In the following sections we offer a critical description of number of \nsemiautomatic terminology extraction systems. In all cases, the following \ninformation is given: \na) The reference data of the system, that is, the author and the publication where \nthe tool is first mentioned and the system goal. \nb) A brief description of the system. \nc) A short evaluation of the most relevant aspects. This evaluation is mainly \nbased on papers, oral presentations in congresses and working papers, etc. \n', 2, 0)
(134.6396942138672, 319.87066650390625, 184.8519287109375, 330.967041015625, '2.1. \nANA \n', 3, 0)
(134.63966369628906, 331.3306579589844, 463.2460021972656, 659.7460327148438, 'Reference publication: Enguehard and Pantera (1994) \nMain goal: Term extraction \nANA (Automatic Natural Acquisition) has been developed in accordance with \nthe following design principles: non-utilisation of linguistic knowledge, dealing \nwith written and oral texts (interview transcripts) and non-concern about \nsyntactic errors. \nAccording to the current trend of harnessing statistical techniques in the study of \nnatural language, scholars use Mutual Information as a measure of lexical \nassociation4. In order to avoid the involvement of linguistic knowledge the \nconcept of “flexible string recognition” is created, which generates a \nmathematical function so as to determine the degree of similarity between words. \nThus, no tool for morphological analysis is needed. For instance, the string \ncolour of painting represents other similar strings like: colour of paintings, \ncolour of this painting, colour of any painting, etc. The system has neither a \ndictionary nor a grammar. \nThe architecture of ANA is composed of 2 modules: a familiarity module and a \ndiscovery module. The first module determines the following 3 groups of words, \nwhich constitute the only required knowledge for term detection: \na. function words (i.e. empty words): a, any, for, in, is, of, to... \nb. scheme words (i.e. words establishing semantic relationships) such as box of \nnails, where the preposition shows some kind of relationship between box and \nnails. \nc. bootstrap (i.e. set of terms that constitutes the kernel of the system and the \nstarting point for term detection). \nThe second module consists in a gradual acquisition process of new terms from \nexisting ones. Further, links between detected terms are automatically generated \n                                                           \n4 Remarkable examples of the use of these techniques are the works of Church & Hanks (1989) on \nword association and Smadja (1991) on collocation extraction from large corpora. \n', 4, 0)

page suivante
(134.63999938964844, 175.99102783203125, 374.3021240234375, 187.08740234375, '6 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.63929748535156, 204.3707275390625, 463.2679443359375, 390.1874084472656, 'to build a semantic network. This module is based on word co-occurrence that \ncan have 3 types of interpretations: \n• expression: high-frequency existing terms (TEXP) in the same window. The new \nword is considered a new term and thus is included in the semantic network. For \ninstance if the system has diesel and engine as a known terms and finds \nsequences like: ... the diesel engine is... or ... this diesel engine has... Then the \nsequence diesel engine is accepted as a new term and is included in the semantic \nnetwork as a new node with links to diesel and engine (see figure below). \n• candidate: an existing term appears frequently (TCAND) together with another \nword and a scheme word as in: ... any shade of wood... or ... this shade of \ncolour... Here shade becomes a new term and is placed in a new node of the \nsemantic network (see figure below). \n• expansion: an existing term appears frequently (TEXPA) in the same word \nsequence, without including any scheme word: ... use any soft woods to... or ... \nthis soft woods or... As a result, soft wood is incorporated into the term list and \nthe semantic network as a new node with a link to woods (see fig. 1 below). \n', 1, 0)
(317.7001037597656, 390.61114501953125, 463.2091979980469, 505.14752197265625, 'The system keeps on recursively \nseeking elements with the three \ninterpretations already mentioned \nuntil a new term is found. \nEnguehard and Pantera (1994) \ntested it by processing a document \nin English of around 25,000 words \nand 29 reference terms. The system \nmanaged to extract 200 terms with \nan error rate of 25%. \n', 2, 0)
(271.0199890136719, 413.2910461425781, 296.9868469238281, 424.3874206542969, 'shade \n', 3, 0)
(199.9199981689453, 450.8510437011719, 303.285400390625, 473.647705078125, 'Soft \nwood \nwood\n', 4, 0)
(208.55999755859375, 413.3510437011719, 237.84445190429688, 436.147705078125, 'Diesel \nengine \n', 5, 0)
(141.72000122070312, 436.81103515625, 170.9853973388672, 447.90740966796875, 'engine \n', 6, 0)
(143.22000122070312, 403.81103515625, 169.06460571289062, 414.90740966796875, 'diesel \n', 7, 0)
(151.5, 492.1396789550781, 297.1457824707031, 502.1064758300781, 'Figure 1 Term candidates interpretation \n', 8, 0)
(134.6396942138672, 505.69134521484375, 463.2880554199219, 597.1875, 'Evaluation \nMinimising linguistic resources is an extremely interesting issue, since it is \ndifficult to compile them. Likewise flexible string recognition may well apply to \nactual texts. \nA negative aspect of the system is that those terminological units added to the list \nof valid terms after each cycle are not validated. Thus ANA allows for the \ninclusion of non-valid terms that add up to the term list. However, no data about \nthe efficiency of this proposal are reported. \n', 9, 0)
(134.6396942138672, 608.2058715820312, 202.60501098632812, 620.6473999023438, '2.2. \nCLARIT5 \n', 10, 0)
(134.63999938964844, 621.071044921875, 331.2143249511719, 664.3058471679688, 'Reference publication: Evans and Zhai (1996) \nMain goal: Document indexing \n                                                           \n5 Further information can be found at: http://www.clarit.com. \n', 11, 0)

page suivante
(134.6396942138672, 175.99102783203125, 463.1709899902344, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  7 \n \n', 0, 0)
(134.6396942138672, 204.3707275390625, 463.3064880371094, 445.44732666015625, 'Document indexing for IR is an important field of application of NLP \ntechniques. This branch holds common points with term detection since the word \nsequences that help in document indexing are normally terminological units too.  \nCLARIT belongs to the group of systems that advocate an elaborated textual \nprocessing to detect complex terms in order to reach a more appropriate \ndescription of documents. This is the reason why we have included this system \namongst terminology detectors. \nEvans and Zhai (1996) propose the following kind of phrases for indexation \npurposes: \n1. lexical atoms (hot dog, stainless steel, data base, on line, ...) \n2. head modifier pairs (treated strip, ...) \n3. subcompounds (stainless steel strip, ...) \n4. cross-preposition modification pairs (quality surface vs. quality of surface) \nThe methodology starts with the morphological analysis of words and the \ndetection of noun phrases (NPs). The system distinguishes simplex noun phrases \nfrom cross-preposition simplex phrases. \nWhat is behind this is the introduction of statistics to corpus linguistics. Statistics \nhere focuses on documents, that is, there is no prior training corpus. Linguistic \nknowledge facilitates the calculation weeding out irrelevant structures, improves \nthe reliability of statistical decisions and adjusts the statistical parameters. \nThe whole process is showed in the figure below:  \n', 1, 0)
(134.6407012939453, 445.81085205078125, 463.21441650390625, 631.5075073242188, 'First, the raw text is parsed so as to \nextract NPs. Then each NP is \nrecursively parsed with the purpose \nof \nfinding \nthe \nmost \nsecure \ngroupings. In this phase lexical \natoms are also detected and NPs \nare structured. Finally at the \ngeneration phase the remaining \ncompounds are obtained. \nLexical atoms are defined as \nsequences of two or more words \nconstituting a single semantic unit \nsuch as space shuttle, part of \nspeech and hot dog. Since the \ndetection of these units is fraught \nwith problems two heuristical rules are proposed: \n', 2, 0)
(158.22000122070312, 607.3396606445312, 290.4317626953125, 617.3064575195312, 'Figure 2 Whole process in CLARIT \n', 3, 0)
(144.53939819335938, 490.1593933105469, 161.7248992919922, 500.1261901855469, 'NPs \n', 4, 0)
(203.6396942138672, 579.4393920898438, 303.1842041015625, 589.4061889648438, 'meaningful subcompounds \n', 5, 0)
(145.9199981689453, 447.1996765136719, 182.71600341796875, 457.1664733886719, 'Raw Text \n', 6, 0)
(252.77999877929688, 529.0996704101562, 307.3498229980469, 549.5667724609375, 'Lexical atoms \nAttested terms \n', 7, 0)
(165.24000549316406, 551.0596923828125, 219.452392578125, 561.0264892578125, 'Subcompound \n', 8, 0)
(174.17970275878906, 561.6192626953125, 210.51092529296875, 571.5860595703125, 'generator \n', 9, 0)
(134.639404296875, 531.799560546875, 176.751220703125, 552.2667846679688, 'Structured  \nNPs \n', 10, 0)
(172.6199951171875, 506.7796630859375, 220.5443878173828, 516.7464599609375, 'Simplex NP  \n', 11, 0)
(183.05999755859375, 517.3392944335938, 207.80020141601562, 527.3060913085938, 'Parser \n', 12, 0)
(178.13999938964844, 463.6996765136719, 215.03390502929688, 473.6664733886719, 'CLARIT  \n', 13, 0)
(170.82040405273438, 474.1999816894531, 220.0460205078125, 484.1667785644531, 'NP Extractor \n', 14, 0)
(134.6407012939453, 631.8711547851562, 463.2467041015625, 666.0075073242188, 'a. The words that constitute a lexical atom establish a close relationship and tend \nto lexicalise as if they were a single-word lexical unit. \nb. When acting as NP, lexical atoms hardly allow the insertion of words. \n', 15, 0)

page suivante
(134.63999938964844, 175.99102783203125, 374.29931640625, 187.08740234375, '8 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.63949584960938, 204.3707275390625, 463.2571716308594, 318.96734619140625, 'The first condition takes place if the frequency of the target pair W1W2 is higher \nthan any other pair from the NP that is being processed. In the second condition \nthe frequencies of grouped and separated occurrences are compared and there is \na threshold beyond which the association is weeded out. This threshold is \nvariable according to the function of sentence morphological category. In \nEnglish texts, the most favoured sequence is that of noun-noun. \nNP analysis is also a recursive process. At every new phase the most recent \nlexical atoms are used for finding new associations that will be used in the \nfollowing phase. The process keeps going until the whole NP is analysed. Let us \nconsider the example below: \n', 1, 0)
(191.58030700683594, 319.3309326171875, 406.1536865234375, 376.567626953125, 'general purpose high performance computer \ngeneral purpose [high performance] computer \n[general purpose] [high performance] computer \n[general purpose] [[high performance] computer] \n[[general purpose] [[high performance] computer]] \n', 2, 0)
(134.64059448242188, 376.81085205078125, 463.1676330566406, 434.647705078125, 'The grouping order shows those sequences with a more reliable association \nscore. In order to determine the association score a number of rules are taken into \naccount: \n• \nLexical atoms are given score 0 as well as adverb combination with \nadjective, past participles and progressive verbs, \n', 3, 0)
(134.64059448242188, 434.64697265625, 460.60736083984375, 458.34808349609375, '• \nSyntactically impossible pairs are given score 100 (noun-adjective, noun-\nadverb, adjective-adjective, etc.). \n', 4, 0)
(134.6396942138672, 458.3473815917969, 463.10540771484375, 493.5684814453125, '• \nAs to the remaining pairs, there is a formula that account for the frequency \nof each word, the association score of this word with other words from the \nNP and of two random parameters. \n', 5, 0)
(134.6396942138672, 493.99224853515625, 463.2147521972656, 654.5473022460938, 'To increase its reliability the association score is recomputed after every \nassignation association. The system has been tested in an actual retrieval task of \ndocument indexing substituting the default NLP module in the CLARIT system. \nThe corpus and the queries were the standards used in the TREC conferences. \nThere have been noticed some improvements in recall as well as in precision, \nwhich, in the author’s opinion, justifies the use of these techniques. Then in the \nTREC-5 report a more detailed evaluation of the system is made (Zhai et al. \n1996). All in all it is concluded that the use of these techniques is effective, \nwhich enforces the similarities between term indexing and terminology \nextraction. \nEvaluation \nThis seems to be an interesting system and the applicability of some basic ideas \nto terminology detection appears to be feasible. Actually CLARIT holds \nsimilarities with the Daille’s (1994) proposal (a linguistically-driven statistics). \n', 6, 0)

page suivante
(134.64065551757812, 175.99102783203125, 463.1714172363281, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  9 \n \n', 0, 0)
(134.6396942138672, 204.3707275390625, 463.25079345703125, 273.375732421875, 'It should be borne in mind, however, that problems of terminology extraction \nand document indexing are similar but no identical so that many decisions \nshould be re-considered strictly from the point of view of term detection. It is \nalso noteworthy that this system only extracts NP terminological units and the \ndata provided about how this system works are related to the application for \nwhich it has been designed. \n', 1, 0)
(134.63999938964844, 285.37103271484375, 247.4591522216797, 296.4674072265625, '2.3. \nDaille-94 (ACABIT) \n', 2, 0)
(134.63999938964844, 296.8309326171875, 463.2009582519531, 457.4472351074219, 'Reference publication: Daille (1994) \nMain goal: Term extraction \nThe main idea behind this system is to combine linguistic knowledge with \nstatistical measures. Here the corpus should contain all the morphological \ninformation. Then a list of candidate terms is created according to text sequences \nthat provide syntactic patterns of term formation. This information uses \nstatistical methods to filter out this list. This final process is different from other \nsystems in that it only uses linguistic resources. \nAssuming the fact that all terminological banks are basically composed of \ncompound nouns, the program focuses on the detection of binary compound \nnouns and disregards other co-occurring categories. This assumption lies in the \nfact that there is a large number of this kind of nouns in specialised languages. \nFurther, most of these compounds of 3 or more constituents can be treated in a \nbinary form. \n', 3, 0)
(134.6397705078125, 457.81103515625, 463.1925048828125, 514.9271850585938, 'Those patterns considered relevant for French are N1 PREP (DET) N2 and N \nADJ PREP à (DET) N2, together with right and left coordination. Statistical \nalgorithms are applied to these patterns. The author is aware of the fact that the \napplication of statistical measures leads to some noise rate, that is, low-frequency \nterms will not be recognised. \n', 4, 0)
(134.63980102539062, 515.35107421875, 463.2275390625, 652.9271240234375, 'The technique used for pattern recognition is that of finite state automata. \nAutomata are represented by a subset of grammatical tags to which some \nlemmas, inflected forms and a punctuation mark are added. Thus we can regard \nautomata as linguistic filters that select defined patterns and also determine their \noccurrence frequency, distance and variation. Each morphosyntactic pattern is \nassociated with a specific finite automaton. \nThe corpus is given a statistical treatment based on a large number of statistical \nmeasures, which are grouped in the following classes: frequency measures, \nassociation criteria, diversity criteria and distance measures. The starting point is \nconsidering the two lemmas that constitute a pair within a pattern as two \nvariables on which the dependence degree is measured. Data are represented in a \nstandard contingency table: \n', 5, 0)

page suivante
(134.63999938964844, 175.99102783203125, 379.3233642578125, 187.08740234375, '10 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.63999938964844, 204.3707275390625, 137.14500427246094, 215.46710205078125, ' \n', 1, 0)
(170.39999389648438, 215.71966552734375, 368.4569091796875, 226.537841796875, ' \nL2 \nLn \nwhere \na = L1L2 occurrences \n', 2, 0)
(170.39999389648438, 227.23138427734375, 396.36138916015625, 239.077880859375, 'L1 \nA \nb \n \nb = L1+Ln (n≠2) occurrences \n', 3, 0)
(170.39999389648438, 239.05133056640625, 400.0210876464844, 250.8978271484375, 'Lm \nC \nd \n \nc = Lm+L2 (m≠1) occurrences \n', 4, 0)
(170.39999389648438, 250.0313720703125, 386.73028564453125, 261.87786865234375, ' \n \n \n \nd = Lm+Ln (m≠1 and n≠2) \n', 5, 0)
(134.63967895507812, 261.551025390625, 463.18316650390625, 514.0872802734375, 'Eighteen measures are applied with the aim of establishing the degree of \nindependence of the variables in the contingency table. The analysis of the \nresults shows that only four of these measures are relevant to the purpose: \nfrequency, \ncubed \nassociation \ncriterion6 \n(IM3), \nlikelihood \ncriterion, \nFager/MacGowan criterion. \nEvaluation \nUnlike in other systems, in ACABIT frequency has turned out to be one of the \nmost important measures for term detection from a given area. However, the \nclassification resulting from the application of this frequency shows an important \nnumber of frequent sequences that are not terms and, in contrast, does not \nsuggest the low-frequency terms. \nDaille (1994) believes that the best measure is the likelihood criterion, since it is \na real statistical test, it proposes a classification that accounts for frequency, it \nbehaves adequately with large and medium size corpora and it is not defined in \nthose cases that are not to be considered. In any case, this measure yields some \nnoise due to several reasons: \na. Errors in the morphological mark-up. \nb. Some combinations that are never of a compounding nature: ko bits (kilobits), \nà titre d’exemple (as an example)... ... \nc. Combinations of 3 or more elements, related to the problems of composition \nand modification: bande latérale -unique- (-single- side band), service fixe -par \nsatellite- (-satellite- fixed service), etc. \n', 6, 0)
(134.6407928466797, 525.1658935546875, 196.84500122070312, 537.607421875, '2.4. \nFASTR7 \n', 7, 0)
(134.63980102539062, 537.970947265625, 463.1487731933594, 668.6258544921875, 'Reference publication: Jacquemin (1996) \nMain goal: Term variation detection \nThe aim of this tool is to detect terms variants from a set of previously known \nterms. These terms may be available from a reference database or a term \nacquisition software. What is crucial in this system that it is not needed to start \nfrom scratch every time. Optionally Fastr can also be used for TE. \nThe first step for applying Fastr is to obtain and analyse a set of existing terms \nand thus having a set of rules of a given grammar. The FASTR grammatical \n                                                           \n6 The formula was experimentally obtained by the autor from the association number described in \nBrown et al. (1988) in the aim of favouring the most frequent pairs: IM3=log2 (a3/(a+b)(a-b)) \n7 Further information can be obtained at http://www.limsi.fr/Individu/jacquemi/FASTR/index.html \n', 8, 0)

page suivante
(147.89999389648438, 175.99102783203125, 465.6802978515625, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  \n11 \n', 0, 0)
(134.63861083984375, 198.9710693359375, 463.1756591796875, 279.06744384765625, ' \nformalism is an extension of that of PATR-II (Shieber, 1986). A partial parser \nbased on the unification mechanism is responsible for the application of these \nrules. Term variants are obtained through a metarule mechanism that is \ndynamically calculated. \nFor instance, the term serum albumin corresponds to the Noun-Noun sequence \nand is associated with the following rule: \n', 1, 0)
(148.85890197753906, 279.1262512207031, 226.97999572753906, 292.10198974609375, 'rule 1: N1 → N2 N3 \n', 2, 0)
(163.01980590820312, 291.73101806640625, 269.084716796875, 326.5419921875, '<N1 lexicalization>= ‘N2’ \n<N2 lemma>=serum \n<N3 lemma>=albumin.  \n', 3, 0)
(134.64010620117188, 326.2310485839844, 463.1940612792969, 417.8473815917969, 'The value indicated by the feature “lexicalization” will be use just before partial \nparsing to selectively activate the target rules. Thus the above rule is linked to the \nword serum and so is activated when this word occurs in the sentence that is \nbeing parsed. \nAt a different level several metarules generate new rules in order to describe all \npossible variations of each term from the reference list. Each metarule presents a \nparticular structure and a specific pattern type. For instance, the following \nmetarule can be applied to the previous rule:  \n', 4, 0)
(197.76109313964844, 418.4927673339844, 399.1199951171875, 430.5089416503906, 'Metarule Coor(X1 Æ X2 X3) = X1 Æ X2 C4 X5 X3 \n', 5, 0)
(134.63980102539062, 430.4927673339844, 463.1778259277344, 580.3873901367188, 'which leads to the new rule: N1 Æ N2 C4 X5 N3 \nThis latter rule allows new constructions that substitute C4 for a conjunction and \nX5 for an isolated word such as serum and egg albumin. The candidate term is \nnot the whole new construction but the coordinated term (i.e., egg albumin). The \nwords that have given way to the new rule (egg and albumin) maintain their \nfunction of constricted equations of the original rule. Further, they are the \nanchoring point for the application of the metarule. A metarule can be associated \nwith specific restrictions, as for instance: (<C4 lemma>≠but) or (<X5 cat>≠ Dd). \nIn this way, those sequences with no lexical relationship such as serum and the \nalbumin are rejected.  \nThe above rule is a coordination rule and it should be noted that there are also \nother types of rules that account for different kinds of variations:  \n1. insertion rules:  \nmedullary carcinoma \nÎ medullary thyroid carcinoma \n', 6, 0)
(134.6387939453125, 580.7328491210938, 442.12353515625, 591.9075317382812, '2. permutation rules: control center   \n \nÎ center for disease control \n', 7, 0)
(134.6387939453125, 592.3311767578125, 463.25421142578125, 673.8819580078125, 'The FASTR metagrammar for English contains 73 metarules altogether: 25 \ncoordination rules, 17 insertion rules and 31 permutation rules. In any case, for \nefficiency reasons the new rules are dynamically generated. Each rule is linked \nto a pattern extractor that permits a very quick acquisition of information. As has \nbeen pointed out, the FASTR grammatical formalism is a PATR-II extension \n(Shieber, 1986). This language allows to write grammars using feature structures. \nThe rules describing terms are composed of a free-context part (N1 → N2 N3) and \n', 8, 0)

page suivante
(134.63999938964844, 175.99102783203125, 379.3233642578125, 187.08740234375, '12 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.63999938964844, 204.3707275390625, 463.29144287109375, 641.355712890625, 'a number of restriction equations (e.g. <N2 lemma>=serum). First, the system \nfilters the rules that are to be applied according to the given text and then an \nanalysis take place. \nWhen Fastr is applied for term acquisition the process is gradual: from a given \nset of terms the system detects new ones, which allows the beginning of a new \ncycle and the detection of new candidates. The loop goes on until new terms \ncannot be detected. The author presents an experiment carried out on a medicine \ncorpus of 1,5 million words and a reference list of 70,000 terms from different \nspecialised domains. After 15 cycles 17,000 terms were detected of which 5,000 \nwere new. The text was processed at a 2,562 word/minute speed. \nHowever, the number of recognised terms decreases when the reference list has \nfewer items. For instance, if the reference sublist of medicine drops to 6,000 \nterms, then only 3,800 new terms are recognised. \nThe author also postulates the existence of a conceptual relation. between the \nnew terms and the term that has led to their recognition. This relationship is \nvariable in accordance with the type of rule that is applied i.e., insertion or \ncoordination rule. Permutation does not allow any relationship due to the phrasal \nnature of the relationship. \nAll the language dependent data used by Fastr is stored in separated text files. \nThis feature facilitates the use of the system in other languages as showed by the \nrecent application of Fastr to Japanese, German and Spanish/Catalan. \nRecently Jacquemin has developed the detection of semantic variation using \nresources like WordNet or the Microsoft Word97 thesaurus (Jacquemin, 1999). \nEvaluation \nThe main characteristic of FASTR is its ability for detecting term variants, an \naspect often not considered by other systems. The fact of using already \nrecognised and accepted terms is very useful, although, as the author admits, it \nplaces restrictions on the acquisition of new terms that are not related to the \nsource terms. \nTE in Fastr implies that terms that are added to the list of valid terms after each \ncycle are not validated. Thus, a non-valid term may be added to the list so it is \nlikely that in forecoming cycles more non-valid terms are added. Jacquemin \n(1996) believes that this is not an important error source because the system, in \nsome way, corrects itself since “normally” non-correct candidates do not give \nway to new potential candidate terms. \nActually this technique should not be isolately applied. Rather, it should be \ncoordinated with other strategies as in (Jacquin & Liscouet 1996) and (Daille \n1998). \n', 1, 0)

page suivante
(147.89999389648438, 175.99102783203125, 465.6802978515625, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  \n13 \n', 0, 0)
(134.6405029296875, 198.9710693359375, 137.1455078125, 210.06744384765625, ' \n', 1, 0)
(134.63949584960938, 625.8721923828125, 280.843017578125, 636.9685668945312, '                                                          \n', 2, 0)
(134.6405029296875, 210.49102783203125, 189.94589233398438, 221.58740234375, '2.5. \nHEID \n', 3, 0)
(134.63949584960938, 222.01116943359375, 463.17437744140625, 348.0673828125, 'Reference publication: Heid et al. (1996) \nMain goal: Term extraction \nHeid et al. (1996) believe that automatic TE has various applications and \ndictionary or glossary construction would be the major one. In dictionary \nconstruction from computerized corpora two phases are distinguished: linguistic \npre-analysis and a term identification tool. Each of these phases requires specific \ncomputer tools. \nIn the linguistic pre-processing phase the following processes are required8: \na) tokenizing, which identifies word and sentence boundaries. \nb) morphosyntactic analysis, which identifies grammatical categories as well as \ndistributional and morphosyntactic features. \n', 4, 0)
(134.6394805908203, 348.49114990234375, 463.1803894042969, 567.3080444335938, 'c) POS tagging, which disambiguate morphosyntactic hypotheses. \nd) lemmatization, which identifies the lemma candidates. \nFor term identification the system has a general corpus retrieval interface that \nincludes a corpus query processor (CQP), a macroprocessor for the CQP query \nlanguage and a key word in context (KWIC) program, to extract and sort \nconcordances and lists of absolute and relative frequency of search items. \nTE is linked to a complex query language. The queries will be different \naccording to the types of candidate terms searched for. Thus, for instance, \nqueries about single-word terms are made from morphemes or typical \ncomponents of compound or derived words (derivatives). In these queries it is \nassumed that NP affixed terms from specialised languages use more specific \naffixes and/or prefixes than others. All the word sequence extracted (N-A, N-N, \nN-V), are based on POS patterns. \nHeid et al. (1996) have applied these tools to technical texts on automobile \nengineering in German, which amounts to 35,000 words. The sample has been \nmanually analysed before the application of the above procedures. The results \nare as follows: \n• \nWith regard to single-word terms, there has been found a 90% of candidate \nterms and a 10% of silence. This rate varies from one scheme to another. \n', 5, 0)
(134.6405029296875, 567.247314453125, 463.1636657714844, 625.5083618164062, '• \nWith regard to multiword terms, there are no concluding results. The results \nare less satisfactory and that the same problems as linguistic based are \nfound: POS patterns do not constrain enough the context and produce too \nmuch noise. Heid et al. (1996) believe that by using a syntactic parser, as it \nis the case in English, noise would diminish. \n', 6, 0)
(278.6400146484375, 625.8710327148438, 281.14501953125, 636.9674072265625, ' \n', 7, 0)
(134.63999938964844, 636.0458984375, 462.641845703125, 658.08740234375, '8 Heid et al. (1996) note that a broad coverage morphosyntactic parser for German is not attained. \nThus parser results are simulated using POS patterns.  \n', 8, 0)

page suivante
(134.63999938964844, 175.99102783203125, 379.3233642578125, 187.08740234375, '14 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.63999938964844, 203.94625854492188, 463.2378845214844, 227.64739990234375, '• \nFinally, collocation extraction is shown to produce noise but not silence, \nsince Heid et al. (1996) consider the frequency criterion. \n', 1, 0)
(134.64016723632812, 228.0711669921875, 463.22821044921875, 446.1673583984375, 'The Ahmad’s statistical measure (Ahmad et al, 1992) of relative frequency in \ncorpora of specialised and general language is applied to this corpus of 35,000 \nwords. They show that the results produced by linguistic corpus query are \nincluded in the output of statistical methods. However noise in statistical \nmethods is higher than in linguistic methods. \nEvaluation  \nTo tackle this system it should be taken into account the morphosyntactic \nfeatures of the German language. Unlike Romance languages, German prefers to \nform compounds in a synthesising manner. It means that what other languages \nexpress via terminological phrases in German is expressed with a single-word \nterm (by word is meant any segment found between two gaps). Thus it can be \nseen that in German automatic term detection does not depend much on term \ndelimitation but on the terminological nature of a word. This is the reason why \nwe need parameters to distinguish a term from a word of the general language, \nboth having the same morphosyntactic structure. \nLike most of the reviewed programs, Heid focuses on NP terms although it can \nalso extract collocations combining nouns and verbs. In this case Heid et al. \n(1996) note that the results are much worse. We do not have specific data about \nthe performance and the results of this system. \n', 2, 0)
(134.64019775390625, 458.53094482421875, 201.58285522460938, 469.6273193359375, '2.6. \nLEXTER \n', 3, 0)
(134.63980102539062, 470.0510559082031, 463.1832580566406, 666.7014770507812, 'Reference publication: Bourigault (1994) \nMain goal: Term extraction \nThis system has been developed in the need of the EDF (Electricité de France) \nsociety for improving their indexation system. LEXTER aims at locating \nboundaries among which potentially terminological NPs could be isolated. \nLEXTER carries a superficial analysis and makes use of the text heuristics in \norder to obtain those NPs of maximum length that it regards as candidate terms. \nThe program is composed of several modules and works as follows: \n1. Morphological analysis and disambiguation module. Texts receive information \nabout the POS and the lemma assigned to every word. \n2. Delimitation module. At this stage a local syntactic analysis is carried so as to \nsplit the text into maximal-length NPs. For example: alimentation en eau (water \nsupply), pompe d’extraction (extraction pump), alimentation electrique de la \npompe de refoulement (electric supply of the forcing back supply). Here the \nsystem takes advantage of the negative knowledge about the parts of complex \nterms. Thus those patterns of a potential term −finite verbs, pronouns and \nconjunctions− that will never become part of a term are identified and considered \n', 4, 0)

page suivante
(147.89999389648438, 175.99102783203125, 465.6802978515625, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  \n15 \n', 0, 0)
(134.64004516601562, 198.9710693359375, 463.18707275390625, 394.087158203125, ' \nas boundaries. Some of these patterns are simple whereas others are complex \n(sequences of preposition + determiner).  \nA French example of the latter would be SUR (prep) + LE (definite article): the \nmost common analysis is to propose that this sequence establishes a boundary \nbetween NPs as in: on raccorde le câble d’alimentation du banc sur le coffret de \ndécharge batterie. However there is a rate (10%) in which this sequence is part \nof the term: action sur le bouton poussoir de réarmement or action sur le \nsysteme d’alimentation de secours \nTo solve this and other similar situations, the system uses an endogenous \nlearning strategy of the patterns sub-categorisation. This strategy consists in \nlooking at the corpus to find those sequences of (noun) + sur + le having \ndifferent contexts on the right hand side. Then non-productive nouns are weeded \nout. Then sequences such as sur + le are considered sentence boundaries, except \nfor those cases wherein sequences are preceded by the productive noun located \nin the learning phase. To see how this system works let us suppose that at a first \nanalysis the sequences below are found. \n', 1, 0)
(285.8999938964844, 394.3996887207031, 362.30999755859375, 404.3664855957031, 'Le protection contre \n', 2, 0)
(296.159912109375, 404.719970703125, 362.3099365234375, 414.686767578125, 'Protection contre \n', 3, 0)
(141.60009765625, 415.040283203125, 362.3097839355469, 425.007080078125, 'il s’agit de maintenir la teneur en oxygène de cette eau dans \n', 4, 0)
(230.9998016357422, 425.41998291015625, 362.3097839355469, 445.70709228515625, 'on procède à l’injection d’eau dans \non procède à l’injection d’eau dans \n', 5, 0)
(183.72000122070312, 446.1199951171875, 362.31060791015625, 456.0867919921875, 'le système permet l’aiguillage des automates sur \n', 6, 0)
(362.8208923339844, 394.3996887207031, 458.47027587890625, 456.0867919921875, 'le gel est assurée par \nles grands froids \nles limites fixées \nles limites fixées \nles générateurs de vapeur \nle prélèvement effectué \n', 7, 0)
(134.63890075683594, 456.5510559082031, 463.2741394042969, 674.6472778320312, 'Then productive sequences are not regarded as term boundaries whereas non-\nproductive sequences are viewed as external boundaries of the candidate term. In \nthe example above protection contre and eau dans do not become boundaries \nwhereas automates sur does. \nThis strategy permits to detect a considerable amount of complex nouns which \notherwise would have been lost. Unfortunately it also allows a great deal of \nundesirable material (between 10% and 50%). \n3. Splitting module. NPs are analysed and their constituents are divided into head \nand expansion. For example the term candidate pompe d’extraction (extraction \npump) is splitted into: pompe –head– (pump) + extraction –expansion– \n(extraction).  \nAt this point the system may find ambiguous situations such as “Noun Adj1 \nAdj2” and “Noun1 Prep Noun2 Adj” whose analysis is uncertain. To solve these \ncases an endogenous learning process is followed which is similar to that \npresented in the delimitation module. \n4. Structuring module. The list of term candidates is organised in a \nterminological network. This network can be produced only by looking at a list \nof candidate terms and recognising the different parts of each candidate term, \nlike in the following example: \n', 8, 0)

page suivante
(134.63999938964844, 175.99102783203125, 379.32611083984375, 187.08740234375, '16 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(352.5, 283.3486328125, 393.87469482421875, 292.18585205078125, 'E expansion \n', 1, 0)
(348.7799987792969, 242.66864013671875, 390.1546936035156, 251.505859375, 'E expansion \n', 2, 0)
(353.6400146484375, 264.2086181640625, 378.486572265625, 273.04583740234375, 'N head \n', 3, 0)
(416.82000732421875, 288.80859375, 442.2874450683594, 297.64581298828125, 'electric \n', 4, 0)
(408.8399963378906, 264.4486083984375, 451.10894775390625, 273.28582763671875, 'forcing back \n', 5, 0)
(238.97999572753906, 277.108642578125, 340.94970703125, 292.22650146484375, 'head N’ \nelectric supply \n', 6, 0)
(348.659912109375, 217.34832763671875, 390.03460693359375, 226.185546875, 'E expansion \n', 7, 0)
(208.5, 251.66864013671875, 251.91436767578125, 260.505859375, 'E’ expansion \n', 8, 0)
(209.76010131835938, 235.40863037109375, 234.60662841796875, 244.245849609375, 'N head \n', 9, 0)
(222.24000549316406, 216.568603515625, 247.08653259277344, 225.40582275390625, 'N head \n', 10, 0)
(418.0799865722656, 235.108642578125, 441.4350891113281, 243.94586181640625, 'supply \n', 11, 0)
(412.44000244140625, 211.108642578125, 446.44281005859375, 219.94586181640625, 'extraction \n', 12, 0)
(151.32000732421875, 270.73968505859375, 229.7562255859375, 291.206787109375, 'electric supply of the \nforcing back pump \n', 13, 0)
(260.8800048828125, 234.6796875, 331.23028564453125, 244.646484375, 'forcing back pump \n', 14, 0)
(170.22000122070312, 211.33966064453125, 331.409912109375, 221.92584228515625, 'extraction pump \npump \n', 15, 0)
(134.63999938964844, 302.23101806640625, 463.2380676269531, 669.7875366210938, 'Additionally Lexter calculate some productivity figures based on links type \noccurrences. These coefficients do not become filters, but are passed on to the \nterminologist as a piece of data so as to facilitate the evaluation of candidate \nterms. \n5. Navigation module. A consulting interface is built (called terminological \nhypertext) from the source corpus, the candidate term network and the above-\nmentioned coefficients and lists. \nAlthough LEXTER is exclusively based on linguistic techniques it produces \nhighly satisfying results and is currently used to exploit different corpora from \nEDF and different research projects. Besides it has been proved helpful in: text \nindexation, hypertextual consulting of technical documentation, knowledge \nacquisition and construction of terminological databases. \nLEXTER is also used as a terminology extractor in the terminological \nknowledge base designed by the Terminologie et Intelligence Artificielle \n(Terminology & Artificial Intelligence) terminology group. SYCLADE (Habert, \n1996), a tool for word classification also makes use of LEXTER. \nEvaluation \nLEXTER was born in an industrial environment and from the very beginning it \nsought a robust, accurate and domain-independent tools. These objectives were \nbasically attained although mark-up and disambiguation errors weaken the \ncapacity of the system. Some scholars note that this system (like those which \nmake use of symbolic techniques) produce a considerable amount of noise. Thus \nof a corpus of 200,000 words there are obtained 20,000 candidate terms which, \nafter the validation stage, amount to 10,000. Also, Bourigault stresses the silence \nproblem, which he estimated around 5% of the total valid terms. Like the vast \nmajority of systems, LEXTER only focuses on NPs since verbs are believed to \nbe term boundary and so they are never part of candidate terms. \nOne of the most remarkable achievements of this system is the endogenous \nlearning mechanism that allow to work autonomously and so there is no need for \na complex and large dictionary. In a similar vein it should be highlighted the \nusefulness of presenting the results hypertextually, since it facilitates the \nterminologist’s task. \n', 16, 0)

page suivante
(147.89999389648438, 175.99102783203125, 465.6802978515625, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  \n17 \n', 0, 0)
(134.6405029296875, 198.9710693359375, 216.64720153808594, 221.58740234375, ' \n2.7. \nNAULLEAU \n', 1, 0)
(134.64048767089844, 222.01116943359375, 463.30047607421875, 371.1085205078125, 'Reference publication: Naulleau (1998) \nMain goal: Noun phrase filtering system \nThe model designed by Naulleau is a NP extraction system that proposes as term \ncandidates those sequences that comply with certain user tailored profile. The \nwhole process can be divided in two main stages: profile acquisition and profile \napplication. \nTo define its own profile the user chooses the set of phrases that s/he considers \nrelevant for his task and discards the ones that s/he does not consider useful at \nthat time. The data collected in such way is generalised according to their \nmorphological, syntactical and semantic characteristics dynamically creating a \nset of positive and negative filters. A simple example of positive and negative \nfilters is the following: \n(1) positive filter: \nmetallic/automatic/nuclear/industrial taps \n', 2, 0)
(134.6405029296875, 371.4720458984375, 394.0653076171875, 382.56842041015625, '(2) negative filter: \nimportant/recent/necessary/unreliable taps \n', 3, 0)
(134.63946533203125, 382.9921569824219, 463.2263488769531, 658.568359375, 'Then, those filters produced in the learning stage are applied to new sequences \nanalysed. As a result, some noun arguments and/or PPs can be eliminated. Thus a \nNP can be divided or reduced and the resulting sequences are passed on to an \nexpert to be evaluated.  \nIn doing so the author acknowledges the sociolinguistic nature of the term. It \nimplies that there is no linguistic model that can tell whether a NP is a term or \nnot beyond the scope of a field or even the application. Also, this procedure \nintroduces the idea of how relevant a phrase is in relation to the interest profile of \nthe user and assumes that such relevance may be evaluated on linguistic grounds. \nThis is a fully symbolic approach that uses the AlethIP engine that produces \nsentences fully lemmatised, tagged and syntactically parsed. Then nouns and \nadjectives are semantically tagged according to both suffix information and \nsemantic data from AlethIP and using a set of contextual rules for the more \nfrequent and ambiguous words. The whole strategy is based on the evaluation of \nthe relevance of simple syntactic dependencies. Such relevance is only based on \nthe data provided by the user. \nAccording to the author, the results are encouraging. However it is difficult to \nevaluate due to the practical problem posed by such a detailed evaluation. Some \nadditional experiments are described in (Nalleau, 1999). \nEvaluation \nThis system may be considered the first one to use semantic data as a specific \nresource for proposing term candidates. Also, as far as we know, is the first time \nsince the very beginning in the design of a TE systems that the user and the idea \nof relevance to an application are taken into account. \n', 4, 0)

page suivante
(134.63999938964844, 175.99102783203125, 379.3233642578125, 187.08740234375, '18 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.63998413085938, 204.3707275390625, 463.14959716796875, 238.447021484375, 'In this way the user may adapt the system to its specific needs but also its \nintervention may crucially affect the performance of the system. The loss of \nspecific data makes difficult to evaluate the tool behaviour in an actual context. \n', 1, 0)
(134.63999938964844, 250.8707275390625, 203.8070831298828, 261.96710205078125, '2.8. \nNEURAL \n', 2, 0)
(134.63999938964844, 262.3316650390625, 463.23150634765625, 664.5084228515625, 'Reference publication: Frantzi and Ananiadou (1995)  \nMain goal: Term extraction \nNeural is a system for TE of a hybrid nature, that is it uses both linguistic \n(morphosyntactic patterns and a list of suffixes specific to the domain) and \nstatistically knowledge (frequency and mutual information). Frantzi & \nAnaniadou (1995) pays special attention to two different problems: detection of \nnested terms and detection of low frequency terms using statistical methods. \nThe test bench is a corpus of 55,000 words in the domain of medicine \n(ophthalmology). The structures analysed are Noun-Noun and Adjective-Noun \nthat are identified using a standard tagger. The list of suffixes includes those \nfrequently found in terminological units in the field of ophthalmology like -oid, -\noma, -ium. The system is implemented using a Back-Propagation (BP) two \nlayers neural network. The threshold has been set to .5 but this may vary. The BP \nneural network has been trained with a set of 300 compounds and the tests were \nmade with another set of 300 words. It obtained a success rate of 70 %. \nThe author and other scholars from the Manchester Metropolitan University have \nbeen active since 1995 developing specific statistical figures for TE. In this way \nit is necessary to mention those tasks related to the adding of context information \n(Frantzi, 1997, Maynard & Ananiadou, 1999). Usually the context is discarded \nor, alternatively, considered as a bag of words although its relevance is signalled \nby many scholars. Here the basic assumption is that terms tend to appear \ngrouped in real text, so the termhood figure of a candidate would increase if \nthere are other terms (or candidates highly ranked) in the context. \nBoth Frantzi, 1997 and Maynard & Ananiadou, 1999 propose a similarity figure \nbased on the distance between the candidate and the context words (nouns, \nadjectives and verbs). This figure is calculated by Frantzi (1997) using statistical \nand syntactic information while Maynard & Ananiadou (1999) include also \nsemantic information from a specialised thesaurus (UMLS semantic Network).  \nIn Maynard & Ananiadou (1999) this similarity figure may also be used to take \ninto account some kind of semantic disambiguation for the sense that gets a \nbetter value. A context factor (CF) is added to the figure already used to rank the \ncandidates (Cvalue) and thus reordering the set of candidates as follows: \nSNCvalue(a) = 0.8*Cvalue(a) + 0.2*CF(a). The authors report improvements in \nthe ranking of term candidates from his eye pathology corpus. \nEvaluation \n', 3, 0)

page suivante
(147.89999389648438, 175.99102783203125, 465.6802978515625, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  \n19 \n', 0, 0)
(134.64048767089844, 198.9710693359375, 463.23040771484375, 325.0875244140625, ' \nThe original system can be seen as a standard hybrid system. The linguistic \nknowledge includes Greek and Latin affixes and morphosyntactic patterns. The \nincorporation of this kind of suffixes should be highly productive. However the \nchosen patterns may well apply to English but not to Romance Languages. \nThe incorporation of the context as part of the data available for evaluating the \ntermhood of a candidate is a very interesting contribution to the behaviour of \nterms in real texts. It should also serve to increase the relevance of low frequency \ncandidates but no specific figure is given. \nIt is necessary to mention the use of semantic information as a kind of resource \nthat is increasingly used in the TE field. \n', 1, 0)
(134.6405029296875, 337.5111389160156, 229.01791381835938, 348.6075134277344, '2.9. \nNODALIDA-95 \n', 2, 0)
(134.63999938964844, 348.97113037109375, 463.254638671875, 485.3064880371094, 'Reference publication: Arppe (1995) \nMain goal: Term extraction \nNODALIDA, a product designed by the Lingsoft firm, is based on an enhanced \nversion of NPtool that is a program developed at the Department of General \nLinguistics at the Helsinki University (Finland). NPtool (Voutilanen 1993) \ngenerates lists of NPs occurring in the sentences of a text and provides an \nassessment about whether these phrases are candidates terms or not (ok/?). From \nthese lists all the acceptable sub-chains are obtained. Besides, the source list is \nmultiplied. Let us see an actual example, for the sentence: “exact form of the \ncorrect theory of quantum gravity“ NPtool proposes the following additional \nNPs: \nform of the correct theory of quantum gravity  \nform  \n \ncorrect theory \n', 3, 0)
(134.63999938964844, 485.7193908691406, 414.54547119140625, 495.6861877441406, 'exact form of the correct theory \n \n \n \nexact form   \ngravity \n', 4, 0)
(134.63999938964844, 496.0396728515625, 448.3204345703125, 506.0064697265625, 'form of the correct theory \n \n \n \n \ntheory \n \nquantum gravity \n', 5, 0)
(134.63909912109375, 506.531005859375, 463.1982727050781, 655.626953125, 'Simultaneously there are a number of premises that become the first filter like in \nthe following: “Those NPs preceded by a determiner, adjective or prefixed \nsentence (kind of, some, one, ...) are weeded out.” \nAs for the remaining NPs, their occurrence frequency is calculated. Further, they \nare ordered and grouped according to their grammatical head and are presented \nto the terminologist together with their context. The NPtool module (Voutilanen, \n1993) is at the heart of the system. It is a NP detector largely based on the \nconstraint grammar formalism (Karlsson, 1990). Its main features are: (1) \nMorphological/syntactical descriptions are based on a large set of hand-coded \nlinguistic rules, (2) both the grammar and the lexicon allow a corpus analysis \nwith non-controlled text and (3) disambiguation is made according to only \nlinguistic criteria. As a result, between 3% and 6% of the words remain \nambiguous. \n', 6, 0)

page suivante
(134.63999938964844, 175.99102783203125, 379.3233642578125, 187.08740234375, '20 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.63999938964844, 204.3707275390625, 463.1900329589844, 248.72650146484375, 'The text goes through a previous process so as to determine sentence boundaries, \nidiomatic expressions, compound forms, typographical signs, etc. Then it is \nmorphologically analysed and a result like this is obtained9: \n(“<*the>”  \n(“the” DET CENTRAL ART SG/PL (@>N))) \n', 1, 0)
(134.63999938964844, 249.0799560546875, 278.70758056640625, 259.0467529296875, '(“<inlet>”  \n(“inlet” N NOM SG)) \n', 2, 0)
(134.63999938964844, 259.45965576171875, 272.7603759765625, 269.42645263671875, '(“<and>”  \n(“and” CC (@CC))) \n', 3, 0)
(134.63999938964844, 269.77996826171875, 389.5308837890625, 279.74676513671875, '(“<exhaust>” \n(“exhaust” <SVO> V SUBJUNCTIVE VFIN (@V)) \n', 4, 0)
(134.63999938964844, 280.15966796875, 345.14813232421875, 290.12646484375, ' \n \n \n(“exhaust” <SVO> V IMP VFIN (@V)) \n', 5, 0)
(134.63999938964844, 290.47998046875, 297.358154296875, 300.44677734375, ' \n \n \n(“exhaust” <SVO> V INF) \n', 6, 0)
(134.63999938964844, 300.85980224609375, 371.9476318359375, 310.82659912109375, ' \n \n \n(“exhaust” <SVO> V PRES -SG3 VFIN (@V)) \n', 7, 0)
(134.63999938964844, 311.1800537109375, 357.2064208984375, 331.52655029296875, ' \n \n \n(“exhaust” N NOM SG)) \n(“<manifold>” (“manifold” N NOM PL)) \n', 8, 0)
(134.63897705078125, 331.9910583496094, 463.2038269042969, 477.5064697265625, 'At this moment disambiguation takes place. For example in the sentence: ”The \ninlet and exhaust manifolds are mounted on opposite sides of the cylinder head“ \ntwo analyses are obtained: \n(1) on/@AH opposite/@N sides/@NH of/@N< the/@>N cylinder/@NH head/@V \n(2) on/@AH opposite/@N sides/@NH of/@N< the/@>N cylinder/@>N head/@NH \nWhat distinguishes these two analyses is the consideration of whether the final \nsequence (cylinder head) is a NP or not. The ongoing process gives only two \npossible analyses for each sentence. First, those NPs of a maximal length are \npreferred (NP-friendly) and, second, those NPs of a minimal length are preferred \n(NP-hostile). Then the system compares both strategies and labels each NP as \nok/? by considering whether the analysis is shared or not by both strategies \nThus the last sentence gets this analysis below: \n(3) \nok : inlet and exhaust manifolds \n \n?: opposite sides of the cylinder \n', 9, 0)
(155.63949584960938, 477.91949462890625, 438.9333801269531, 487.88629150390625, 'ok: exhaust manifolds  \n \n \n?: opposite sides of the cylinder head \n', 10, 0)
(134.63992309570312, 488.3510437011719, 463.16705322265625, 665.0260620117188, 'In order to validate this additional information the terminologist is provided with \na list of candidate terms. The results reported by the NPtool module are pretty \ngood (precision=95-98% and recall=98.5-100%) with a text of about 20 Kwords. \nEvaluation \nNODALIDA is based on the use of linguistic knowledge through a structural \napproach (i.e., detection of phrasal structures and structural disambiguation). \nArppe (1995) presents high-quality results. However, the corpus should be \nenlarged, since so far tests have been made on quite small corpora. It is not clear \nhow precision and recall figures are calculated, particularly how to determine \nwhich terms are deemed to be correct (i.e., those which have the ok signal or all \nof them). Also it should be stressed that NODALIDA has not been tested using \nthe NPtool enhanced version in an actual situation of terminology problems. \n                                                           \n9 The meaning of the syntactic function tags are: @>N = pre-modifier; @<N = post-modifier; @CC \nand @CS= coordination and subordination conjunction; @V = Verb; @NH = nominal head. Finally, \n“>” and “<” indicate the direction of the phrasal head. \n', 11, 0)

page suivante
(147.89999389648438, 175.99102783203125, 465.6802978515625, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  \n21 \n', 0, 0)
(134.64048767089844, 198.9710693359375, 463.1853942871094, 313.9957275390625, ' \nTaking into account that the disambiguator is one of the main error sources in \nthis kind of systems, Arppe (1995) believes that a high-degree quality is achieved \ndespite the fact that there are no data about terminology extraction in real \nsituations. Besides, to achieve this quality NODALIDA proposes a great deal of \nrules, which yields management and control overhead. \nThe list that is passed on to the terminologist to be validated comprises those \ncandidates signalled with ok and ?. The way in which potential NPs are obtained \nby the system leads us to suspect that there are many candidate terms in the \nvalidation list that the terminologist has to analyse. \n', 1, 0)
(134.63980102539062, 325.9910583496094, 463.34588623046875, 659.0473022460938, '2.10. TERMIGHT \nReference publication: Dagan and Church (1994) \nMain goal: Translation aid \nTermight is currently used by A&T Business Translation Systems. It was created \nto be a tool for automating some stages of the professional translator \nterminological research.  \nTo do so it starts with a tagged and disambiguated text as well as a list of \npredetermined syntactic patterns that could be adjusted to every document. Thus, \na list of candidate terms is obtained comprising one or more words. Single-word \ncandidates are defined as all those words that are not included in a previously \ndetermined list of empty words (i.e. stop list). Multiword terms are referred to \none of the predetermined syntactic patterns via regular expressions. Dagan and \nChurch (1994) considered only noun sequences patterns. \nCandidate terms are grouped and classified according to their lemma (i.e. the \nright hand side noun) and frequency. Those candidates sharing the same lemma \nare classified alphabetically in accordance with the inverse order of their \ncompounding words. Thus it is showed the order of changes of the English \nsimple NPs. \nFor each candidate term the corresponding concordances are obtained, which are \nalphabetically classified according to their context. This information enables the \nterminologist to evaluate whether each candidate is appropriate or not. \nDagan and Church (1994) note that the rate of term list construction is of 150 \nand 200 terms per hour, which is twice faster than the average. As for the \nextraction quality, they state that, unlike exclusively statistical methods, \nTermight permits to extract low-frequency terms. \nMoreover this system has a bilingual module which, via statistical methods, \nobtains a word-level alignment from texts. Thus terms found in language A are \nreferred to their counterparts in language B. This well-ordered list of candidate \nterms is again passed on to the terminologist to be evaluated. \n', 2, 0)

page suivante
(134.63999938964844, 175.99102783203125, 379.3233642578125, 187.08740234375, '22 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.63999938964844, 204.3707275390625, 463.25, 376.8757019042969, 'The Termight bilingual module does not seem to be developed and tested as the \nbasic one. Tests have been made on 192 terms from a technical manual in \nEnglish and German. The correct translation is found in the first suggested \nsolution in 40% of the cases, whereas only 7% corresponds to the correct \ntranslation suggested in the second place. As for the remaining, the correct \ntranslation was in other places of the proposal list. \nEvaluation \nTermight is a remarkable system in that there is an accurate classification and \npresentation of candidate terms and it does not attempt to become an automatic \nsystem. Rather, it helps the translator. \nHowever, it presents a number of shortcomings: (1) The only syntactic pattern \nconsidered is very simple: noun sequences. This pattern may well be valid for \nEnglish but not for Romance languages and (2) no numerical information about \nthe recognition quality is given. The type of pattern considered may suppose \nhigh precision but low recall \n', 1, 0)
(134.63999938964844, 388.81103515625, 463.3059997558594, 664.38720703125, '2.11. TERMINO \nReference publication: Plante and Dumas (1989) \nMain goal: Facilitation of the term extraction terminographer’s task. \nThe TERMINO program is composed of several tools to facilitate TE in French. \nIt is a help for the terminologist insofar as the identification of those discourse \nunits that denominate notions or objects. Besides it provides every unit with the \nimmediate context from which data relevant to the notions denominated by \ntheses units can be obtained. There are a number of TERMINO versions which \nimprove in some ways previous ones. \nThis tool is based mainly on linguistic knowledge and it comprises 3 sub-\nsystems: a pre-editor, which separates texts into words and sentences and \nidentifies proper nouns, a morphosyntactic parser and a record-drafting facility. \nThe text does not get any special treatment: it is only required to be codified in \nASCII form. \nWith regard to term delimitation and extraction the more interesting sub-system \nis the morphosyntactic parser. It consists of 3 modules: a morphological parser; a \nsyntactic parser and a synapsy detector.  \nThe morphological parser has two functions: a) automatic categorisation; b) \nlemma and tag identification. According to Plante and Dumas, 30% of words in \nFrench can be attributed to more than one category. This has led to the tagging of \nall the possible categories for each word. As a result, there is an overproduction \nof words with different tags. Categorisation and lemmatisation are obtained from \nthe application of the LCML program, it is not a dictionary but a morphological \nparser of lexical forms so it can correctly lemmatise and tag new lexical forms. \n', 2, 0)

page suivante
(147.89999389648438, 175.99102783203125, 465.6802978515625, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  \n23 \n', 0, 0)
(134.64048767089844, 198.9710693359375, 463.2394104003906, 578.04736328125, ' \nThe syntactic parser is responsible for weeding out the vast majority of \nambiguities generated in previous stages. It is managed through the construction \nof a syntactic structure for each sentence. \nFinally, the synapsy detector (MRSF) selects, among the syntactic units from the \nparser, those lexical noun units that are likely to be terms. S. David (David and \nPlante, 1991) created MRSF especially for TERMINO. MRSF is based on \nprinciples of noun group construction. David’s understanding of synapsy is that \nof a polylexical unit of a syntactic nature that is the head of the NP. Thus, \nsynapsies are only NPs groups: some of them will become terms and some of \nthem will not. Further, some of them will only be “topics” that will enable the \nterminologist to know different concepts or grasp an overview of the text topics. \nThe MRSF module comprises 5 sub-modules: (1) head hunter module; (2) \nexpansion recogniser module; (3) categorisation module; (4) synapsy generator \nmodule and (5) representation and evaluation module. \nTERMINO has a set of software tools, which is much larger and comprises \ndifferent modules that allow to manipulate terminological data. These tools help \nthe terminologist decide whether a synapsy is a term or not, elaborate \nterminological filing forms and create terminological databases. \nTERMINO recognises between 70% and 74% of the complex terms. The fact \nthat 30% of terms are not recognised by TERMINO can be explained by \ncoordination (it is a signal of segment breaking), acronyms and common nouns \nin capital letters. Moreover, there is 28% of noise, of which 47% is due to a \nwrong mark-up and a 53% is due to synapsies belonging to general language. \nEvaluation \nTERMINO is one of the first candidate term extractors that worked and it is a \nlinguistically-based extractor, composed of different independent modules. This \nsystem is based on the concept of synapsy. The synapsy detector is based on the \nestablishment of a number of heuristic rules that may well be increased provided \nthe corpus is delimited. \nThere is a need to improve this system taking into account that it is still too noisy \n(28%), which could be improved, for example, with a different treatment of \ncapital letters and acronyms. \n', 1, 0)
(134.6405029296875, 590.4711303710938, 463.214599609375, 670.5674438476562, '2.12. TERMS \nReference publication: Justeson and Katz (1995) \nMain goal: Term extraction \nJusteson and Katz (1995) hold the following views about terms: \na) Terminological noun phrases (TNP) are different from non-terminological \nnoun phrases (nTNP) in that the modifiers of the first ones are much shorter than \nthose of the second ones. \n', 2, 0)

page suivante
(134.63999938964844, 175.99102783203125, 379.3233642578125, 187.08740234375, '24 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.6399688720703, 204.3707275390625, 463.3431091308594, 663.9072875976562, 'b) An entity introduced by a nTNP can be later referred to only by the head of the \nNP and often by other NP (synonyms, hyponyms, hyperonyms). By contrast, an \nentity introduced by a TNP is normally repeated identically in a given document, \nas a single omission of a modifier could yield a change of the referred entity. \nc) In technical texts lexical NPs are almost exclusively terminological. \nd) Multiword technical terms are nearly always composed of nouns and \nadjectives (97%) and some prepositions (3%) between two NPs. \ne) The average length of a TNP is of 1.91 words. \nThe proposed filter finds strings with a frequency equal or higher than two. \nThese strings follow with this regular expression: ((A|N)+ | ((A|N)*(N \nP)?)(A|N)*N. Those candidate terms of a length of 2 (2 patterns: AN and NA) \nand 3 (5 patterns: AAN, ANN, NAN, NNN and NPN) are by far the most \ncommonly encountered. \nThe purpose of this algorithm is to combine good coverage of the usual \nterminology from technical texts with high quality in the extraction phase. The \nalgorithm prefers quality to coverage, since if it only made use of the \ngrammatical constraints then the system would propose many irrelevant NPs. \nThe vast majority of relevant NPs overcome the frequency constraint. \nSelection of grammatical patterns also affects quality. If prepositions are \nadmitted within the pattern many candidates are introduced, although few will be \nvalid. As a result, quality decreases whereas quantity increases and, accordingly, \nJusteson and Katz (1995) prefer not to take prepositions into consideration. \nThe implementation of grammatical patterns also affects the quality/coverage \ntrade-off. There are two ways in which a given linguistic unit is attributed to a \ngrammatical category: disambiguation and filtering. The first one is rejected \nbecause disambiguators are not totally reliable yet. \nFiltering consist in parsing and lemmatising each word of the text. Then those \nsequences following the pattern are considered. If a word is not identified as a \nnoun, adjective or preposition, it is discarded. Thus each word maintains its \nnominal, adjectival and prepositional values and in this order. The chain is \nweeded out if more than one word can be identified as a preposition or if it does \nnot follow the pattern (e.g. if the pattern ends with a noun and there is more than \none preposition then the word following the preposition is not a noun). \nFiltering has a coverage at least as good as what can be attained by a standard \ntagger. However, quality is not that good (e.g. fixed is only identified as an \nadjective –bug fixed–, but it can also become a verb: fixed disk drive). In \ncontrast, filtering is much faster than parsing.  \nIn any case, Justeson and Katz (1995) suggest to control the patterns, the list of \ngrammatical words and the frequency to adjust the performance of the system to \neach type of text. \n', 1, 0)

page suivante
(147.89999389648438, 175.99102783203125, 465.6802978515625, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  \n25 \n', 0, 0)
(134.6405029296875, 198.9710693359375, 463.2592468261719, 371.107177734375, ' \nThis system has been applied to different domains (metallurgy, spatial \nengineering and nuclear energy) and it is used at IBM Translation Center. The \nTERMS results are presented on the basis of 3 technical texts (statistical \nclassification of patterns, lexical semantics and chromatography). Coverage has \nonly been estimated for one of the text and it is of 71%. Quality has been \nestimated between 77% and 96% of the instances. \nEvaluation \nAlthough Justeson and Katz (1995) present a detailed study on the performance \nof terminological units (wherein there are some overstatements), the proposed \nfilter does not seem to take advantage of these previous analyses of terms. \nFurther, it should be noted that this type of filtering based on quite simple \npatterns would not be so efficient if they were applied to languages other than \nEnglish such as Romance languages. Also, this kind of patterns produces a lot of \nnoise. \n', 1, 0)
(134.6405029296875, 395.5911560058594, 246.58697509765625, 406.6875305175781, '3. \nContrastive Analysis \n', 2, 0)
(134.6405029296875, 406.9908447265625, 463.2036437988281, 475.5669250488281, 'Here we will contrast the systems’ main features, according to six relevant \naspects when designing a new detection system of terminological units: \nlinguistic resources, strategies of term delimitation, strategies of term filtering, \nclassification of recognised terms and obtained results. For some of these criteria \nwe have created a table containing the most significant data so as to make the \nsystem comparison easier. \n', 3, 0)
(134.6405029296875, 487.9908447265625, 246.85745239257812, 499.08721923828125, '3.1. \nLinguistic resources \n', 4, 0)
(134.6405029296875, 499.4508361816406, 463.2774963378906, 614.0471801757812, 'It has been observed that the vast majority of the reviewed systems make use of \nsome sort of linguistic information, at least a list of empty words taken as \nboundaries. The standard process includes a morphological analysis followed by \nsome kind of disambiguation system. The systems altering this procedure are the \nfollowing: \na. ANA: does not use any linguistic resource, just a list of auxiliary words \nb. TERMS: use its own disambiguation system: POS filtering \nc. Naulleau: introduces semantic information  \nAdditionally, for the systems that use an incremental strategy, like ANA and \nFastr, it is necessary a set of initial terms to bootstrap the process. \n', 5, 0)
(134.6405029296875, 626.470947265625, 288.2491455078125, 637.5673217773438, '3.2. \nStrategies of term delimitation \n', 6, 0)
(134.6405029296875, 637.990966796875, 463.1824645996094, 672.0671997070312, 'All systems of terminology extraction have to determine at some point the \nbeginning and the end of the candidate term, that is, delimit the potential \nterminological unit. The reviewed programs have different strategies to delimit \n', 7, 0)

page suivante
(134.63999938964844, 175.99102783203125, 379.3233642578125, 187.08740234375, '26 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.63999938964844, 204.3707275390625, 463.1258850097656, 261.4271240234375, 'terms: word-boundary elements, structural patterns, syntactic parser, text \ndistribution, typographical elements, term lists, structure disambiguation. Below \nwe show a summary of the different options adopted by each system: \n \nTable 1: Strategies of term telimitation \n', 1, 0)
(148.02000427246094, 262.52862548828125, 433.1414794921875, 271.3658447265625, 'System \nterm delimitation \nstructure disamb. \n', 2, 0)
(148.02000427246094, 272.48858642578125, 431.8815002441406, 281.3258056640625, ' \nName /Author \nboundaries \nPatterns \nParser \nOther \nlearning \nOther \n', 3, 0)
(148.02000427246094, 282.4486083984375, 416.354248046875, 291.28582763671875, '1 \nANA \n \n \n \nX \n \n- \n', 4, 0)
(148.02000427246094, 291.568603515625, 443.9276428222656, 300.40582275390625, '2 \nCLARIT \n \n \nX \n \n \nstatistical \n', 5, 0)
(148.02000427246094, 300.80859375, 416.36181640625, 309.64581298828125, '3 \nDaille \n \nX \n \n \n \n- \n', 6, 0)
(148.02000427246094, 309.9888916015625, 416.3600158691406, 318.82611083984375, '4 \nFASTR \n \nX \nX \nX \n \n- \n', 7, 0)
(148.02000427246094, 319.16912841796875, 416.3496398925781, 328.00634765625, '5 \nHeid \n \nX \n \n \n \n- \n', 8, 0)
(148.02000427246094, 328.40911865234375, 413.7210998535156, 337.246337890625, '6 \nLEXTER \nX \n \n \n \nX \n \n', 9, 0)
(148.02000427246094, 337.5893249511719, 416.35333251953125, 346.4265441894531, '7 \nNaulleau \n \n \n \nX \n \n- \n', 10, 0)
(148.02000427246094, 346.7696228027344, 416.36273193359375, 355.6068420410156, '8 \nNEURAL \n \nX \n \n \n \n- \n', 11, 0)
(148.02000427246094, 356.0096130371094, 416.37762451171875, 364.8468322753906, '9 \nNODALIDA-95  \n \n \nX \n \n- \n', 12, 0)
(148.02000427246094, 365.1898193359375, 416.3584899902344, 374.02703857421875, '10 \nTermight \n \nX \n \n \n \n- \n', 13, 0)
(148.02000427246094, 374.3701171875, 416.35015869140625, 383.20733642578125, '11 \nTERMINO \n \n \nX \n \n \n- \n', 14, 0)
(148.02000427246094, 383.610107421875, 416.35028076171875, 392.44732666015625, '12 \nTERMS \n \nX \n \n \n \n- \n', 15, 0)
(134.63999938964844, 405.6110534667969, 272.0722961425781, 416.7074279785156, '3.3. \nStrategies of term filtering \n', 16, 0)
(134.63999938964844, 417.13104248046875, 463.2025451660156, 451.207275390625, 'Term filtering is a key stage of any term detection system. This means that the \nlist of candidates is reduced as much as possible. The following table shows the \nstrategies found in all the reviewed systems: \n', 17, 0)
(148.8603973388672, 451.6311340332031, 151.3654022216797, 462.7275085449219, ' \n', 18, 0)
(134.63999938964844, 463.0486145019531, 249.1346435546875, 471.8858337402344, 'Table 2: Strategies of term filtering \n', 19, 0)
(146.5800018310547, 473.00860595703125, 272.5235290527344, 481.8458251953125, 'System \nTerm Filtering \n', 20, 0)
(146.5800018310547, 482.1624450683594, 285.42138671875, 500.98583984375, ' \nName /Author \nFreq.10 \nLinguisti\nc \n', 21, 0)
(295.20001220703125, 482.9686279296875, 333.84307861328125, 500.98583984375, 'statistical + \nlinguistic \n', 22, 0)
(340.0799865722656, 482.9686279296875, 378.3704528808594, 500.98583984375, 'linguistic + \nstatistical \n', 23, 0)
(384.7799987792969, 482.9686279296875, 416.490966796875, 500.98583984375, 'reference \nterms \n', 24, 0)
(422.3999938964844, 482.9686279296875, 448.4722595214844, 500.98583984375, 'user \ndefined \n', 25, 0)
(146.5800018310547, 502.1086120605469, 424.3992919921875, 510.9458312988281, '1 \nANA \n \n \n \n \nX \n \n', 26, 0)
(146.5800018310547, 511.28857421875, 424.41168212890625, 520.1257934570312, '2 \nCLARIT \n \n \nX \nX \n \n \n', 27, 0)
(146.5800018310547, 520.4688110351562, 424.4145812988281, 529.3060302734375, '3 \nDaille \n \n \n \nX \n \n \n', 28, 0)
(146.5800018310547, 529.6491088867188, 424.3909912109375, 538.486328125, '4 \nFASTR \n \n \n \n \nX \n \n', 29, 0)
(146.5800018310547, 538.88916015625, 424.40069580078125, 547.7263793945312, '5 \nHeid \n \nX \n \n \n \n \n', 30, 0)
(146.5800018310547, 548.0693359375, 424.3996887207031, 556.9065551757812, '6 \nLEXTER \n \nX \n \n \n \n \n', 31, 0)
(146.5800018310547, 557.2496337890625, 430.17083740234375, 566.0868530273438, '7 \nNaulleau \n \n \n \n \n \nX \n', 32, 0)
(146.5800018310547, 566.4896240234375, 424.4101867675781, 575.3268432617188, '8 \nNEURAL \n \n \n \nX \n \n \n', 33, 0)
(146.5800018310547, 575.6697998046875, 424.4273986816406, 584.5070190429688, '9 \nNODALIDA-95  \nX \n \n \n \n \n', 34, 0)
(146.5800018310547, 584.85009765625, 424.3870849609375, 593.6873168945312, '10 \nTermight \nX \nX \n \n \n \n \n', 35, 0)
(146.5800018310547, 594.090087890625, 424.3960876464844, 602.9273071289062, '11 \nTERMINO \n \nX \n \n \n \n \n', 36, 0)
(146.5800018310547, 603.2703247070312, 424.406494140625, 612.1075439453125, '12 \nTERMS \nX \nX \n \n \n \n \n', 37, 0)
(134.6398468017578, 613.3310546875, 462.6451110839844, 642.765869140625, '                                                           \n10 The technique of term filtering through frequency terms has been considered something in between \nthose methods based on linguistic knowledge and those methods based on extralinguistic knowledge. \n', 38, 0)

page suivante
(147.89999389648438, 175.99102783203125, 465.6802978515625, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  \n27 \n', 0, 0)
(134.6405029296875, 198.9710693359375, 302.9473876953125, 221.58740234375, ' \n3.4. \nClassification of recognised terms \n', 1, 0)
(134.6405029296875, 222.01116943359375, 463.20416259765625, 325.0875244140625, 'Some of the analysed systems classify recognised terms by grouping them \naccording to some criteria. Thus, the related terms stay close to each other. Even \nFASTR attempts to infer an ontology from the recognised terms. Those systems \nwhich show some classification of recognised terms are the following: \na. ANA: it builds a semantic network from the detected terms. \nb. FASTR: it builds a graph to relate recognised terms. Also it proposes the \nconstruction of partial ontologies for some terms. \nc. LEXTER: it builds a terminological network splitting terms into head and \nexpansion. \n', 2, 0)
(134.6405029296875, 337.5111389160156, 194.8777618408203, 348.6075134277344, '3.5. \nResults \n', 3, 0)
(134.63999938964844, 348.97113037109375, 463.1163635253906, 392.2058410644531, 'The table below summarises for each system the type of corpus used for the tests \nand the results attained: \n \nTable 3: Results \n', 4, 0)
(142.1999969482422, 393.62860107421875, 408.1889343261719, 402.4658203125, 'System \nTest corpora \nTerms % \n', 5, 0)
(142.1999969482422, 403.588623046875, 436.1690368652344, 412.42584228515625, ' \nName /Author \nDomain \nLanguage Size.[Kw.] precision \nrecall \n', 6, 0)
(142.1999969482422, 413.4886169433594, 288.4876708984375, 431.5658264160156, '1 \nANA \nAviation engineering \nAcoustics \n', 7, 0)
(297.17950439453125, 413.4886169433594, 323.66912841796875, 431.5658264160156, 'French \nEnglish \n', 8, 0)
(336.5392150878906, 413.4886169433594, 350.5951843261719, 431.5658264160156, '120 \n25 \n', 9, 0)
(377.8796081542969, 413.4886169433594, 383.53582763671875, 431.5658264160156, ' \n? \n', 10, 0)
(416.4595947265625, 413.4886169433594, 422.1158142089844, 431.5658264160156, '? \n? \n', 11, 0)
(142.1999969482422, 431.5224609375, 432.5235900878906, 441.16583251953125, '2 \nCLARIT11 \nNews \nEnglish \n240 Mb \n- \n81.6 \n', 12, 0)
(142.1999969482422, 442.0486145019531, 422.18890380859375, 450.8858337402344, '3 \nDaille \nTelecommunications \nFrench \n800 \n? \n? \n', 13, 0)
(142.1999969482422, 451.76861572265625, 432.5437316894531, 460.6058349609375, '4 \nFASTR \nMedicine (abstracts) \nFrench \n1.560 \n86.7 \n74.9 \n', 14, 0)
(142.1999969482422, 461.4286193847656, 422.15380859375, 470.2658386230469, '5 \nHeid \nEngineering \nGerman \n35 \n? \n? \n', 15, 0)
(142.1999969482422, 471.14862060546875, 422.1405334472656, 479.98583984375, '6 \nLEXTER \nEngineering \nFrench \n3.250 \n95 \n? \n', 16, 0)
(142.1999969482422, 480.8686218261719, 422.17181396484375, 489.7058410644531, '7 \nNaulleau \nTechnical \nFrench \n? \n? \n? \n', 17, 0)
(142.1999969482422, 490.52862548828125, 426.5249938964844, 499.3658447265625, '8 \nNEURAL \nMedicine  \nEnglish \n55 \n? \n70 \n', 18, 0)
(142.1999969482422, 500.2486267089844, 266.174072265625, 518.2660522460938, '9 \nNODALIDA-95 \nCosmology \nTechnical text \n', 19, 0)
(297.17950439453125, 500.2486267089844, 447.21405029296875, 509.0858459472656, 'English \n20 \n95-98 \n98.5-100 \n', 20, 0)
(142.1999969482422, 519.1486206054688, 422.13482666015625, 527.98583984375, '10 Termight \nComputer science \nEnglish \n? \n? \n? \n', 21, 0)
(142.1999969482422, 528.86865234375, 437.1938781738281, 537.7058715820312, '11 TERMINO \nMedicine \nFrench \n? \n72 \n70-74 \n', 22, 0)
(142.1999969482422, 538.5286254882812, 275.3199768066406, 565.7861328125, '12 TERMS \nStatistics \nSemantics \nChromatography \n', 23, 0)
(297.17950439453125, 538.5286254882812, 352.5726318359375, 565.7861328125, 'English \n2.3 \n6.3 \n14.9 \n', 24, 0)
(377.8796081542969, 538.5286254882812, 387.9152526855469, 565.7861328125, '77 \n86 \n96 \n', 25, 0)
(416.4595947265625, 538.5286254882812, 418.45458984375, 547.3658447265625, ' \n', 26, 0)
(134.63999938964844, 591.13134765625, 209.96234130859375, 602.2277221679688, '4. \nConclusions \n', 27, 0)
(134.63980102539062, 602.5310668945312, 463.1788635253906, 654.946044921875, 'We can reach some conclusions after having analysed and evaluated some of the \nmain systems of TE designed in the last decade: \n                                                           \n11 The system has been intensively tested with regard to the indexing frequency, but not in relation to \nthe quality of the extracted terms. \n', 28, 0)

page suivante
(134.63999938964844, 175.99102783203125, 379.3233642578125, 187.08740234375, '28 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.63909912109375, 204.3707275390625, 463.2414245605469, 675.4274291992188, 'a) The efficiency of the extraction presents a high degree of variation from one \nto another. Broadly speaking, there is neither clear nor measurable explanation of \nthe final results. Besides, we have to bear in mind that these systems are tested \nwith small and highly specialised corpora. This lack of data makes it difficult to \nevaluate and compare them. However, it does not prevent pinpointing those \nsolutions, which are considered valid to solve specific problems. \nb) None of the systems is entirely satisfactory due to two main reasons. First, all \nsystems produce too much silence, especially statistically-based systems. \nSecond, all of them generate a great deal of noise, especially linguistically-based \nsystems. \nc) Taking into account the noise generated, all systems propose large lists of \ncandidate terms, which at the end of the process have to be manually accepted or \nrejected. \nd) Most of the TE systems are related to only one language: French or English. \nUsually the language specific data is embedded in the tool. This makes difficult \nto use the system in a language other than the original. \ne) As has been already pointed out, training corpora tend to be small (from 2.3 to \n12 Kwords) and highly specialised with regard to the topic as well as the \nspecialisation degree. This allows for a quite precise patterns and lexicosemantic, \nformal and morphosyntactic heuristics albeit this only applies to highly \nspecialised corpora. \nf) All systems focus entirely on NPs and none of them deals with verbal phrases. \nThis is because there is a high rate of terminological NPs in specialised texts. \nThis rate can vary according to the topic and the specialisation degree. Despite \nwhat has just been noted, it is noteworthy that all specialised languages have \ntheir own verbs (or specific combinations of a verbal nature), no matter how low \nthe ratio is in comparison with nouns. \ng) As a result, none of the systems refers to the distinction between nominal \ncollocations and nominal terminological units of a syntactic nature. Nor do they \nrefer to phraseology. \nh) Many of the systems make use of a number of morphosyntactic patterns to \nidentify complex terms. However they account for most of the terminological \nunits they are still too few and also not very constraining. Thus, for English are \nAN and NN, for French NA and N prep N. Some terms present structures other \nthan these ones and they are never detected. Those systems based only on these \ntypes of linguistic techniques generate too much noise. \ni) It is generally agreed that frequency is a good criterion to indicate that a \ncandidate term is actually a terminological unit. However, frequency is not on its \nown a sufficient criterion, as it yields a great deal of noise. \nj) Only a few recent systems use semantic information to recognise and delimit \nterminological units although its use takes place at different levels. \n', 1, 0)

page suivante
(147.89999389648438, 175.99102783203125, 465.6802978515625, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  \n29 \n', 0, 0)
(134.63955688476562, 198.9710693359375, 463.267578125, 589.5672607421875, ' \nk) None of the systems uses extensively the combinatory features of terms from \nspecialised languages in relation to a given domain. It is needed more studies \nabout the type of constraints that terminological units present with regard to \nconceptual field and text type. \nl) Only one of the analysed systems take profit of the possibilities given by the \nalignment of specialised text.  \ni) Most of the authors consider the POS disambiguation as one of the most \nimportant error sources. However, they do not provide exact figures about its \nincidence degree.  \nTo improve these systems of terminology extraction and lessen the noise and \nsilence that are generated, two type of studies should be encouraged. First, it is \nrequired more linguistic oriented studies on the semantic relationships among \nterms, the semantic relationships among constituents of a terminological unit, \nsemantico-lexical representation, constraints of terminological units within a \ngiven specialised domain and in a given text type, all the grammatical categories \nthat are likely to become terms in specialised domains, the influence of the \nsyntactic function of terminological phrases on texts, the relationships between \nterms and their arrangement in texts. \nSecond, we should focus on software systems that: combine in a more active \nmanner statistical and linguistic methods; improve statistical measures; combine \nmore than one strategy; are easily applicable to more than one language; improve \ninterfaces to facilitate the machine-user interaction. Also it should be very useful, \nas suggested in Kageura et al. (1998), the development of a common test bench \nfor aiding the evaluation/comparison of extracting methods. \nIn sum, should we progress in the field of automatic terminology extraction, \nstatistical and linguistic methods have to actively be combined. It means that \nthey are not either-or approaches but complementary ones. The final goal is to \nreduce the amount of silence and noise so that the process of terminological \nextraction becomes as automatic and precise as possible. In the future, we \nbelieve that any current terminology extractor, apart from accounting for the \nmorphological, syntactic and structural aspects of terminological units, has to \nnecessarily include semantic aspects if the efficiency of the system is to be \nimproved with regard to the existing ones. \n', 1, 0)
(134.6396026611328, 614.111572265625, 220.12823486328125, 625.2079467773438, 'Acknowledgements \n', 2, 0)
(134.6396026611328, 637.451171875, 463.1523132324219, 660.0675048828125, 'This paper has been written within the research project PB-96-0293, financially \nsupported by the Spanish government. \n', 3, 0)

page suivante
(134.63999938964844, 175.99102783203125, 379.3233642578125, 187.08740234375, '30 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.63999938964844, 204.3707275390625, 463.1577453613281, 261.42694091796875, 'We would like to thank Dr. H. Rodríguez (Universitat Politècnica de Catalunya), \nDr. C. Jacquemin (Université de Nantes), Dr. L. de Yzaguirre and Mrs. J. Freixa \n(Universitat Pompeu Fabra) their valuable comments on preliminary versions of \nthis paper. The remaining shortcomings are ours. We thank J. Morel for \ntranslating this paper from Catalan into English and C. Rodríguez its revision. \n', 1, 0)
(134.63999938964844, 285.970947265625, 183.70794677734375, 297.06732177734375, 'References \n', 2, 0)
(134.63999938964844, 309.37054443359375, 463.1596374511719, 331.92681884765625, 'Arppe, A. 1995. “Term extraction from unrestricted text”. Lingsoft Web Site: \nhttp://www.lingsoft.com \n', 3, 0)
(134.63980102539062, 335.3504333496094, 463.1605224609375, 369.4266662597656, 'Ahmad, K., Davies, A., Fulford, H. and Rogers, M. 1992. “What is a term? The \nsemiautomatic extraction of terms from text”. Translation Studies – an \ninterdiscipline. Amsterdam: John Benjamins. \n', 4, 0)
(134.63980102539062, 372.8504333496094, 463.15155029296875, 406.9266662597656, "Bourigault, D. 1994. LEXTER, un Logiciel d'EXtraction de TERminologie. \nApplication à l'acquisition des connaissances à partir de textes. PhD Thesis. \nParis: École des Hautes Études en Sciences Sociales. \n", 5, 0)
(134.63980102539062, 410.350341796875, 463.11846923828125, 444.42657470703125, 'Bourigault, D., Gonzalez-Mullier, I. and Gros, C. 1996. “LEXTER, a Natural \nLanguage Processing Tool for Terminology Extraction”. Proceedings of the \n7th EURALEX International Congress. Göteborg. \n', 6, 0)
(134.63980102539062, 447.850341796875, 463.13909912109375, 481.9273986816406, 'Brown, P. F., Cocke, F., Pietra, S., Felihek. F., Merces, R. and Rossin, P. (1988) \nA statistical approach to language translation. Procedings of 12th International \nConference of Computational Linguistic (Coling-88). Budapest, Hungary.  \n', 7, 0)
(134.64019775390625, 485.3510437011719, 463.15399169921875, 507.90728759765625, 'Cabré, M.T. 1999. Terminology. Theory, methods and applications. Amsterdam: \nJohn Benjamins. \n', 8, 0)
(134.64019775390625, 511.3309326171875, 463.1328125, 545.4072875976562, 'Church, K. 1989. “Word association norms, mutual information and \nlexicography”. Proceedings of the 27th annual meeting of the ACL. Vancouver, \n76-83. \n', 9, 0)
(134.64019775390625, 548.8309326171875, 463.17779541015625, 571.4473266601562, 'Condamines, A. 1995. “Terminology: new needs, new perspectives”. \nTerminology, 2, 2: 219-238. \n', 10, 0)
(134.64019775390625, 574.8108520507812, 463.21917724609375, 608.9472045898438, 'Dagan, I. and Church, K. 1994. “Termight: Identifying and translating technical \nterminology”. Proceedings of the Fourth Conference on Applied Natural \nLanguage Processing, 34-40. \n', 11, 0)
(134.63999938964844, 612.3108520507812, 463.10589599609375, 634.9271850585938, "Daille, B. 1994. Approche mixte pour l'extraction de terminologie: statistique \nlexicale et filtres linguistiques. PhD dissertation. Paris: Université Paris VII.  \n", 12, 0)
(134.639892578125, 638.290771484375, 463.2267761230469, 660.9071044921875, 'Daille, B. and Jacquemin, C 1998. “Lexical database and information access: a \nfruitfull association?”. First International Conference on LREC. Granada.  \n', 13, 0)

page suivante
(147.89999389648438, 175.99102783203125, 465.6802978515625, 198.607421875, 'AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS  \n31 \n', 0, 0)
(134.6405029296875, 198.9710693359375, 463.1813049316406, 256.08734130859375, " \nDavid, S. and Plante, P. 1991. “Le progiciel TERMINO: de la nécessité d'une \nanalyse morphosyntaxique pour le dépouillement terminologique des textes”. \nProceedings of the Montreal Colloquium Les industries de la langue: \nperspectives des années 1990, 1: 71-88. \n", 1, 0)
(134.6405029296875, 259.51104736328125, 463.17236328125, 282.06732177734375, 'Enguehard, C. and Pantera, L. 1994. “Automatic Natural Acquisition of a \nTerminology”. Journal of Quantitative Linguistics, 2, 1: 27-32. \n', 2, 0)
(134.6405029296875, 285.490966796875, 463.1571044921875, 319.5672607421875, 'Estopà, R. 1999. Extracció de terminologia: elements per a la construcció d’un \nSEACUSE (Sistema d’extracció automàtica de candidats a unitats de \nsignificació especialitzada). PhD thesis, Barcelona: Universitat Pompeu Fabra. \n', 3, 0)
(134.6405029296875, 322.9909362792969, 463.18609619140625, 357.0671691894531, 'Estopà, R. and Vivaldi, J. 1998. “Systèmes de détection automatique de \n(candidats à) termes: vers une proposition intégratrice”. Actes des 7èmes \nJournées ERLA-GLAT, Brest, 385-410 \n', 4, 0)
(134.639404296875, 360.4908447265625, 463.16107177734375, 394.56707763671875, 'Evans, D.A. and Zhai, C. 1996. “Noun-phrase Analysis in Unrestricted Text for \ninformation retrieval”. Proceedings of ACL, Santa Cruz, University of \nCalifornia, 17-24. \n', 5, 0)
(134.6396026611328, 397.9908447265625, 463.1808776855469, 432.06707763671875, 'Frantzi, K. and Ananiadou, S. 1995. Statistical measures for terminological \nextraction. Working paper of the Department of Computing of Manchester \nMetropolitan University.  \n', 6, 0)
(134.6396942138672, 435.4907531738281, 463.26519775390625, 458.0469970703125, 'Frantzi, K. T. 1997. “Incorporating context information for extraction of terms”. \nProceedings of ACL/EACL, Madrid, 501-503. \n', 7, 0)
(134.6396026611328, 461.47064208984375, 463.16033935546875, 484.0870056152344, 'Habert, B., Naulleau, E. and Nazarenko, A. 1996. “Symbolic word clustering for \nmedium-size corpora”. Proceedings of Coling’96: 490-495. \n', 8, 0)
(134.6396026611328, 487.4505310058594, 463.1492614746094, 521.5868530273438, 'Heid, U., Jauss, S., Krüger, K. and Hohmann, A. 1996. “Term extraction with \nstandard tools for corpus exploration. Experience from German”. In: TKE ‘96: \nTerminology and Knowledge Engineering,, 139-150. Berlin: Indeks Verlag.  \n', 9, 0)
(134.639404296875, 524.9505615234375, 463.12408447265625, 547.56689453125, 'Jacquemin, C. 1994. “Recycling Terms into a Partial Parser”. Proceedings of \nANLP’94, 113-118. \n', 10, 0)
(134.63929748535156, 550.990478515625, 463.210205078125, 573.5466918945312, "Jacquemin, C. 1999. “Syntagmatic and paradigmatic representations of term \nvariation”. Proceedings of ACL'99, University of Maryland, 341-348. \n", 11, 0)
(134.63929748535156, 576.970458984375, 463.2312927246094, 611.0466918945312, 'Jacquin, C. and Liscouet, M. 1996. “Terminology extraction from texts corpora: \napplication to document keeping via Internet”. In: TKE ‘96: Terminology and \nKnowledge Engineering, 74-83. Berlin: Indeks Verlag. \n', 12, 0)
(134.63909912109375, 614.4703369140625, 463.1910400390625, 648.5465698242188, 'Justeson, J. and Katz, S. 1995. “Technical terminology: some linguistic \nproperties and an algorithm for identification in text”. Natural Language \nEngineering, 1, 1: 9-27. \n', 13, 0)
(134.63900756835938, 651.97021484375, 463.16375732421875, 674.5866088867188, 'Kageura, K. and Umino, B. 1996. “Methods of Automatic Term Recognition”. \nPapers of the National Center for Science Information Systems, 1-22. \n', 14, 0)

page suivante
(134.63999938964844, 175.99102783203125, 379.3233642578125, 187.08740234375, '32 Automatic Term Detection: a Review of Current Systems \n', 0, 0)
(134.63999938964844, 204.3707275390625, 463.1418151855469, 238.447021484375, 'Kageura, K., Yoshioka, M., Koyama, T. and Nozue, T. 1998. “Towards a \ncommon testbed for corpus-based computational terminology”. Proceedings of \nComputerm ‘98, Montreal, 81-85. \n', 1, 0)
(134.63980102539062, 241.8707275390625, 463.1951599121094, 275.94732666015625, 'Karlsson, F. 1990. “Constraint grammar as a framework for parsing running \ntext”. Proceedings of the 13th International conference on computational \nlinguistic, 3: 168-173. \n', 2, 0)
(134.64019775390625, 279.3709716796875, 463.17572021484375, 301.92724609375, 'Lauriston, A. 1994. “Automatic recognition of complex terms: Problems and the \nTERMINO solution”. Terminology, 1, 1: 147-170. \n', 3, 0)
(134.64019775390625, 305.350830078125, 463.2142333984375, 339.4270935058594, 'Maynard, D. and Ananiadou, S. 1999. “Identifying contextual information for \nmulti-word term extraction”. In: TKE ‘99: Terminology and Knowledge \nEngineering, 212-221. Vienna: TermNet.  \n', 4, 0)
(134.64019775390625, 342.850830078125, 463.16082763671875, 365.4070739746094, 'Nakagawa, H. and Mori , T. 1998. “Nested collocation and Compound Noun for \nTerm Extraction”. Proceedings of Computerm ’98, Montreal, 64-70. \n', 5, 0)
(134.64120483398438, 368.83074951171875, 463.1428527832031, 402.906982421875, 'Naulleau, E 1998. Apprentissage et filtrage syntaxico-sémantique de syntagmes \nnominaux pertinents pour la recherche documentaire. PhD thesis. Paris: \nUniversité Paris 13. \n', 6, 0)
(134.64219665527344, 406.3306579589844, 463.1169128417969, 428.947021484375, 'Naulleau, E. 1999. “Profile-guided terminology extraction”. In: TKE‘99: \nTerminology and Knowledge Engineering. 222-240. Vienna: TermNet.  \n', 7, 0)
(134.64239501953125, 432.310546875, 463.13397216796875, 454.9269104003906, 'Plante, P. and Dumas, L. 1998. “Le Dépoulliment terminologique assisté par \nordinateur”. Terminogramme, 46, 24-28. \n', 8, 0)
(134.6425018310547, 458.3505554199219, 463.2098083496094, 480.90679931640625, 'Shieber, S.N. 1986. “An Introduction to Unification-Based Approaches to \ngrammar”. CSLI Lecture Notes of University Press, 4. \n', 9, 0)
(134.64239501953125, 484.3304443359375, 463.1996154785156, 519.2214965820312, 'Smadja, F. 1991. Extracting collocations from text. An application : language \ngeneration. Columbia: Columbia University. Department of Computer \nScience. [Unpublished doctoral dissertation] \n', 10, 0)
(134.6425018310547, 522.5508422851562, 463.1342468261719, 545.1671752929688, 'Voutilainen, A. 1993. “NPtool, a detector of English noun phrases”. Proceedings \nof the Workshop on Very Large Corpora.  \n', 11, 0)
(134.6425018310547, 548.5908203125, 463.16339111328125, 582.6670532226562, 'Zhai, C., Tong, X., Milic-Frayling, N. and Evans, D.A. 1996. “Evaluation of \nsyntactic phrase indexing CLARIT. NLP track report”. Proceedings of the \nTREC-5. TREC Web Site: http://trec.nist.gov/pubs/trec5/t5_proceedings.html \n', 12, 0)

page suivante
