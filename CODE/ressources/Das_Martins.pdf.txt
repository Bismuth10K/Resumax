(147.97500610351562, 113.78776550292969, 458.86773681640625, 131.00315856933594, 'A Survey on Automatic Text Summarization\n', 0, 0)
(199.8280029296875, 162.43960571289062, 408.2908935546875, 174.40675354003906, 'Dipanjan Das\nAndr´e F.T. Martins\n', 1, 0)
(223.427001953125, 183.58761596679688, 388.6000671386719, 231.927001953125, 'Language Technologies Institute\nCarnegie Mellon University\n{dipanjan, afm}@cs.cmu.edu\n', 2, 0)
(257.22698974609375, 235.87564086914062, 354.805419921875, 247.84278869628906, 'November 21, 2007\n', 3, 0)
(284.0820007324219, 275.69140625, 327.9173889160156, 287.606689453125, 'Abstract\n', 4, 0)
(135.2729949951172, 293.6432800292969, 476.7711486816406, 425.3395690917969, 'The increasing availability of online information has necessitated intensive\nresearch in the area of automatic text summarization within the Natural Lan-\nguage Processing (NLP) community. Over the past half a century, the prob-\nlem has been addressed from many diﬀerent perspectives, in varying domains\nand using various paradigms. This survey intends to investigate some of the\nmost relevant approaches both in the areas of single-document and multiple-\ndocument summarization, giving special emphasis to empirical methods and\nextractive techniques. Some promising approaches that concentrate on speciﬁc\ndetails of the summarization problem are also discussed. Special attention is\ndevoted to automatic evaluation of summarization systems, as future research\non summarization is strongly dependent on progress in this area.\n', 5, 0)
(107.99999237060547, 446.4612731933594, 221.0767059326172, 460.8218078613281, '1\nIntroduction\n', 6, 0)
(107.99999237060547, 471.0019836425781, 504.0986633300781, 552.0462036132812, 'The subﬁeld of summarization has been investigated by the NLP community for\nnearly the last half century. Radev et al. (2002) deﬁne a summary as “a text that\nis produced from one or more texts, that conveys important information in the\noriginal text(s), and that is no longer than half of the original text(s) and usually\nsigniﬁcantly less than that”. This simple deﬁnition captures three important aspects\nthat characterize research on automatic summarization:\n', 7, 0)
(124.36399841308594, 561.2640380859375, 500.43072509765625, 582.3076782226562, '• Summaries may be produced from a single document or multiple documents,\n', 8, 0)
(124.364013671875, 578.1099853515625, 378.3714294433594, 599.1536254882812, '• Summaries should preserve important information,\n', 9, 0)
(124.364013671875, 594.95703125, 267.9604187011719, 616.0006713867188, '• Summaries should be short.\n', 10, 0)
(108.00001525878906, 617.4720458984375, 504.0657653808594, 671.418212890625, 'Even if we agree unanimously on these points, it seems from the literature that\nany attempt to provide a more elaborate deﬁnition for the task would result in\ndisagreement within the community. In fact, many approaches diﬀer on the manner\nof their problem formulations. We start by introducing some common terms in the\n', 11, 0)
(303.27301025390625, 692.291015625, 308.7275695800781, 705.5892333984375, '1\n', 12, 0)

page suivante
(107.99993896484375, 72.38806915283203, 504.05560302734375, 492.1624450683594, 'summarization dialect: extraction is the procedure of identifying important sections\nof the text and producing them verbatim; abstraction aims to produce important\nmaterial in a new way; fusion combines extracted parts coherently; and compression\naims to throw out unimportant sections of the text (Radev et al., 2002).\nEarliest instances of research on summarizing scientiﬁc documents proposed\nparadigms for extracting salient sentences from text using features like word and\nphrase frequency (Luhn, 1958), position in the text (Baxendale, 1958) and key\nphrases (Edmundson, 1969). Various work published since then has concentrated on\nother domains, mostly on newswire data. Many approaches addressed the problem\nby building systems depending of the type of the required summary. While extractive\nsummarization is mainly concerned with what the summary content should be, usu-\nally relying solely on extraction of sentences, abstractive summarization puts strong\nemphasis on the form, aiming to produce a grammatical summary, which usually\nrequires advanced language generation techniques. In a paradigm more tuned to\ninformation retrieval (IR), one can also consider topic-driven summarization, that\nassumes that the summary content depends on the preference of the user and can\nbe assessed via a query, making the ﬁnal summary focused on a particular topic.\nA crucial issue that will certainly drive future research on summarization is\nevaluation. During the last ﬁfteen years, many system evaluation competitions like\nTREC,1 DUC2 and MUC3 have created sets of training material and have estab-\nlished baselines for performance levels. However, a universal strategy to evaluate\nsummarization systems is still absent.\nIn this survey, we primarily aim to investigate how empirical methods have been\nused to build summarization systems. The rest of the paper is organized as fol-\nlows: Section 2 describes single-document summarization, focusing on extractive\ntechniques. Section 3 progresses to discuss the area of multi-document summariza-\ntion, where a few abstractive approaches that pioneered the ﬁeld are also considered.\nSection 4 brieﬂy discusses some unconventional approaches that we believe can be\nuseful in the future of summarization research. Section 5 elaborates a few eval-\nuation techniques and describes some of the standards for evaluating summaries\nautomatically. Finally, Section 6 concludes the survey.\n', 0, 0)
(107.99998474121094, 513.0475463867188, 364.5960388183594, 527.4080810546875, '2\nSingle-Document Summarization\n', 1, 0)
(107.99998474121094, 537.5882568359375, 504.0657043457031, 618.6324462890625, 'Usually, the ﬂow of information in a given document is not uniform, which means\nthat some parts are more important than others. The major challenge in summa-\nrization lies in distinguishing the more informative parts of a document from the\nless ones. Though there have been instances of research describing the automatic\ncreation of abstracts, most work presented in the literature relies on verbatim ex-\ntraction of sentences to address the problem of single-document summarization. In\n', 2, 0)
(120.4369888305664, 627.3677978515625, 464.46453857421875, 670.781005859375, '1See http://trec.nist.gov/.\n2See http://duc.nist.gov/.\n3See http://www.itl.nist.gov/iad/894.02/related projects/muc/proceedings/.\nmuc 7 toc.html\n', 3, 0)
(303.27301025390625, 692.2910766601562, 308.7275695800781, 705.5892944335938, '2\n', 4, 0)

page suivante
(108.0, 72.38806915283203, 504.0547180175781, 139.88328552246094, 'this section, we describe some eminent extractive techniques. First, we look at early\nwork from the 1950s and 60s that kicked oﬀ research on summarization. Second,\nwe concentrate on approaches involving machine learning techniques published in\nthe 1990s to today. Finally, we brieﬂy describe some techniques that use a more\ncomplex natural language analysis to tackle the problem.\n', 0, 0)
(108.0, 157.40158081054688, 206.9412384033203, 169.3687286376953, '2.1\nEarly Work\n', 1, 0)
(107.99996948242188, 176.39202880859375, 504.1203308105469, 582.617431640625, 'Most early work on single-document summarization focused on technical documents.\nPerhaps the most cited paper on summarization is that of (Luhn, 1958), that de-\nscribes research done at IBM in the 1950s. In his work, Luhn proposed that the\nfrequency of a particular word in an article provides an useful measure of its sig-\nniﬁcance. There are several key ideas put forward in this paper that have assumed\nimportance in later work on summarization. As a ﬁrst step, words were stemmed to\ntheir root forms, and stop words were deleted. Luhn then compiled a list of content\nwords sorted by decreasing frequency, the index providing a signiﬁcance measure of\nthe word. On a sentence level, a signiﬁcance factor was derived that reﬂects the\nnumber of occurrences of signiﬁcant words within a sentence, and the linear distance\nbetween them due to the intervention of non-signiﬁcant words. All sentences are\nranked in order of their signiﬁcance factor, and the top ranking sentences are ﬁnally\nselected to form the auto-abstract.\nRelated work (Baxendale, 1958), also done at IBM and published in the same\njournal, provides early insight on a particular feature helpful in ﬁnding salient parts\nof documents: the sentence position. Towards this goal, the author examined 200\nparagraphs to ﬁnd that in 85% of the paragraphs the topic sentence came as the ﬁrst\none and in 7% of the time it was the last sentence. Thus, a naive but fairly accurate\nway to select a topic sentence would be to choose one of these two. This positional\nfeature has since been used in many complex machine learning based systems.\nEdmundson (1969) describes a system that produces document extracts. His\nprimary contribution was the development of a typical structure for an extractive\nsummarization experiment. At ﬁrst, the author developed a protocol for creating\nmanual extracts, that was applied in a set of 400 technical documents. The two\nfeatures of word frequency and positional importance were incorporated from the\nprevious two works.\nTwo other features were used: the presence of cue words\n(presence of words like signiﬁcant, or hardly), and the skeleton of the document\n(whether the sentence is a title or heading). Weights were attached to each of these\nfeatures manually to score each sentence. During evaluation, it was found that about\n44% of the auto-extracts matched the manual extracts.\n', 2, 0)
(108.0, 600.1357421875, 300.49066162109375, 612.1028442382812, '2.2\nMachine Learning Methods\n', 3, 0)
(108.0, 619.126220703125, 504.0439147949219, 673.0723876953125, 'In the 1990s, with the advent of machine learning techniques in NLP, a series of semi-\nnal publications appeared that employed statistical techniques to produce document\nextracts. While initially most systems assumed feature independence and relied on\nnaive-Bayes methods, others have focused on the choice of appropriate features and\n', 4, 0)
(303.27301025390625, 692.2911987304688, 308.7275695800781, 705.5894165039062, '3\n', 5, 0)

page suivante
(108.0, 72.38806915283203, 504.05474853515625, 153.4322967529297, 'on learning algorithms that make no independence assumptions. Other signiﬁcant\napproaches involved hidden Markov models and log-linear models to improve ex-\ntractive summarization. A very recent paper, in contrast, used neural networks and\nthird party features (like common words in search engine queries) to improve purely\nextractive single document summarization. We next describe all these approaches\nin more detail.\n', 0, 0)
(108.0, 169.19903564453125, 265.81109619140625, 182.24632263183594, '2.2.1\nNaive-Bayes Methods\n', 1, 0)
(108.0, 189.5430908203125, 504.0440673828125, 257.0382995605469, 'Kupiec et al. (1995) describe a method derived from Edmundson (1969) that is able\nto learn from data. The classiﬁcation function categorizes each sentence as worthy\nof extraction or not, using a naive-Bayes classiﬁer. Let s be a particular sentence,\nS the set of sentences that make up the summary, and F1, . . . , Fk the features.\nAssuming independence of the features:\n', 2, 0)
(176.1360321044922, 262.34521484375, 504.0028991699219, 319.8999328613281, 'P(s ∈ S | F1, F2, ..Fk) =\nQk\ni=1 P(Fi | s ∈ S) · P(s ∈ S)\nQk\ni=1 P(Fi)\n(1)\n', 3, 0)
(108.00001525878906, 308.58209228515625, 504.0767517089844, 484.4703674316406, 'The features were compliant to (Edmundson, 1969), but additionally included the\nsentence length and the presence of uppercase words. Each sentence was given a\nscore according to (1), and only the n top sentences were extracted. To evaluate\nthe system, a corpus of technical documents with manual abstracts was used in\nthe following way: for each sentence in the manual abstract, the authors manually\nanalyzed its match with the actual document sentences and created a mapping\n(e.g. exact match with a sentence, matching a join of two sentences, not matchable,\netc.). The auto-extracts were then evaluated against this mapping. Feature analysis\nrevealed that a system using only the position and the cue features, along with the\nsentence length sentence feature, performed best.\nAone et al. (1999) also incorporated a naive-Bayes classiﬁer, but with richer\nfeatures. They describe a system called DimSum that made use of features like\nterm frequency (tf ) and inverse document frequency (idf) to derive signature words.4\n', 4, 0)
(108.00003051757812, 484.72216796875, 504.06573486328125, 633.5123901367188, 'The idf was computed from a large corpus of the same domain as the concerned\ndocuments. Statistically derived two-noun word collocations were used as units for\ncounting, along with single words. A named-entity tagger was used and each entity\nwas considered as a single token. They also employed some shallow discourse analysis\nlike reference to same entities in the text, maintaining cohesion.\nThe references\nwere resolved at a very shallow level by linking name aliases within a document\nlike “U.S.” to “United States”, or “IBM” for “International Business Machines”.\nSynonyms and morphological variants were also merged while considering lexical\nterms, the former being identiﬁed by using Wordnet (Miller, 1995). The corpora\nused in the experiments were from newswire, some of which belonged to the TREC\nevaluations.\n', 5, 0)
(120.43699645996094, 640.1257934570312, 322.61090087890625, 650.6595458984375, '4Words that indicate key concepts in a document.\n', 6, 0)
(303.27301025390625, 692.2910766601562, 308.7275695800781, 705.5892944335938, '4\n', 7, 0)

page suivante
(108.0, 72.63897705078125, 330.0111389160156, 85.6862564086914, '2.2.2\nRich Features and Decision Trees\n', 0, 0)
(107.99993896484375, 92.98303985595703, 504.0657958984375, 634.699462890625, 'Lin and Hovy (1997) studied the importance of a single feature, sentence position.\nJust weighing a sentence by its position in text, which the authors term as the\n“position method”, arises from the idea that texts generally follow a predictable\ndiscourse structure, and that the sentences of greater topic centrality tend to occur in\ncertain speciﬁable locations (e.g. title, abstracts, etc). However, since the discourse\nstructure signiﬁcantly varies over domains, the position method cannot be deﬁned\nas naively as in (Baxendale, 1958). The paper makes an important contribution by\ninvestigating techniques of tailoring the position method towards optimality over a\ngenre and how it can be evaluated for eﬀectiveness. A newswire corpus was used, the\ncollection of Ziﬀ-Davis texts produced from the TIPSTER5 program; it consists of\ntext about computer and related hardware, accompanied by a set of key topic words\nand a small abstract of six sentences. For each document in the corpus, the authors\nmeasured the yield of each sentence position against the topic keywords. They then\nranked the sentence positions by their average yield to produce the Optimal Position\nPolicy (OPP) for topic positions for the genre.\nTwo kinds of evaluation were performed. Previously unseen text was used for\ntesting whether the same procedure would work in a diﬀerent domain. The ﬁrst\nevaluation showed contours exactly like the training documents. In the second eval-\nuation, word overlap of manual abstracts with the extracted sentences was measured.\nWindows in abstracts were compared with windows on the selected sentences and\ncorresponding precision and recall values were measured. A high degree of coverage\nindicated the eﬀectiveness of the position method.\nIn later work, Lin (1999) broke away from the assumption that features are\nindependent of each other and tried to model the problem of sentence extraction\nusing decision trees, instead of a naive-Bayes classiﬁer. He examined a lot of fea-\ntures and their eﬀect on sentence extraction.\nThe data used in this work is a\npublicly available collection of texts, classiﬁed into various topics, provided by the\nTIPSTER-SUMMAC6 evaluations, targeted towards information retrieval systems.\nThe dataset contains essential text fragments (phrases, clauses, and sentences) which\nmust be included in summaries to answer some TREC topics. These fragments were\neach evaluated by a human judge. The experiments described in the paper are with\nthe SUMMARIST system developed at the University of Southern California. The\nsystem extracted sentences from the documents and those were matched against\nhuman extracts, like most early work on extractive summarization.\nSome novel features were the query signature (normalized score given to sen-\ntences depending on number of query words that they contain), IR signature (the\nm most salient words in the corpus, similar to the signature words of (Aone et al.,\n1999)), numerical data (boolean value 1 given to sentences that contained a num-\nber in them), proper name (boolean value 1 given to sentences that contained a\nproper name in them), pronoun or adjective (boolean value 1 given to sentences\n', 1, 0)
(120.43699645996094, 644.040771484375, 464.12744140625, 665.5360107421875, '5See http://www.itl.nist.gov/iaui/894.02/related_projects/tipster/.\n6See http://www-nlpir.nist.gov/related_projects/tipster_summac/index.html.\n', 2, 0)
(303.27301025390625, 692.2910766601562, 308.7275695800781, 705.5892944335938, '5\n', 3, 0)

page suivante
(108.0, 72.38806915283203, 504.0658264160156, 234.7273406982422, 'that contained a pronoun or adjective in them), weekday or month (similar as pre-\nvious feature) and quotation (similar as previous feature). It is worth noting that\nsome features like the query signature are question-oriented because of the setting\nof the evaluation, unlike a generalized summarization framework.\nThe author experimented with various baselines, like using only the positional\nfeature, or using a simple combination of all features by adding their values. When\nevaluated by matching machine extracted and human extracted sentences, the deci-\nsion tree classiﬁer was clearly the winner for the whole dataset, but for three topics,\na naive combination of features beat it. Lin conjectured that this happened because\nsome of the features were independent of each other. Feature analysis suggested\nthat the IR signature was a valuable feature, corroborating the early ﬁndings of\nLuhn (1958).\n', 0, 0)
(108.0, 250.49407958984375, 276.3383483886719, 263.5413513183594, '2.2.3\nHidden Markov Models\n', 1, 0)
(107.99993896484375, 270.8381652832031, 504.0330810546875, 365.431396484375, 'In contrast with previous approaches, that were mostly feature-based and non-\nsequential, Conroy and O’leary (2001) modeled the problem of extracting a sentence\nfrom a document using a hidden Markov model (HMM). The basic motivation for\nusing a sequential model is to account for local dependencies between sentences.\nOnly three features were used: position of the sentence in the document (built into\nthe state structure of the HMM), number of terms in the sentence, and likeliness of\nthe sentence terms given the document terms.\n', 2, 0)
(141.90997314453125, 428.8357849121094, 452.3271484375, 436.1829528808594, 'no\n3\n2\n1\nno\nno\nno\n', 3, 0)
(108.0, 454.5210876464844, 504.01129150390625, 481.3682861328125, 'Figure 1: Markov model to extract to three summary sentences from a document\n(Conroy and O’leary, 2001).\n', 4, 0)
(108.00001525878906, 494.18011474609375, 504.05499267578125, 670.0693359375, 'The HMM was structured as follows: it contained 2s + 1 states, alternating be-\ntween s summary states and s+1 nonsummary states. The authors allowed “hesita-\ntion” only in nonsummary states and “skipping next state” only in summary states.\nFigure 1 shows an example HMM with 7 nodes, corresponding to s = 3. Using the\nTREC dataset as training corpus, the authors obtained the maximum-likelihood\nestimate for each transition probability, forming the transition matrix estimate ˆ\nM,\nwhose element (i, j) is the empirical probability of transitioning from state i to j.\nAssociated with each state i was an output function, bi(O) = Pr(O | state i) where\nO is an observed vector of features. They made a simplifying assumption that the\nfeatures are multivariate normal. The output function for each state was thus esti-\nmated by using the training data to compute the maximum likelihood estimate of\nits mean and covariance matrix. They estimated 2s+1 means, but assumed that all\nof the output functions shared a common covariance matrix. Evaluation was done\n', 5, 0)
(303.2730712890625, 692.2910766601562, 308.7276306152344, 705.5892944335938, '6\n', 6, 0)

page suivante
(108.0, 72.38806915283203, 328.1783447265625, 85.6862564086914, 'by comparing with human generated extracts.\n', 0, 0)
(108.0, 101.4530029296875, 249.48016357421875, 114.50028228759766, '2.2.4\nLog-Linear Models\n', 1, 0)
(108.0, 121.79706573486328, 504.0440368652344, 216.3903045654297, 'Osborne (2002) claims that existing approaches to summarization have always as-\nsumed feature independence.\nThe author used log-linear models to obviate this\nassumption and showed empirically that the system produced better extracts than\na naive-Bayes model, with a prior appended to both models. Let c be a label, s\nthe item we are interested in labeling, fi the i-th feature, and λi the corresponding\nfeature weight. The conditional log-linear model used by Osborne (2002) can be\nstated as follows:\n', 2, 0)
(217.8370361328125, 215.799072265625, 283.1466064453125, 244.2227325439453, 'P(c | s) =\n1\n', 3, 0)
(269.5069885253906, 223.1790771484375, 311.0151062011719, 243.9602508544922, 'Z(s) exp\n', 4, 0)
(312.83099365234375, 206.67417907714844, 337.2207336425781, 255.67796325683594, ' X\n', 5, 0)
(327.9049987792969, 223.1790771484375, 380.6797180175781, 248.83053588867188, 'i\nλifi(c, s)\n', 6, 0)
(380.6780700683594, 206.67417907714844, 389.3180847167969, 247.38694763183594, '!\n', 7, 0)
(391.133056640625, 223.1790771484375, 504.0029296875, 236.47727966308594, ',\n(2)\n', 8, 0)
(108.00006103515625, 246.9781951904297, 504.06585693359375, 334.0553283691406, 'where Z(s) = P\nc exp (P\ni λifi(c, s)). In this domain, there are only two possible\nlabels: either the sentence is to be extracted or it is not. The weights were trained\nby conjugate gradient descent. The authors added a non-uniform prior to the model,\nclaiming that a log-linear model tends to reject too many sentences for inclusion in\na summary. The same prior was also added to a naive-Bayes model for comparison.\nThe classiﬁcation took place as follows:\n', 9, 0)
(137.2240753173828, 353.9251403808594, 340.6634216308594, 379.37152099609375, 'label(s) = arg max\nc∈C P(c) · P(s, c) = arg max\nc∈C\n', 10, 0)
(342.4770812988281, 337.41925048828125, 351.1170959472656, 378.1319885253906, ' \n', 11, 0)
(351.1130676269531, 345.7102355957031, 417.83282470703125, 386.4229736328125, 'log P(c) +\nX\n', 12, 0)
(408.51708984375, 353.92510986328125, 461.2917785644531, 379.5755615234375, 'i\nλifi(c, s)\n', 13, 0)
(461.2911376953125, 337.4192199707031, 469.93115234375, 378.1319580078125, '!\n', 14, 0)
(471.7451477050781, 353.92510986328125, 504.0030212402344, 367.2232971191406, '.\n(3)\n', 15, 0)
(108.00015258789062, 387.63409423828125, 504.0331115722656, 428.0303039550781, 'The authors optimized the prior using the f2 score of the classiﬁer as an objective\nfunction on a part of the dataset (in the technical domain). The summaries were\nevaluated using the standard f2 score where f2 = 2pr\n', 16, 0)
(108.00001525878906, 414.7320861816406, 504.0111083984375, 495.77630615234375, 'p+r, where the precision and recall\nmeasures were measured against human generated extracts. The features included\nword pairs (pairs of words with all words truncated to ten characters), sentence\nlength, sentence position, and naive discourse features like inside introduction or\ninside conclusion. With respect to f2 score, the log-linear model outperformed the\nnaive-Bayes classiﬁer with the prior, exhibiting the former’s eﬀectiveness.\n', 17, 0)
(108.00001525878906, 511.54302978515625, 384.7748107910156, 524.59033203125, '2.2.5\nNeural Networks and Third Party Features\n', 18, 0)
(108.00001525878906, 531.8870849609375, 504.09844970703125, 667.1282958984375, 'In 2001-02, DUC issued a task of creating a 100-word summary of a single news\narticle. However, the best performing systems in the evaluations could not outper-\nform the baseline with statistical signiﬁcance. This extremely strong baseline has\nbeen analyzed by Nenkova (2005) and corresponds to the selection of the ﬁrst n\nsentences of a newswire article. This surprising result has been attributed to the\njournalistic convention of putting the most important part of an article in the initial\nparagraphs. After 2002, the task of single-document summarization for newswire\nwas dropped from DUC. Svore et al. (2007) propose an algorithm based on neu-\nral nets and the use of third party datasets to tackle the problem of extractive\nsummarization, outperforming the baseline with statistical signiﬁcance.\n', 19, 0)
(303.27301025390625, 692.2910766601562, 308.7275695800781, 705.5892944335938, '7\n', 20, 0)

page suivante
(108.0, 72.38806915283203, 504.0657958984375, 424.4164123535156, 'The authors used a dataset containing 1365 documents gathered from CNN.com,\neach consisting of the title, timestamp, three or four human generated story high-\nlights and the article text.\nThey considered the task of creating three machine\nhighlights. The human generated highlights were not verbatim extractions from the\narticle itself. The authors evaluated their system using two metrics: the ﬁrst one\nconcatenated the three highlights produced by the system, concatenated the three\nhuman generated highlights, and compared these two blocks; the second metric con-\nsidered the ordering and compared the sentences on an individual level.\nSvore et al. (2007) trained a model from the labels and the features for each\nsentence of an article, that could infer the proper ranking of sentences in a test\ndocument. The ranking was accomplished using RankNet (Burges et al., 2005), a\npair-based neural network algorithm designed to rank a set of inputs that uses the\ngradient descent method for training. For the training set, they used ROUGE-1\n(Lin, 2004) to score the similarity of a human written highlight and a sentence\nin the document. These similarity scores were used as soft labels during training,\ncontrasting with other approaches where sentences are “hard-labeled”, as selected\nor not.\nSome of the used features based on position or n-grams frequencies have been\nobserved in previous work. However, the novelty of the framework lay in the use\nof features that derived information from query logs from Microsoft’s news search\nengine7 and Wikipedia8 entries. The authors conjecture that if a document sentence\ncontained keywords used in the news search engine, or entities found in Wikipedia\narticles, then there is a greater chance of having that sentence in the highlight. The\nextracts were evaluated using ROUGE-1 and ROUGE-2, and showed statistically\nsigniﬁcant improvements over the baseline of selecting the ﬁrst three sentences in a\ndocument.\n', 0, 0)
(108.0000228881836, 441.9357604980469, 388.73199462890625, 453.9029235839844, '2.3\nDeep Natural Language Analysis Methods\n', 1, 0)
(108.0, 460.92523193359375, 504.0766296386719, 636.8134765625, 'In this subsection, we describe a set of papers that detail approaches towards single-\ndocument summarization involving complex natural language analysis techniques.\nNone of these papers solve the problem using machine learning, but rather use a set\nof heuristics to create document extracts. Most of these techniques try to model the\ntext’s discourse structure.\nBarzilay and Elhadad (1997) describe a work that used considerable amount of\nlinguistic analysis for performing the task of summarization. For a better under-\nstanding of their method, we need to deﬁne a lexical chain: it is a sequence of related\nwords in a text, spanning short (adjacent words or sentences) or long distances (en-\ntire text). The authors’ method progressed with the following steps: segmentation\nof the text, identiﬁcation of lexical chains, and using strong lexical chains to identify\nthe sentences worthy of extraction. They tried to reach a middle ground between\n(McKeown and Radev, 1995) and (Luhn, 1958) where the former relied on deep\n', 2, 0)
(120.43699645996094, 646.1558227539062, 270.6294250488281, 667.6510009765625, '7See http://search.live.com/news.\n8See http://en.wikipedia.org.\n', 3, 0)
(303.27301025390625, 692.2910766601562, 308.7275695800781, 705.5892944335938, '8\n', 4, 0)

page suivante
(107.99998474121094, 72.38806915283203, 504.0220947265625, 139.88328552246094, 'semantic structure of the text, while the latter relied on word statistics of the doc-\numents. The authors describe the notion of cohesion in text as a means of sticking\ntogether diﬀerent parts of the text. Lexical cohesion is a notable example where\nsemantically related words are used. For example, let us take a look at the following\nsentence.9\n', 0, 0)
(215.70297241210938, 140.13409423828125, 504.0028381347656, 153.4322967529297, 'John bought a Jag. He loves the car.\n(4)\n', 1, 0)
(107.99996948242188, 160.080078125, 504.0547180175781, 254.67332458496094, 'Here, the word car refers to the word Jag in the previous sentence, and exempliﬁes\nlexical cohesion. The phenomenon of cohesion occurs not only at the word level,\nbut at word sequences too, resulting in lexical chains, which the authors used as\na source representation for summarization. Semantically related words and word\nsequences were identiﬁed in the document, and several chains were extracted, that\nform a representation of the document. To ﬁnd out lexical chains, the authors used\nWordnet (Miller, 1995), applying three generic steps:\n', 2, 0)
(121.33296966552734, 263.75909423828125, 300.2858581542969, 277.0572814941406, '1. Selecting a set of candidate words.\n', 3, 0)
(121.33296966552734, 286.22210693359375, 504.07867431640625, 313.0693054199219, '2. For each candidate word, ﬁnding an appropriate chain relying on a relatedness\ncriterion among members of the chains,\n', 4, 0)
(121.33296203613281, 322.234130859375, 487.70416259765625, 335.5323181152344, '3. If it is found, inserting the word in the chain and updating it accordingly.\n', 5, 0)
(107.99995422363281, 344.6181335449219, 504.07196044921875, 655.9984130859375, 'The relatedness was measured in terms of Wordnet distance. Simple nouns and\nnoun compounds were used as starting point to ﬁnd the set of candidates. In the\nﬁnal steps, strong lexical chains were used to create the summaries. The chains were\nscored by their length and homogeneity. Then the authors used a few heuristics to\nselect the signiﬁcant sentences.\nIn another paper, Ono et al. (1994) put forward a computational model of dis-\ncourse for Japanese expository writings, where they elaborate a practical procedure\nfor extracting the discourse rhetorical structure, a binary tree representing relations\nbetween chunks of sentences (rhetorical structure trees are used more intensively in\n(Marcu, 1998a), as we will see below). This structure was extracted using a series\nof NLP steps: sentence analysis, rhetorical relation extraction, segmentation, can-\ndidate generation and preference judgement. Evaluation was based on the relative\nimportance of rhetorical relations. In the following step, the nodes of the rhetori-\ncal structure tree were pruned to reduce the sentence, keeping its important parts.\nSame was done for paragraphs to ﬁnally produce the summary. Evaluation was done\nwith respect to sentence coverage and 30 editorial articles of a Japanese newspaper\nwere used as the dataset. The articles had corresponding sets of key sentences and\nmost important key sentences judged by human subjects. The key sentence coverage\nwas about 51% and the most important key sentence coverage was 74%, indicating\nencouraging results.\nMarcu (1998a) describes a unique approach towards summarization that, unlike\nmost other previous work, does not assume that the sentences in a document form\na ﬂat sequence.\nThis paper used discourse based heuristics with the traditional\n', 6, 0)
(120.43699645996094, 664.6817626953125, 413.1444091796875, 675.218017578125, '9Example from http://www.cs.ucd.ie/staff/jcarthy/home/Lex.html.\n', 7, 0)
(303.2729797363281, 692.2910766601562, 308.7275390625, 705.5892944335938, '9\n', 8, 0)

page suivante
(108.0, 72.38806915283203, 504.0439758300781, 194.07933044433594, 'features that have been used in the summarization literature. The discourse theory\nused in this paper is the Rhetorical Structure Theory (RST) that holds between\ntwo non-overlapping pieces of text spans: the nucleus and the satellite. The author\nmentions that the distinction between nuclei and satellites comes from the empir-\nical observation that the nucleus expresses what is more essential to the writer’s\npurpose than the satellite; and that the nucleus of a rhetorical relation is compre-\nhensible independent of the satellite, but not vice versa. Marcu (1998b) describes\nthe details of a rhetorical parser producing a discourse tree.\nFigure 2 shows an\nexample discourse tree for a text example detailed in the paper. Once such a dis-\n', 0, 0)
(450.63275146484375, 313.31085205078125, 478.8002014160156, 320.2552185058594, 'Antithesis\n', 1, 0)
(298.08837890625, 213.59315490722656, 301.0645446777344, 219.54547119140625, '2\n', 2, 0)
(281.3226318359375, 224.02622985839844, 313.72503662109375, 230.9705810546875, 'Elaboration\n', 3, 0)
(171.6017608642578, 268.6687316894531, 204.004150390625, 275.61309814453125, 'Elaboration\n', 4, 0)
(186.4825439453125, 258.2354736328125, 189.4586944580078, 264.1877746582031, '2\n', 5, 0)
(134.39984130859375, 302.8778076171875, 137.37599182128906, 308.8301086425781, '2\n', 6, 0)
(225.53631591796875, 313.31085205078125, 257.9386901855469, 320.2552185058594, 'Elaboration\n', 7, 0)
(242.3019561767578, 302.8778076171875, 245.27810668945312, 308.8301086425781, '3\n', 8, 0)
(119.51903533935547, 313.31085205078125, 153.4700164794922, 320.2552185058594, 'Justification\n', 9, 0)
(409.6941223144531, 258.2354736328125, 412.6702880859375, 264.1877746582031, '8\n', 10, 0)
(389.22479248046875, 268.6685485839844, 433.97430419921875, 275.6129150390625, 'Exemplification\n', 11, 0)
(117.6341552734375, 345.63525390625, 381.0571594238281, 353.4726257324219, '1\n2\n3\n4\n5\n7\n8\n', 12, 0)
(251.5610809326172, 390.3108825683594, 317.79718017578125, 398.1149597167969, '4\n5\n', 13, 0)
(361.3484191894531, 302.8780212402344, 467.7292175292969, 308.830322265625, '8\n10\n', 14, 0)
(437.5712890625, 345.6685791015625, 493.7872619628906, 351.6208801269531, '9\n10\n', 15, 0)
(296.2373046875, 434.95318603515625, 343.8558044433594, 440.9054870605469, '5\n6\n', 16, 0)
(266.44189453125, 359.8050231933594, 289.9751281738281, 366.7493896484375, 'Contrast\n', 17, 0)
(303.67681884765625, 402.5954895019531, 329.5157775878906, 409.53985595703125, 'Evidence\n', 18, 0)
(342.73052978515625, 313.3111572265625, 374.7509765625, 320.2555236816406, 'Concession\n', 19, 0)
(108.0, 458.3750915527344, 504.0330505371094, 512.3203125, 'Figure 2: Example of a discourse tree from Marcu (1998a). The numbers in the\nnodes denote sentence numbers from the text example. The text below the number\nin selected nodes are rhetorical relations. The dotted nodes are SATELLITES and\nthe normals ones are the NUCLEI.\n', 20, 0)
(108.0, 522.4051513671875, 504.0549011230469, 671.1953125, 'course structure is created, a partial ordering of important units can be developed\nfrom the tree. Each equivalence class in the partial ordering is derived from the\nnew sentences at a particular level of the discourse tree. In Figure 2, we observe\nthat sentence 2 is at the root, followed by sentence 8 in the second level. In the\nthird level, sentence 3 and 10 are observed, and so forth. The equivalence classes\nare 2 > 8 > 3, 10 > 1, 4, 5, 7, 9 > 6.\nIf it is speciﬁed that the summary should contain the top k% of the text, the ﬁrst\nk% of the units in the partial ordering can be selected to produce the summary. The\nauthor talks about a summarization system based just on this method in (Marcu,\n1998b) and in one of his earlier papers. In this paper, he merged the discourse\nbased heuristics with traditional heuristics. The metrics used were clustering based\n', 21, 0)
(300.54510498046875, 692.2910766601562, 311.4542236328125, 705.5892944335938, '10\n', 22, 0)

page suivante
(107.99996185302734, 72.38806915283203, 504.0548400878906, 316.0223693847656, 'metric (each node in the discourse tree was assigned a cluster score; for leaves the\nscore was 0, for the internal nodes it was given by the similarity of the immediate\nchildren; discourse tree A was chosen to be better than B if its clustering score\nwas higher), marker based metric (a discourse structure A was chosen to be better\nthan a discourse structure B if A used more rhetorical relations than B), rhetorical\nclustering based technique (measured the similarity between salient units of two text\nspans), shape based metric (preferred a discourse tree A over B if A was more skewed\ntowards the right than B), title based metric, position based metric, connectedness\nbased metric (cosine similarity of an unit to all other text units, a discourse structure\nA was chosen to be better than B if its connectedness measure was more than B).\nA weighted linear combination of all these scores gave the score of a discourse\nstructure.\nTo ﬁnd the best combination of heuristics, the author computed the\nweights that maximized the F-score on the training dataset, which was constituted\nby newswire articles. To do this, he used a GSAT-like algorithm (Selman et al.,\n1992) that performed a greedy search in a seven dimensional space of the metrics.\nFor a part of his corpus (the TREC dataset), a best F-score of 75.42% was achieved\nfor the 10% summaries which was 3.5% higher than a baseline lead based algorithm,\nwhich was very encouraging.\n', 0, 0)
(107.99996948242188, 336.907470703125, 361.7841491699219, 351.26800537109375, '3\nMulti-Document Summarization\n', 1, 0)
(107.99996948242188, 361.44818115234375, 504.0220642089844, 415.3943786621094, 'Extraction of a single summary from multiple documents has gained interest since\nmid 1990s, most applications being in the domain of news articles. Several Web-\nbased news clustering systems were inspired by research on multi-document summa-\nrization, for example Google News,10 Columbia NewsBlaster,11 or News In Essence.12\n', 2, 0)
(108.0, 415.6451721191406, 504.0549621582031, 510.2384033203125, 'This departs from single-document summarization since the problem involves mul-\ntiple sources of information that overlap and supplement each other, being contra-\ndictory at occasions. So the key tasks are not only identifying and coping with\nredundancy across documents, but also recognizing novelty and ensuring that the\nﬁnal summary is both coherent and complete.\nThe ﬁeld seems to have been pioneered by the NLP group at Columbia University\n(McKeown and Radev, 1995), where a summarization system called SUMMONS13\n', 3, 0)
(108.0, 510.490234375, 504.0657653808594, 618.6324462890625, 'was developed by extending already existing technology for template-driven message\nunderstanding systems. Although in that early stage multi-document summariza-\ntion was mainly seen as a task requiring substantial capabilities of both language\ninterpretation and generation, it later gained autonomy, as people coming from dif-\nferent communities added new perspectives to the problem. Extractive techniques\nhave been applied, making use of similarity measures between pairs of sentences.\nApproaches vary on how these similarities are used: some identify common themes\nthrough clustering and then select one sentence to represent each cluster (McKeown\n', 4, 0)
(116.78399658203125, 627.9738159179688, 303.5804138183594, 671.3845825195312, '10See http://news.google.com.\n11See http://newsblaster.cs.columbia.edu.\n12See http://NewsInEssence.com.\n13SUMMarizing Online NewS articles.\n', 5, 0)
(300.5459899902344, 692.2910766601562, 311.4551086425781, 705.5892944335938, '11\n', 6, 0)

page suivante
(108.0, 72.38806915283203, 504.0877380371094, 410.8674011230469, 'et al., 1999; Radev et al., 2000), others generate a composite sentence from each\ncluster (Barzilay et al., 1999), while some approaches work dynamically by includ-\ning each candidate passage only if it is considered novel with respect to the previous\nincluded passages, via maximal marginal relevance (Carbonell and Goldstein, 1998).\nSome recent work extends multi-document summarization to multilingual environ-\nments (Evans, 2005).\nThe way the problem is posed has also varied over time. While in some pub-\nlications it is claimed that extractive techniques would not be eﬀective for multi-\ndocument summarization (McKeown and Radev, 1995; McKeown et al., 1999), some\nyears later that claim was overturned, as extractive systems like MEAD14 (Radev\net al., 2000) achieved good performance in large scale summarization of news arti-\ncles. This can be explained by the fact that summarization systems often distinguish\namong themselves about what their goal actually is. While some systems, like SUM-\nMONS, are designed to work in strict domains, aiming to build a sort of brieﬁng\nthat highlights diﬀerences and updates accross diﬀerent news reports, putting much\nemphasis on how information is presented to the user, others, like MEAD, are large\nscale systems that intend to work in general domains, being more concerned with\ninformation content rather than form. Consequently, systems of the former kind re-\nquire a strong eﬀort on language generation to produce a grammatical and coherent\nsummary, while latter systems are probably more close to the information retrieval\nparadigm. Abstractive systems like SUMMONS are diﬃcult to replicate, as they\nheavily rely on the adaptation of internal tools to perform information extraction\nand language generation. On the other hand, extractive systems are generally easy\nto implement from scratch, and this makes them appealing when sophisticated NLP\ntools are not available.\n', 0, 0)
(108.00001525878906, 428.3857421875, 350.7742919921875, 440.3529052734375, '3.1\nAbstraction and Information Fusion\n', 1, 0)
(108.0, 447.376220703125, 504.0456848144531, 528.4204711914062, 'As far as we know, SUMMONS (McKeown and Radev, 1995; Radev and McKeown,\n1998) is the ﬁrst historical example of a multi-document summarization system. It\ntackles single events about a narrow domain (news articles about terrorism) and\nproduces a brieﬁng merging relevant information about each event and how reports\nby diﬀerent news agencies have evolved over time. The whole thread of reports is\nthen presented, as illustrated in the following example of a “good” summary:\n', 2, 0)
(135.2729949951172, 540.626220703125, 476.77142333984375, 608.1214599609375, '“In the afternoon of February 26, 1993, Reuters reported that a suspect\nbomb killed at least ﬁve people in the World Trade Center. However,\nAssociated Press announced that exactly ﬁve people were killed in the\nblast.\nFinally, Associated Press announced that Arab terrorists were\npossibly responsible for the terrorist act.”\n', 3, 0)
(108.0, 620.3272705078125, 504.06097412109375, 647.1754760742188, 'Rather than working with raw text, SUMMONS reads a database previously\nbuilt by a template-based message understanding system. A full multi-document\n', 4, 0)
(116.78399658203125, 655.9107666015625, 394.21441650390625, 666.447021484375, '14Available for download at http://www.summarization.com/mead/.\n', 5, 0)
(300.54498291015625, 692.2910766601562, 311.4541015625, 705.5892944335938, '12\n', 6, 0)

page suivante
(107.9999771118164, 72.38806915283203, 504.06591796875, 641.2034912109375, 'summarizer is built by concatenating the two systems, ﬁrst processing full text as\ninput and ﬁlling template slots, and then synthesizing a summary from the extracted\ninformation. The architecture of SUMMONS consists of two major components: a\ncontent planner that selects the information to include in the summary through\ncombination of the input templates, and a linguistic generator that selects the right\nwords to express the information in grammatical and coherent text.\nThe latter\ncomponent was devised by adapting existing language generation tools, namely the\nFUF/SURGE system15.\nContent planning, on the other hand, is made through\nsummary operators, a set of heuristic rules that perform operations like “change of\nperspective”, “contradiction”, “reﬁnement”, etc. Some of these operations require\nresolving conﬂicts, i.e., contradictory information among diﬀerent sources or time\ninstants; others complete pieces of information that are included in some articles\nand not in others, combining them into a single template. At the end, the linguis-\ntic generator gathers all the combined information and uses connective phrases to\nsynthesize a summary.\nWhile this framework seems promising when the domain is narrow enough so that\nthe templates can be designed by hand, a generalization for broader domains would\nbe problematic. This was improved later by McKeown et al. (1999) and Barzilay\net al. (1999), where the input is now a set of related documents in raw text, like\nthose retrieved by a standard search engine in response to a query. The system starts\nby identifying themes, i.e., sets of similar text units (usually paragraphs). This is\nformulated as a clustering problem. To compute a similarity measure between text\nunits, these are mapped to vectors of features, that include single words weighted\nby their TF-IDF scores, noun phrases, proper nouns, synsets from the Wordnet\ndatabase and a database of semantic classes of verbs. For each pair of paragraphs, a\nvector is computed that represents matches on the diﬀerent features. Decision rules\nthat were learned from data are then used to classify each pair of text units either\nas similar or dissimilar; this in turn feeds a subsequent algorithm that places the\nmost related paragraphs in the same theme.\nOnce themes are identiﬁed, the system enters its second stage: information fu-\nsion. The goal is to decide which sentences of a theme should be included in the\nsummary. Rather than just picking a sentence that is a group representative, the\nauthors propose an algorithm which compares and intersects predicate argument\nstructures of the phrases within each theme to determine which are repeated often\nenough to be included in the summary. This is done as follows: ﬁrst, sentences are\nparsed through Collins’ statistical parser (Collins, 1999) and converted into depen-\ndency trees, which allows capturing the predicate-argument structure and identify\nfunctional roles. Determiners and auxiliaries are dropped; Fig. 3 shows a sentence\nrepresentation.\nThe comparison algorithm then traverses these dependency trees recursively,\nadding identical nodes to the output tree. Once full phrases (a verb with at least\ntwo constituents) are found, they are marked to be included in the summary. If two\n', 0, 0)
(108.0, 650.5447998046875, 504.04620361328125, 672.0400390625, '15FUF, SURGE, and other tools developed by the Columbia NLP group are available at\nhttp://www1.cs.columbia.edu/nlp/tools.cgi.\n', 1, 0)
(300.54498291015625, 692.2910766601562, 311.4541015625, 705.5892944335938, '13\n', 2, 0)

page suivante
(13.939056396484375, 75.01750183105469, 191.67095947265625, 91.95105743408203, 'and Kan 1998]. We match two verbs that share the same\nsemantic class in this classiﬁ cation.\n', 0, 0)
(13.939128875732422, 96.68247985839844, 191.63336181640625, 104.46448516845703, 'In addition to the above primitive features that all com-\n', 1, 0)
(6.187999725341797, 105.18046569824219, 193.34510803222656, 232.30694580078125, 'pare single items from each text unit, we use composite fea-\ntures that combine pairs of primitive features. Our compos-\nite features impose particular constraints on the order of the\ntwo elements in the pair, on the maximum distance between\nthe two elements, and on the syntactic classes that the two\nelements come from. They can vary from a simple com-\nbination (e.g., “two text units must share two words to be\nsimilar”) to complex cases with many conditions (e.g., “two\ntext units must have matching noun phrases that appear in\nthe same order and with relative difference in position no\nmore than ﬁ ve”). In this manner, we capture information\non how similarly related elements are spaced out in the two\ntext units, as well as syntactic information on word combi-\nnations. Matches on composite features indicate combined\nevidence for the similarity of the two units.\n', 2, 0)
(13.938888549804688, 233.1162872314453, 191.63299560546875, 240.89828491210938, 'To determine whether the units match overall, we employ\n', 3, 0)
(6.187751770019531, 240.96046447753906, 193.25149536132812, 292.0726318359375, 'a machine learning algorithm [Cohen 1996] that induces de-\ncision rules using the features that really make a difference.\nA set of pairs of units already marked as similar or not by a\nhuman is used for training the classiﬁ er. We have manually\nmarked a set of 8,225 paragraph comparisons from the TDT\ncorpus for training and evaluating our similarity classiﬁ er.\n', 4, 0)
(13.938606262207031, 292.78857421875, 191.56932067871094, 300.570556640625, 'For comparison, we also use an implementation of the\n', 5, 0)
(6.187587738037109, 301.2865295410156, 191.8272247314453, 351.7450866699219, 'TF*IDF method which is standard for matching texts in in-\nformation retrieval. We compute the total frequency (TF) of\nwords in each text unit and the number of units in our train-\ning set each word appears in (DF, or document frequency).\nThen each text unit is represented as a vector of TF*IDF\nscores, calculated as\n', 6, 0)
(39.338897705078125, 356.5699462890625, 157.67335510253906, 375.1066589355469, 'TF(wordi) · log Total number of units\n', 7, 0)
(106.7628173828125, 365.41827392578125, 141.9141082763672, 375.4414978027344, 'DF(wordi)\n', 8, 0)
(6.1881103515625, 380.5698547363281, 191.60215759277344, 422.53045654296875, 'Similarity between text units is measured by the cosine of\nthe angle between the corresponding two vectors (i.e., the\nnormalized inner product of the two vectors), and the opti-\nmal value of a threshold for judging two units as similar is\ncomputed from the training set.\n', 9, 0)
(13.93899154663086, 423.24639892578125, 191.64752197265625, 431.02838134765625, 'After all pairwise similarities between text units have\n', 10, 0)
(6.187976837158203, 431.744384765625, 193.13369750976562, 524.8793334960938, 'been calculated, we utilize a clustering algorithm to iden-\ntify themes. As a paragraph may belong to multiple themes,\nmost standard clustering algorithms, which partition their\ninput set, are not suitable for our task. We use a greedy,\none-pass algorithm that ﬁ rst constructs groups from the most\nsimilar paragraphs, seeding the groups with the fully con-\nnected subcomponents of the graph that the similarity rela-\ntionship induces over the set of paragraphs, and then places\nadditional paragraphs within a group if the fraction of the\nmembers of the group they are similar to exceeds a preset\nthreshold.\n', 11, 0)
(55.68153381347656, 533.766357421875, 142.1752166748047, 543.104736328125, 'Language Generation\n', 12, 0)
(6.187889099121094, 546.606689453125, 191.72642517089844, 579.975830078125, 'Given a group of similar paragraphs—a theme—the prob-\nlem is to create a concise and ﬂuent fusion of information in\nthis theme, reﬂecting facts common to all paragraphs. A\nstraightforward method would be to pick a representative\n', 13, 0)
(248.33200073242188, 110.1116714477539, 268.5442199707031, 128.9014892578125, 'subject\n', 14, 0)
(235.6350555419922, 143.74664306640625, 281.8373718261719, 151.16937255859375, 'class: noun\n', 15, 0)
(248.18040466308594, 175.4257354736328, 263.6776428222656, 186.55982971191406, '27\n', 16, 0)
(226.0414276123047, 175.9564666748047, 384.4146423339844, 193.58517456054688, 'class: cardinal\nbombing \n', 17, 0)
(328.12646484375, 186.69241333007812, 374.3288269042969, 194.11514282226562, 'class: noun\n', 18, 0)
(229.7316436767578, 133.0106964111328, 362.3228454589844, 144.6750030517578, 'McVeigh\nwith\n', 19, 0)
(310.9072265625, 144.27737426757812, 382.9382019042969, 151.70010375976562, 'class: preposition\n', 20, 0)
(326.65020751953125, 194.46896362304688, 377.4413757324219, 201.89169311523438, 'definite: yes\n', 21, 0)
(277.6990966796875, 80.52113342285156, 322.45269775390625, 91.65522766113281, 'charge\n', 22, 0)
(244.4901885986328, 91.2577896118164, 365.4642028808594, 106.63339233398438, 'class: verb\nvoice :passive\npolarity: +\ntense: past\n', 23, 0)
(212.84695434570312, 211.45143127441406, 398.2065124511719, 227.73135375976562, 'Figure 4: Dependency grammar representation of the sen-\ntence “McVeigh, 27, was charged with the bombing”.\n', 24, 0)
(212.84634399414062, 245.1630401611328, 398.32122802734375, 329.7067565917969, 'sentence that meets some criteria (e.g., a threshold number\nof common content words). In practice, however, any repre-\nsentative sentence will usually include embedded phrase(s)\ncontaining information that is not common to all sentences\nin the theme. Furthermore, other sentences in the theme of-\nten contain additional information not presented in the rep-\nresentative sentence. Our approach, therefore, uses inter-\nsection among theme sentences to identify phrases common\nto most paragraphs and then generates a new sentence from\nidentiﬁ ed phrases.\n', 25, 0)
(212.84634399414062, 337.9926452636719, 347.2208557128906, 346.513916015625, 'Intersection among Theme Sentences\n', 26, 0)
(212.84579467773438, 350.5002746582031, 398.5635681152344, 417.95465087890625, 'Intersection is carried out in the content planner, which uses\na parser for interpreting the input sentences, with our new\nwork focusing on the comparison of phrases. Theme sen-\ntences are ﬁ rst run through a statistical parser[Collins 1996]\nand then, in order to identify functional roles (e.g., subject,\nobject), are converted to a dependency grammar representa-\ntion [Kittredge and Mel’ˇcuk 1983], which makes predicate-\nargument structure explicit.\n', 27, 0)
(220.5967254638672, 418.67059326171875, 398.3298034667969, 426.45257568359375, 'We developed a rule-based component to produce func-\n', 28, 0)
(212.84524536132812, 427.1685485839844, 398.3296203613281, 494.7162780761719, 'tional roles, which transforms the phrase-structure output of\nCollins’ parser to dependency grammar; function words (de-\nterminers and auxiliaries) are eliminated from the tree and\ncorresponding syntactic features are updated. An example\nof a theme sentence and its dependency grammar represen-\ntation are shown in Figure 4. Each non-auxiliary word in the\nsentence has a node in the representation, and this node is\nconnected to its direct dependents.\n', 29, 0)
(220.5962677001953, 495.4322509765625, 398.25152587890625, 503.2142333984375, 'The comparison algorithm starts with all subtrees rooted\n', 30, 0)
(212.84515380859375, 503.93017578125, 398.28253173828125, 545.8906860351562, 'at verbs from the input dependency structure, and traverses\nthem recursively: if two nodes are identical, they are added\nto the output tree, and their children are compared. Once\na full phrase (verb with at least two constituents) has been\nfound, it is conﬁ rmed for inclusion in the summary.\n', 31, 0)
(220.59580993652344, 546.606689453125, 399.8295593261719, 554.388671875, 'Difﬁ culties arise when two nodes are not identical, but are\n', 32, 0)
(212.8447723388672, 555.1046142578125, 399.791748046875, 579.9758911132812, 'similar. Such phrases may be paraphrases of each other and\nstill convey essentially the same information. Since theme\nsentences are a priori close semantically, this signiﬁ cantly\n', 33, 0)
(108.0, 222.68206787109375, 504.0111389160156, 249.52928161621094, 'Figure 3: Dependency tree representing the sentence “McVeigh, 27, was charged\nwith the bombing” (extracted from (McKeown et al., 1999)).\n', 34, 0)
(107.99999237060547, 269.8420715332031, 504.0439453125, 350.88629150390625, 'phrases, rooted at some node, are not identical but yet similar, the hypothesis that\nthey are paraphrases of each other is considered; to take this into account, corpus-\ndriven paraphrasing rules are written to allow paraphrase intersection.16 Once the\nsummary content (represented as predicate-argument structures) is decided, a gram-\nmatical text is generated by translating those structures into the arguments expected\nby the FUF/SURGE language generation system.\n', 35, 0)
(108.0, 368.4056396484375, 371.6600036621094, 380.372802734375, '3.2\nTopic-driven Summarization and MMR\n', 36, 0)
(107.99995422363281, 387.3961181640625, 504.0549011230469, 545.5678100585938, 'Carbonell and Goldstein (1998) made a major contribution to topic-driven sum-\nmarization by introducing the maximal marginal relevance (MMR) measure. The\nidea is to combine query relevance with information novelty; it may be applicable\nin several tasks ranging from text retrieval to topic-driven summarization. MMR\nsimultaneously rewards relevant sentences and penalizes redundant ones by consid-\nering a linear combination of two similarity measures.\nLet Q be a query or user proﬁle and R a ranked list of documents retrieved by\na search engine. Consider an incremental procedure that selects documents, one at\na time, and adds them to a set S. So let S be the set of already selected documents\nin a particular step, and R \\ S the set of yet unselected documents in R. For each\ncandidate document Di ∈ R \\ S, its marginal relevance MR(Di) is computed as:\n', 37, 0)
(178.53797912597656, 547.3961791992188, 504.001953125, 572.842529296875, 'MR(Di) := λSim1(Di, Q) − (1 − λ) max\nDj∈S Sim2(Di, Dj)\n(5)\n', 38, 0)
(108.00003051757812, 578.2651977539062, 504.0211181640625, 606.7493896484375, 'where λ is a parameter lying in [0, 1] that controls the relative importance given\nto relevance versus redundancy. Sim1 and Sim2 are two similarity measures; in the\n', 39, 0)
(108.0, 613.8488159179688, 504.0817565917969, 668.2175903320312, '16A full description of the kind of paraphrasing rules used can be found in (Barzilay et al.,\n1999). Examples are: ordering of sentence components, main clause vs. relative clause, realization\nin diﬀerent syntactic categories (e.g.\nclassiﬁer vs.\napposition), change in grammatical features\n(active/passive, time, number, etc.), head omission, transformation from one POS to another,\nusing semantically related words (e.g. synonyms), etc.\n', 40, 0)
(300.54498291015625, 692.2910766601562, 311.4541015625, 705.5892944335938, '14\n', 41, 0)

page suivante
(108.0, 72.38806915283203, 504.0438537597656, 100.13650512695312, 'experiments both were set to the standard cosine similarity traditionally used in the\nvector space model, Sim1(x, y) = Sim2(x, y) =\n⟨x,y⟩\n', 0, 0)
(107.99990844726562, 85.93708038330078, 504.0548095703125, 317.5493469238281, '∥x∥·∥y∥. The document achieving the\nhighest marginal relevance, DMMR = arg maxDi∈R\\S MR(Di), is then selected, i.e.,\nadded to S, and the procedure continues until a maximum number of documents\nare selected or a minimum relevance threshold is attained. Carbonell and Goldstein\n(1998) found experimentally that choosing dynamically the value of λ turns out to be\nmore eﬀective than keeping it ﬁxed, namely starting with small values (λ ≈ 0.3) to\ngive more emphasis to novelty, and then increasing it (λ ≈ 0.7) to focus on the most\nrelevant documents. To perform summarization, documents can be ﬁrst segmented\ninto sentences or paragraphs, and after a query is submitted, the MMR algorithm\ncan be applied followed by a selection of the top ranking passages, reordering them as\nthey appeared in the original documents, and presenting the result as the summary.\nOne of the attractive points in using MMR for summarization is its topic-oriented\nfeature, through its dependency on the query Q, which makes it particularly ap-\npealing to generate summaries according to a user proﬁle: as the authors claim, “a\ndiﬀerent user with diﬀerent information needs may require a totally diﬀerent sum-\nmary of the same document.” This assertion was not being taken into account by\nprevious multi-document summarization systems.\n', 1, 0)
(107.99990844726562, 335.06768798828125, 304.89007568359375, 347.03485107421875, '3.3\nGraph Spreading Activation\n', 2, 0)
(107.99990844726562, 354.05816650390625, 504.0548400878906, 435.1023864746094, 'Mani and Bloedorn (1997) describe an information extraction framework for sum-\nmarization, a graph-based method to ﬁnd similarities and dissimilarities in pairs\nof documents. Albeit no textual summary is generated, the summary content is\nrepresented via entities (concepts) and relations that are displayed respectively as\nnodes and edges of a graph. Rather than extracting sentences, they detect salient\nregions of the graph via a spreading activation technique.17\n', 3, 0)
(107.99992370605469, 435.3531799316406, 504.0547790527344, 624.7914428710938, 'This approach shares with the method described in Section 3.2 the property\nof being topic-driven; there is an additional input that stands for the topic with\nrespect to which the summary is to be generated. The topic is represented through\na set of entry nodes in the graph. A document is represented as a graph as follows:\neach node represents the occurrence of a single word (i.e., one word together with\nits position in the text).\nEach node can have several kinds of links: adjacency\nlinks (ADJ) to adjacent words in the text, SAME links to other occurrences of the\nsame word, and ALPHA links encoding semantic relationships captured through\nWordnet and NetOwl18. Besides these, PHRASE links tie together sequences of\nadjacent nodes which belong to the same phrase, and NAME and COREF links\nstand for co-referential name occurrences; Fig. 4 shows some of these links.\nOnce the graph is built, topic nodes are identiﬁed by stem comparison and be-\ncome the entry nodes. A search for semantically related text is then propagated from\nthese to the other nodes of the graph, in a process called spreading activation. Salient\n', 4, 0)
(108.0, 633.5267944335938, 504.04620361328125, 665.9810180664062, '17The name “spreading activation” is borrowed from a method used in information retrieval\n(Salton and Buckley, 1988) to expand the search vocabulary.\n18See http://www.netowl.com.\n', 5, 0)
(300.54498291015625, 692.2910766601562, 311.4541015625, 705.5892944335938, '15\n', 6, 0)

page suivante
(143.2908935546875, 319.57659912109375, 445.7315979003906, 330.8709411621094, '1.39: Aoki, the Japanese ambassador, said in telephone calls to\n', 0, 0)
(143.29092407226562, 336.5181884765625, 453.8749084472656, 364.7540588378906, 'Fujimori.\nJapanesebroadcaster NHK that the rebels wanted to talk directly to\n', 1, 0)
(143.29092407226562, 376.0484619140625, 436.9332580566406, 398.6371765136719, '1.43:According to some estimates, only a couple hundred armed\nfollowers remain.\n', 2, 0)
(499.0635986328125, 59.80607604980469, 612.0072631835938, 71.10044860839844, '2.19 They are freeing u\n', 3, 0)
(499.0635986328125, 76.74763488769531, 609.5328979492188, 88.04200744628906, 'not doing us any harm,"\n', 4, 0)
(499.06329345703125, 179.52638244628906, 507.5340881347656, 190.8207550048828, '...\n', 5, 0)
(499.06329345703125, 232.60997009277344, 616.7186889648438, 259.7164001464844, '2.27:Although the MRTA\nearly days in the mid-198\n', 6, 0)
(499.06329345703125, 278.9168701171875, 615.4631958007812, 306.0233459472656, 'give to the poor, it lost pu\nturning increasingly to ki\n', 7, 0)
(499.06329345703125, 342.16534423828125, 614.196533203125, 369.2718200683594, 'billion in damage to the c\nsince 1980.\n', 8, 0)
(499.06329345703125, 308.2822265625, 612.9445190429688, 319.5765686035156, 'and drug activities. 2.28:\n', 9, 0)
(499.06329345703125, 325.22381591796875, 612.616943359375, 337.6475524902344, 'Peru have cost at least 3\n', 10, 0)
(143.2906494140625, 180.6558380126953, 151.7614288330078, 191.95021057128906, '...\n', 11, 0)
(143.2906494140625, 207.76231384277344, 239.281494140625, 219.0566864013672, 'close ties with Japan.\n', 12, 0)
(143.2906494140625, 234.8688201904297, 476.4182434082031, 259.7164001464844, '1.33: Among the hostages were Japanese Ambassador Morihisa Aoki and\nthe ambassadors of Brazil, Bolivia, Cuba, Canada, South Korea,\n', 13, 0)
(143.2906494140625, 275.528564453125, 151.7614288330078, 286.8229064941406, '...\n', 14, 0)
(143.2906494140625, 360.236328125, 151.7614288330078, 371.5306701660156, '...\n', 15, 0)
(143.2906494140625, 400.89605712890625, 151.7614288330078, 412.1903991699219, '...\n', 16, 0)
(499.06329345703125, 200.98570251464844, 614.5012817382812, 223.57444763183594, '2.26:The MRTA called T\n"Breaking The Silence."\n', 17, 0)
(143.2906494140625, 194.20909118652344, 461.6903381347656, 205.5034637451172, '1.32: President Alberto Fujimori, who is of Japanese ancestry, has had\n', 18, 0)
(143.2906494140625, 261.9753112792969, 616.3876342773438, 274.39910888671875, 'Germany, Austria and Venezuela.\nHood-style movement tha\n', 19, 0)
(600.3626098632812, -10.21888542175293, 603.5005493164062, 1.0754847526550293, 'j\n', 20, 0)
(499.06329345703125, 5.59324836730957, 613.5770263671875, 48.51182556152344, 'negotiations with the gov\ndawn on Wednesday.\n...\n', 21, 0)
(499.06329345703125, 90.30101013183594, 617.3399658203125, 180.65599060058594, '...\n2.22:The attack was a ma\nFujimori’s government, w\nvirtual victory in a 16-yea\nrebels belonging to the M\nand better-known Maoist\n', 22, 0)
(143.2906494140625, 99.33638000488281, 452.2613525390625, 124.18397521972656, '...\n1.28:Many leaders of the Tupac Amaru which is smaller than Peru’s\n', 23, 0)
(143.2906494140625, 139.99610900878906, 437.522216796875, 151.2904815673828, 'was captured in June 1992 and is serving a life sentence, as is his\n', 24, 0)
(143.2906494140625, -2.3129405975341797, 366.56005859375, 20.275802612304688, 'us: ‘Don’t lift your heads up or you will be shot."\n1.19:\n', 25, 0)
(143.2906494140625, 72.22987365722656, 471.6407775878906, 97.07752990722656, 'hostages," a rebel who did not give his name told  a local radio station in\na telephone call from inside the compound.\n', 26, 0)
(165.87939453125, -2.3129405975341797, 475.8421325683594, 8.981430053710938, '"The guerillas stalked around the residence grounds threatening\n', 27, 0)
(143.2906494140625, 153.5493621826172, 267.1899108886719, 164.84373474121094, 'lieutenant, Peter Cardenas. \n', 28, 0)
(143.2906494140625, 58.67649841308594, 488.2190856933594, 69.97087097167969, '1.25: "We are clear: the liberation of all our comrades, or we die with all the\n', 29, 0)
(143.2906494140625, 167.1024932861328, 484.9453430175781, 178.39686584472656, '1.30:Other top commanders conceded defeat\nJuly 1993.\nand surrendered in\n', 30, 0)
(578.1239013671875, 34.60004425048828, 615.50830078125, 45.894412994384766, 'COREF\n', 31, 0)
(143.2906494140625, 126.44273376464844, 462.6616516113281, 137.7371063232422, 'Maoist Shining Path movement are in jail. 1.29:Its chief, Victor Polay,\n', 32, 0)
(233.64561462402344, 96.71894073486328, 255.85035705566406, 108.0133056640625, 'ADJ\n', 33, 0)
(143.2906494140625, 291.3404235839844, 342.8039855957031, 313.92913818359375, '1.38:Fujimori whose sister was among the\nan emergency cabinet meeting today.\n', 34, 0)
(346.58929443359375, 291.3404235839844, 463.6007995605469, 302.634765625, 'hostages released, called\n', 35, 0)
(340.942138671875, 150.97320556640625, 384.5428771972656, 164.52645874023438, 'ALPHA\n', 36, 0)
(386.11962890625, 83.20697784423828, 412.7652587890625, 96.76022338867188, 'ADJ\n', 37, 0)
(256.234375, 25.92271614074707, 452.9145812988281, 37.21708679199219, ', the rebels threatened to kill the remaining \n', 38, 0)
(143.2906494140625, 25.92271614074707, 252.4733428955078, 54.15861511230469, 'captives.\n1.24:Early Wednesday\n', 39, 0)
(138.14492797851562, 436.4067077636719, 616.940673828125, 462.61334228515625, 'Figure 5: Texts of two related articles. The top 5 salient sentences containing common\nwords in bold face; likewise, the top 5 salient sentences containing unique words have th\n', 40, 0)
(108.0, 191.60906982421875, 504.06573486328125, 218.45726013183594, 'Figure 4: Examples of nodes and links in the graph for a particular sentence (detail\nextracted from from a ﬁgure in (Mani and Bloedorn, 1997)).\n', 41, 0)
(108.0, 238.77008056640625, 504.0765686035156, 441.7573547363281, 'words and phrases are initialized according to their TF-IDF score. The weight of\nneighboring nodes depends on the node link traveled and is an exponentially decay-\ning function of the distance of the traversed path. Traveling within a sentence is\nmade cheaper than across sentence boundaries, which in turn is cheaper than across\nparagraph boundaries. Given a pair of document graphs, common nodes are identi-\nﬁed either by sharing the same stem or by being synonyms. Analogously, diﬀerence\nnodes are those that are not common. For each sentence in both documents, two\nscores are computed: one score that reﬂects the presence of common nodes, which\nis computed as the average weight of these nodes; and another score that computes\ninstead the average weights of diﬀerence nodes. Both scores are computed after\nspreading activation. In the end, the sentences that have higher common and dif-\nferent scores are highlighted, the user being able to specify the maximal number of\ncommon and diﬀerent sentences to control the output. In the future, the authors\nexpect to use these structure to actually compose abstractive summaries, rather\nthan just highlighting pieces of text.\n', 42, 0)
(108.00000762939453, 459.2767028808594, 321.02972412109375, 471.2438659667969, '3.4\nCentroid-based Summarization\n', 43, 0)
(108.00000762939453, 478.26617431640625, 504.0548400878906, 640.6053466796875, 'Although clustering techniques were already being employed by McKeown et al.\n(1999) and Barzilay et al. (1999) for identiﬁcation of themes, Radev et al. (2000)\npioneered the use of cluster centroids to play a central role in summarization. A full\ndescription of the centroid-based approach that underlies the MEAD system can\nbe found in (Radev et al., 2004); here we sketch brieﬂy the main points. Perhaps\nthe most appealing feature is the fact that it does not make use of any language\ngeneration module, unlike most previous systems. All documents are modeled as\nbags-of-words. The system is also easily scalable and domain-independent.\nThe ﬁrst stage consists of topic detection, whose goal is to group together news\narticles that describe the same event. To accomplish this task, an agglomerative\nclustering algorithm is used that operates over the TF-IDF vector representations\nof the documents, successively adding documents to clusters and recomputing the\n', 44, 0)
(300.5450134277344, 692.2911376953125, 311.4541320800781, 705.58935546875, '16\n', 45, 0)

page suivante
(108.0, 72.38806915283203, 213.28370666503906, 85.6862564086914, 'centroids according to\n', 0, 0)
(271.64801025390625, 95.52704620361328, 293.1232604980469, 110.4622573852539, 'cj =\n', 1, 0)
(297.34698486328125, 79.63713073730469, 339.15692138671875, 120.34989929199219, 'P\nd∈Cj ˜d\n', 2, 0)
(309.1319885253906, 95.52710723876953, 504.0028381347656, 124.05372619628906, '|Cj|\n(6)\n', 3, 0)
(107.99993896484375, 121.24408721923828, 504.0437927246094, 378.4283752441406, 'where cj is the centroid of the j-th cluster, Cj is the set of documents that belong\nto that cluster, its cardinality being |Cj|, and ˜d is a “truncated version” of d that\nvanishes on those words whose TF-IDF scores are below a threshold. Centroids\ncan thus be regarded as pseudo-documents that include those words whose TF-\nIDF scores are above a threshold in the documents that constitute the cluster. Each\nevent cluster is a collection of (typically 2 to 10) news articles from multiple sources,\nchronologically ordered, describing an event as it develops over time.\nThe second stage uses the centroids to identify sentences in each cluster that\nare central to the topic of the entire cluster. In (Radev et al., 2000), two metrics\nare deﬁned that resemble the two summands in the MMR (see Section 3.2): cluster-\nbased relative utility (CBRU) and cross-sentence informational subsumption (CSIS).\nThe ﬁrst accounts for how relevant a particular sentence is to the general topic of\nthe entire cluster; the second is a measure of redundancy among sentences. Unlike\nMMR, these metrics are not query-dependent. Given one cluster C of documents\nsegmented into n sentences, and a compression rate R, a sequence of nR sentences\nare extracted in the same order as they appear in the original documents, which in\nturn are ordered chronologically. The selection of the sentences is made by approx-\nimating their CBRU and CSIS.19 For each sentence si, three diﬀerent features are\nused:\n', 4, 0)
(124.36393737792969, 387.64520263671875, 504.0312805175781, 414.4924011230469, '• Its centroid value (Ci), deﬁned as the sum of the centroid values of all the\nwords in the sentence,\n', 5, 0)
(124.36392211914062, 423.710205078125, 504.0307922363281, 465.743408203125, '• A positional value (Pi), that is used to make leading sentences more important.\nLet Cmax be the centroid value of the highest ranked sentence in the document.\nThen Pi = n−i+1\n', 6, 0)
(200.0290069580078, 450.8080749511719, 243.59573364257812, 467.1335144042969, 'n\nCmax.\n', 7, 0)
(124.36400604248047, 473.3240661621094, 504.0366516113281, 500.1712646484375, '• The ﬁrst-sentence overlap (Fi), deﬁned as the inner product between the word\noccurrence vector of sentence i and that of the ﬁrst sentence of the document.\n', 8, 0)
(108.00003051757812, 509.3890380859375, 503.98944091796875, 537.1375732421875, 'The ﬁnal score of each sentence is a combination of the three scores above minus a\nredundancy penalty (Rs) for each sentence that overlaps highly ranked sentences.\n', 9, 0)
(108.00003814697266, 553.755615234375, 403.99884033203125, 565.7227172851562, '3.5\nMultilingual Multi-document Summarization\n', 10, 0)
(108.00003814697266, 572.7450561523438, 504.0766906738281, 653.789306640625, 'Evans (2005) addresses the task of summarizing documents written in multiple\nlanguages; this had already been sketched by Hovy and Lin (1999). Multilingual\nsummarization is still at an early stage, but this framework looks quite useful for\nnewswire applications that need to combine information from foreign news agen-\ncies. Evans (2005) considered the scenario where there is a preferred language in\nwhich the summary is to be written, and multiple documents in the preferred and\n', 11, 0)
(116.78399658203125, 662.5247802734375, 492.98150634765625, 673.05859375, '19The two metrics are used directly for evaluation (see (Radev et al., 2004) for more details).\n', 12, 0)
(300.54498291015625, 692.2910766601562, 311.4541015625, 705.5892944335938, '17\n', 13, 0)

page suivante
(108.0, 72.38806915283203, 504.0657653808594, 288.9243469238281, 'in foreign languages are available. In their experiments, the preferred language was\nEnglish and the documents are news articles in English and Arabic. The rationale is\nto summarize the English articles without discarding the information contained in\nthe Arabic documents. The IBM’s statistical machine translation system is ﬁrst ap-\nplied to translate the Arabic documents to English. Then a search is made, for each\ntranslated text unit, to see whether there is a similar sentence or not in the English\ndocuments. If so, and if the sentence is found relevant enough to be included in the\nsummary, the similar English sentence is included instead of the Arabic-to-English\ntranslation. This way, the ﬁnal summary is more likely to be grammatical, since\nmachine translation is known to be far from perfect. On the other hand, the result\nis also expected to have higher coverage than using just the English documents,\nsince the information contained in the Arabic documents can help to decide about\nthe relevance of each sentence. In order to measure similarity between sentences, a\ntool named SimFinder20 was employed: this is a tool for clustering text based on\nsimilarity over a variety of lexical and syntactic features using a log-linear regression\nmodel.\n', 0, 0)
(108.00000762939453, 309.8094482421875, 392.7433166503906, 324.16998291015625, '4\nOther Approaches to Summarization\n', 1, 0)
(108.00000762939453, 334.35015869140625, 504.0331726074219, 388.2963562011719, 'This section describes brieﬂy some unconventional approaches that, rather than\naiming to build full summarization systems, investigate some details that underlie\nthe summarization process, and that we conjecture to have a role to play in future\nresearch on this ﬁeld.\n', 2, 0)
(108.00000762939453, 405.814697265625, 240.14080810546875, 417.7818603515625, '4.1\nShort Summaries\n', 3, 0)
(108.00000762939453, 424.80517578125, 504.0766906738281, 627.7924194335938, 'Witbrock and Mittal (1999) claim that extractive summarization is not very pow-\nerful in that the extracts are not concise enough when very short summaries are\nrequired. They present a system that generated headline style summaries. The cor-\npus used in this work was newswire articles from Reuters and the Associated Press,\npublicly available at the LDC21. The system learned statistical models of the rela-\ntionship between source text units and headline units. It attempted to model both\nthe order and the likelihood of the appearance of tokens in the target documents.\nBoth the models, one for content selection and the other for surface realization were\nused to co-constrain each other during the search in the summary generation task.\nFor content selection, the model learned a translation model between a docu-\nment and its summary (Brown et al., 1993). This model in the simplest case can be\nthought as a mapping between a word in the document and the likelihood of some\nword appearing in the summary. To simplify the model, the authors assumed that\nthe probability of a word appearing in a summary is independent of its structure.\nThis mapping boils down to the fact that the probability of a particular summary\n', 4, 0)
(116.78399658203125, 636.5277709960938, 383.60443115234375, 658.0230102539062, '20See http://www1.cs.columbia.edu/nlp/tools.cgi#SimFinder.\n21See http://ldc.upenn.edu.\n', 5, 0)
(300.54498291015625, 692.2910766601562, 311.4541015625, 705.5892944335938, '18\n', 6, 0)

page suivante
(108.0, 72.38806915283203, 504.06573486328125, 275.3753356933594, 'candidate is the product of the probabilities of the summary content and that con-\ntent being expressed using a particular structure.\nThe surface realization model used was a bigram model. Viterbi beam search\nwas used to eﬃciently ﬁnd a near-optimal summary. The Markov assumption was\nviolated by using backtracking at every state to strongly discourage paths that\nrepeated terms, since bigrams that start repeating often seem to pathologically\noverwhelm the search otherwise.\nTo evaluate the system, the authors compared\nits output against the actual headlines for a set of input newswire stories. Since\nphrasing could not be compared, they compared the generated headlines against\nthe actual headlines, as well as the top ranked summary sentence of the story. Since\nthe system did not have a mechanism to determine the optimal length of a headline,\nsix headlines for each story were generated, ranging in length from 4 to 10 words\nand they measured the term-overlap between each of the generated headlines and\nthe test. For headline length 4, there was 0.89 overlap in the headline and there was\n0.91 overlap amongst the top scored sentence, indicating useful results.\n', 0, 0)
(108.0, 292.8936767578125, 270.12445068359375, 304.86083984375, '4.2\nSentence Compression\n', 1, 0)
(107.99990844726562, 311.8841552734375, 504.0621643066406, 514.8713989257812, 'Knight and Marcu (2000) introduced a statistical approach to sentence compression.\nThe authors believe that understanding the simpler task of compressing a sentence\nmay be a fruitful ﬁrst step to later tackle the problems of single and multi-document\nsummarization.\nSentence compression is deﬁned as follows: given a sequence of words W =\nw1w2 . . . wn that constitute a sentence, ﬁnd a subsequence wi1wi2 . . . wik, with\n1 ≤ i1 < i2 < . . . ik ≤ n, that is a compressed version of W.\nNote that there\nare 2n possibilities of output. Knight and Marcu (2000) considered two diﬀerent\napproaches: one that is inspired by the noisy-channel model, and another one based\non decision trees. Due to its simplicity and elegance, we describe the ﬁrst approach\nhere.\nThe noisy-channel model considers that one starts with a short summary s,\ndrawn according to the source model P(s), which is then subject to channel noise to\nbecome the full sentence t, in a process guided by the channel model P(t|s). When\nthe string t is observed, one wants to recover the original summary according to:\n', 2, 0)
(207.9639892578125, 526.0811767578125, 504.0029602050781, 547.1248168945312, 'ˆs = arg max\ns\nP(s|t) = arg max\ns\nP(s)P(t|s).\n(7)\n', 3, 0)
(108.00003051757812, 553.2481689453125, 504.0876770019531, 661.3912963867188, 'This model has the advantage of decoupling the goals of producing a short text that\nlooks grammatical (incorporated in the source model) and of preserving important\ninformation (which is done through the channel model). In (Knight and Marcu,\n2000), the source and channel models are simple models inspired by probabilistic\ncontext-free grammars (PCFGs). The following probability mass functions are de-\nﬁned over parse trees rather than strings: Ptree(s), the probability of a parse tree\nthat generates s, and Pexpand tree(t|s), the probability that a small parse tree that\ngenerates s is expanded to a longer one that generates t.\n', 4, 0)
(300.5450134277344, 692.2910766601562, 311.4541320800781, 705.5892944335938, '19\n', 5, 0)

page suivante
(107.99998474121094, 72.38806915283203, 504.0657653808594, 343.1213684082031, 'The sentence t is ﬁrst parsed by using Collins’ parser (Collins, 1999). Then,\nrather than computing Ptree(s) over all the 2n hypotheses for s, which would be\nexponential in the sentence length, a shaded-forest structure is used: the parse\ntree of t is traversed and the grammar (learned from the Penn Treebank22) is used\nto check recursively which nodes may be removed from each production in order\nto achieve another valid production. This algorithm allows to compute eﬃciently\nPtree(s) and Pexpand tree(t|s) for all possible grammatical summaries s. Conceptually,\nthe noisy channel model works the other way around: summaries are the original\nstrings that are expanded via expansion templates. Expansion operations have the\neﬀect of decreasing the probability Pexpand tree(t|s). The probabilities Ptree(s) and\nPexpand tree(t|s) consist in the usual factorized expression for PCFGs times a bigram\ndistribution over the leaves of the tree (i.e. the words). In the end, the log probability\nis (heuristically) divided by the length of the sentence s in order not to penalize\nexcessively longer sentences (this is done commonly in speech recognition).\nMore recently, Daum´e III and Marcu (2002) extended this approach to document\ncompression by using rhetorical structure theory as in Marcu (1998a), where the\nentire document is represented as a tree, hence allowing not only to compress relevant\nsentences, but also to drop irrelevant ones. In this framework, Daum´e III and Marcu\n(2004) employed kernel methods to decide for each node in the tree whether or not\nit should be kept.\n', 0, 0)
(108.00003051757812, 360.63970947265625, 350.236328125, 372.60687255859375, '4.3\nSequential document representation\n', 1, 0)
(108.00003051757812, 379.63018798828125, 504.0549621582031, 541.969482421875, 'We conclude this section by mentioning some recent work that concerns document\nrepresentation, with applications in summarization. In the bag-of-words representa-\ntion (Salton et al., 1975) each document is represented as a sparse vector in a very\nlarge Euclidean space, indexed by words in the vocabulary V . A well-known tech-\nnique in information retrieval to capture word correlation is latent semantic indexing\n(LSI), that aims to ﬁnd a linear subspace of dimension k ≤ |V | where documents\nmay be approximately represented by their projections.\nThese classical approaches assume by convenience that Euclidean geometry is\na proper model for text documents. As an alternative, Gous (1999) and Hall and\nHofmann (2000) used the framework of information geometry (Amari and Nagaoka,\n2001) to generalize LSI to the multinomial manifold, which can be identiﬁed with\nthe probability simplex\n', 2, 0)
(174.5860595703125, 557.334228515625, 209.22732543945312, 576.9920043945312, 'Pn−1 =\n', 3, 0)
(212.2550506591797, 545.3333740234375, 221.04779052734375, 586.046142578125, '(\n', 4, 0)
(221.0430450439453, 559.4506225585938, 260.57574462890625, 582.8828735351562, 'x ∈ Rn |\n', 5, 0)
(263.60302734375, 552.7955932617188, 279.35577392578125, 594.3370971679688, 'n\nX\n', 6, 0)
(264.6300354003906, 561.8392333984375, 423.7833251953125, 587.48974609375, 'i=1\nxi = 1, xi ≥ 0 for i = 1, . . . , n\n', 7, 0)
(423.77801513671875, 545.3333740234375, 432.57073974609375, 586.046142578125, ')\n', 8, 0)
(434.3840026855469, 561.8392333984375, 504.00286865234375, 575.137451171875, '.\n(8)\n', 9, 0)
(108.0, 595.5482177734375, 504.0491638183594, 651.3480224609375, 'Instead of ﬁnding a linear subspace, as in the Euclidean case, they learn a subman-\nifold of Pn−1. To illustrate this idea, Gous (1999) split a book (Machiavelli’s The\nPrince) into several text blocks (its numbered pages), considered each page as a\npoint in P|V |−1, and projected data into a 2-dimensional submanifold. The result is\n', 10, 0)
(116.78399658203125, 658.2288208007812, 308.2884216308594, 670.5030517578125, '22See http://www.cis.upenn.edu/~treebank/.\n', 11, 0)
(300.5450134277344, 692.2910766601562, 311.4541320800781, 705.5892944335938, '20\n', 12, 0)

page suivante
(108.0, 72.38806915283203, 504.02398681640625, 99.23526763916016, 'the representation of the book as a sequential path in R2, tracking the evolution of\nthe subject matter of the book over the course of its pages (see Fig. 5). Inspired by\n', 0, 0)
(108.0, 322.0940856933594, 504.0440979003906, 376.039306640625, 'Figure 5: The 113 pages of The Prince projected onto a 2-dimensional space (ex-\ntracted from (Gous, 1999)). The inﬂection around page 85 reﬂects a real change in\nthe subject matter, where the book shifts from political theory to a more biograph-\nical discourse.\n', 1, 0)
(108.0, 384.6611328125, 504.0307312011719, 481.3700256347656, 'this framework, Lebanon et al. (2007) suggested representing a document as a sim-\nplicial curve (i.e. a curve in the probability simplex), yielding the locally weighted\nbag-of-words (lowbow) model. According to this representation, a length-normalized\ndocument is a function x : [0, 1] × V → R+ such that\nX\n', 2, 0)
(219.33999633789062, 448.8721618652344, 504.0103759765625, 480.31854248046875, 'wj∈V\nx(t, wj) = 1,\nfor any t ∈ [0, 1].\n(9)\n', 3, 0)
(107.99990844726562, 483.16815185546875, 504.0118713378906, 550.6633911132812, 'We can regard the document as a continuous signal, and x(t, wj) as expressing\nthe relevance of word wj at instant t. This generalizes both the pure sequential\nrepresentation and the (global) bag-of-words model. Let y = (y1, . . . , yn) ∈ V n be\na n-length document. The pure sequential representation of y arises by deﬁning\nx = xseq with:\n', 4, 0)
(225.27890014648438, 539.4031982421875, 504.00238037109375, 584.2228393554688, 'xseq(t, wj) =\n\x1a 1,\nif wj = y⌈tn⌉\n0,\nif wj ̸= y⌈tn⌉,\n(10)\n', 5, 0)
(107.99996948242188, 580.1221923828125, 504.0161437988281, 606.9694213867188, 'where ⌈a⌉ denotes the smallest integer greater than a.\nThe global bag-of-words\nrepresentation of x corresponds to deﬁning x = xbow, where\n', 6, 0)
(167.7570037841797, 609.3042602539062, 251.33010864257812, 650.0170288085938, 'xbow(µ, wj) =\nZ 1\n', 7, 0)
(242.24998474121094, 622.004150390625, 504.00238037109375, 644.4596557617188, '0\nxseq(t, wj)dt,\nµ ∈ [0, 1], j = 1, . . . , |V |.\n(11)\n', 8, 0)
(107.99996948242188, 648.8541870117188, 504.02203369140625, 675.701416015625, 'In this case, the curve degenerates into a single point in the simplex, which is\nthe maximum likelihood estimate of the multinomial parameters. An intermediate\n', 9, 0)
(300.54498291015625, 692.2911987304688, 311.4541015625, 705.5894165039062, '21\n', 10, 0)

page suivante
(107.99996948242188, 72.38806915283203, 504.00445556640625, 147.6287384033203, 'representation arises by smoothing (10) via a function fµ,σ : [0, 1] → R++, where\nµ ∈ [0, 1] and σ ∈ R++ are respectively a location and a scale parameter.\nAn\nexample of such a smoothing function is the truncated Gaussian deﬁned in [0, 1]\nand normalized. This allows deﬁning the lowbow representation at µ of the n-lenght\ndocument (y1, . . . , yn) ∈ V n as the function x : [0, 1] × V → R+ such that:\n', 0, 0)
(225.20396423339844, 145.39817810058594, 293.22406005859375, 186.11094665527344, 'x(µ, wj) =\nZ 1\n', 1, 0)
(284.1429443359375, 158.0970458984375, 504.00238037109375, 180.55252075195312, '0\nxseq(t, wj)fµ,σ(t)dt.\n(12)\n', 2, 0)
(107.99996948242188, 188.12603759765625, 504.0548400878906, 242.07225036621094, 'The scale of the smoothing function controls the amount of locality/globality in\nthe document representation (see Fig. 6): when σ → ∞ we recover the global bow\nrepresentation (11); when σ → 0, we approach the pure sequential representation\n(10).\n', 3, 0)
(108.0, 473.84307861328125, 504.0024108886719, 500.6902770996094, 'Figure 6: The lowbow representation of a document with |V | = 3, for several values\nof the scale parameter σ (extracted from (Lebanon, 2006)).\n', 4, 0)
(107.99995422363281, 512.8961181640625, 504.04388427734375, 621.039306640625, 'Representing a document as a simplicial curve allows us to characterize geomet-\nrically several properties of the document. For example, the tangent vector ﬁeld\nalong the curve describes sequential “topic trends” and their change; the curvature\nmeasures the amount of wigglyness or deviation from a geodesic path. This prop-\nerties can be useful for tasks like text segmentation or summarization; for example\nplotting the velocity of the curve || ˙x(µ)|| along time oﬀers a visualization of the doc-\nument where local maxima tend to correspond to topic boundaries (see (Lebanon\net al., 2007) for more information).\n', 5, 0)
(300.5449523925781, 692.2910766601562, 311.4540710449219, 705.5892944335938, '22\n', 6, 0)

page suivante
(108.0, 72.19933319091797, 207.40476989746094, 86.55988311767578, '5\nEvaluation\n', 0, 0)
(108.0, 96.74005889892578, 504.0548095703125, 299.7273254394531, 'Evaluating a summary is a diﬃcult task because there does not exist an ideal sum-\nmary for a given document or set of documents. From papers surveyed in the previ-\nous sections and elsewhere in literature, it has been found that agreement between\nhuman summarizers is quite low, both for evaluating and generating summaries.\nMore than the form of the summary, it is diﬃcult to evaluate the summary con-\ntent. Another important problem in summary evaluation is the widespread use of\ndisparate metrics. The absence of a standard human or automatic evaluation met-\nric makes it very hard to compare diﬀerent systems and establish a baseline. This\nproblem is not present in other NLP problems, like parsing. Besides this, manual\nevaluation is too expensive: as stated by Lin (2004), large scale manual evaluation\nof summaries as in the DUC conferences would require over 3000 hours of human ef-\nforts. Hence, an evaluation metric having high correlation with human scores would\nobviate the process of manual evaluation. In this section, we would look at some im-\nportant recent papers that have been able to create standards in the summarization\ncommunity.\n', 1, 0)
(108.0, 317.2466735839844, 341.76007080078125, 329.2138366699219, '5.1\nHuman and Automatic Evaluation\n', 2, 0)
(107.99996948242188, 336.23614501953125, 504.0658874511719, 525.6744384765625, 'Lin and Hovy (2002) describe and compare various human and automatic metrics to\nevaluate summaries. They focus on the evaluation procedure used in the Document\nUnderstanding Conference 2001 (DUC-2001), where the Summary Evaluation En-\nvironment (SEE) interface was used to support the human evaluation part. NIST\nassessors in DUC-2001 compared manually written ideal summaries with summaries\ngenerated automatically by summarization systems and baseline summaries. Each\ntext was decomposed into a list of units (sentences) and displayed in separate win-\ndows in SEE. To measure the content of summaries, assessors stepped through each\nmodel unit (MU) from the ideal summaries and marked all system units (SU) shar-\ning content with the current model unit, rating them with scores in the range 1 − 4\nto specify that the marked system units express all (4), most (3), some (2) or hardly\nany (1) of the content of the current model unit. Grammaticality, cohesion, and co-\nherence were also rated similarly by the assessors. The weighted recall at threshold\nt (where t range from 1 to 4) is then deﬁned as\n', 3, 0)
(185.28396606445312, 534.6342163085938, 418.6861572265625, 556.9484252929688, 'Recallt = Number of MUs marked at or above t\n', 4, 0)
(233.822998046875, 542.0140380859375, 504.00244140625, 562.7952880859375, 'Number of MUs in the model summary.\n(13)\n', 5, 0)
(107.99999237060547, 571.1490478515625, 504.0548095703125, 665.7422485351562, 'An interesting study is presented that shows how unstable the human markings\nfor overlapping units are. For multiple systems, the coverage scores assigned to the\nsame units were diﬀerent by human assessors 18% of the time for the single document\ntask and 7.6% of the time for multi-document task. The authors also observe that\ninter-human agreement is quite low in creating extracts from documents (∼ 40% for\nsingle-documents and ∼ 29% for multi-documents). To overcome the instability of\nhuman evaluations, they proposed using automatic metrics for summary evaluation.\n', 6, 0)
(300.5450439453125, 692.291015625, 311.45416259765625, 705.5892333984375, '23\n', 7, 0)

page suivante
(108.0, 72.38806915283203, 504.06585693359375, 99.23526763916016, 'Inspired by the machine translation evaluation metric BLEU (Papineni et al., 2001),\nthey outline an accumulative n-gram matching score (which they call NAMS),\n', 0, 0)
(162.67701721191406, 107.40106964111328, 504.00250244140625, 130.08070373535156, 'NAMS = a1 · NAM1 + a2 · NAM2 + a3 · NAM3 + a4 · NAM4,\n(14)\n', 1, 0)
(108.00009155273438, 128.86505126953125, 336.4781188964844, 143.7992401123047, 'where the NAMn n-gram hit ratio is deﬁned as:\n', 2, 0)
(204.65008544921875, 147.4730224609375, 407.3582458496094, 160.77122497558594, '# of matched n-grams between MU and S\n', 3, 0)
(243.74099731445312, 154.85308837890625, 504.00244140625, 175.63426208496094, 'total # of n-grams in MU\n(15)\n', 4, 0)
(108.00003051757812, 180.944091796875, 504.0657958984375, 248.43931579589844, 'with S denoting here the whole system summary, and where only content words\nwere used in forming the n-grams. Diﬀerent conﬁgurations of ai were tried; the\nbest correlation with human judgement (using Spearman’s rank order correlation\ncoeﬃcient) was achieved using a conﬁguration giving 2/3 weight to bigram matches\nand 1/3 to unigrams matches with stemming done by the Porter stemmer.\n', 5, 0)
(108.00004577636719, 265.4796447753906, 188.14773559570312, 277.4468078613281, '5.2\nROUGE\n', 6, 0)
(108.00004577636719, 284.4701232910156, 504.03289794921875, 392.61236572265625, 'Lin (2004) introduced a set of metrics called Recall-Oriented Understudy for Gist-\ning Evaluation (ROUGE)23 that have become standards of automatic evaluation of\nsummaries.\nIn what follows, let R = {r1, . . . , rm} be a set of reference summaries, and let s be\na summary generated automatically by some system. Let Φn(d) be a binary vector\nrepresenting the n-grams contained in a document d; the i-th component φi\nn(d) is 1\nif the i-th n-gram is contained in d and 0 otherwise. The metric ROUGE-N is an\nn-gram recall based statistic that can be computed as follows:\n', 7, 0)
(215.15609741210938, 392.49627685546875, 504.00238037109375, 448.6749267578125, 'ROUGE-N(s) =\nP\nPr∈R⟨Φn(r), Φn(s)⟩\nr∈R⟨Φn(r), Φn(r)⟩,\n(16)\n', 8, 0)
(107.99996185302734, 434.81610107421875, 504.04388427734375, 502.3113098144531, 'where ⟨., .⟩ denotes the usual inner product of vectors. This measure is closely related\nto BLEU which is a precision related measure. Unlike other measures previously\nconsidered, ROUGE-N can be used for multiple reference summaries, which is quite\nuseful in practical situations. An alternative is taking the most similar summary in\nthe reference set,\n', 9, 0)
(207.43896484375, 508.22711181640625, 504.0024108886719, 544.1337280273438, 'ROUGE-Nmulti(s) = max\nr∈R\n⟨Φn(r), Φn(s)⟩\n⟨Φn(r), Φn(r)⟩.\n(17)\n', 10, 0)
(107.99996948242188, 542.9100952148438, 504.01226806640625, 610.4052734375, 'Another metric in (Lin, 2004) applies the concept of longest common subse-\nquences24 (LCS). The rationale is: the longer the LCS between two summary sen-\ntences, the more similar they are. Let r1, . . . , ru be the reference sentences of the\ndocuments in R, and s a candidate summary (considered as a concatenation of\nsentences). The ROUGE-L is deﬁned as an LCS based F-measure:\n', 11, 0)
(221.68898010253906, 617.840087890625, 388.6155700683594, 638.518310546875, 'ROUGE-L(s) = (1 + β2)RLCSPLCS\n', 12, 0)
(308.8320007324219, 625.2200927734375, 504.0024108886719, 647.6903076171875, 'RLCS + β2PLCS\n(18)\n', 13, 0)
(116.78399658203125, 653.7227783203125, 503.9953308105469, 682.559814453125, '23See http://openrouge.com/default.aspx.\n24A subsequence of a string s = s1 . . . sn is a string of the form si1 . . . sin where 1 ≤ i1 < . . . in ≤ n.\n', 14, 0)
(300.544921875, 692.2910766601562, 311.45404052734375, 705.5892944335938, '24\n', 15, 0)

page suivante
(108.0, 65.9581069946289, 378.2986755371094, 105.56660461425781, 'where RLCS(s) =\nPu\ni=1 LCS(ri,s)\nPu\ni=1 |ri|\n, PLCS(s) =\nPu\ni=1 LCS(ri,s)\n', 0, 0)
(107.99990844726562, 73.39008331298828, 504.05474853515625, 224.3153533935547, '|s|\n, |x| denotes the length of\nsentence x, LCS(x, y) denotes the length of the LCS between sentences x and y,\nand β is a (usually large) parameter to balance precision and recall. Notice that\nthe LCS function may be computed by a simple dynamic programming approach.\nThe metric (18) is further reﬁned by including weights that penalize subsequence\nmatches that are not consecutive, yielding a new measure denoted ROUGE-W.\nYet another measure introduced by Lin (2004) is ROUGE-S, which can be seen\nas a gappy version of ROUGE-N for n = 2 and is aptly called skip bigram. Let Ψ2(d)\nbe a binary vector indexed by ordered pairs of words; the i-th component ψi\n2(d) is\n1 if the i-th pair is a subsequence of d and 0 otherwise. The metric ROUGE-S is\ncomputed as follows:\n', 1, 0)
(233.47296142578125, 223.23016357421875, 376.8313293457031, 243.9083709716797, 'ROUGE-S(s) = (1 + β2)RSPS\n', 2, 0)
(319.8580017089844, 230.61004638671875, 504.0024108886719, 253.0792694091797, 'RS + β2PS\n(19)\n', 3, 0)
(107.99996948242188, 251.70309448242188, 504.04400634765625, 368.8063049316406, 'where RS(s) =\nPu\ni=1⟨Ψ2(ri),Ψ2(s)⟩\nPu\ni=1⟨Ψ2(ri),Ψ2(ri)⟩ and PS(s) =\nPu\ni=1⟨Ψ2(ri),Ψ2(s)⟩\n⟨Ψ2(s),Ψ2(s)⟩\n.\nThe various versions of ROUGE were evaluated by computing the correlation\ncoeﬃcient between ROUGE scores and human judgement scores. ROUGE-2 per-\nformed the best among the ROUGE-N variants.\nROUGE-L, ROUGE-W, and\nROUGE-S all performed very well on the DUC-2001 and DUC-2002 datasets. How-\never, correlation achieved with human judgement for multi-document summarization\nwas not as high as single-document ones; improvement on this side of the paradigm\nis an open research topic.\n', 4, 0)
(107.99996948242188, 386.1646423339844, 416.599609375, 398.1318054199219, '5.3\nInformation-theoretic Evaluation of Summaries\n', 5, 0)
(107.99993896484375, 405.1551208496094, 504.0439147949219, 608.142333984375, 'A very recent approach (Lin et al., 2006) proposes to use an information-theoretic\nmethod to automatic evaluation of summaries. The central idea is to use a diver-\ngence measure between a pair of probability distributions, in this case the Jensen-\nShannon divergence, where the ﬁrst distribution is derived from an automatic sum-\nmary and the second from a set of reference summaries. This approach has the\nadvantage of suiting both the single-document and the multi-document summariza-\ntion scenarios.\nLet D = {d1, . . . , dn} be the set of documents to summarize (which is a singleton\nset in the case of single-document summarization).\nAssume that a distribution\nparameterized by θR generates reference summaries of the documents in D. The\ntask of summarization can be seen as that of estimating θR. Analogously, assume\nthat every summarization system is governed by some distribution parameterized\nby θA. Then, we may deﬁne a good summarizer as one for which θA is close to θR.\nOne information-theoretic measure between distributions that is adequate for this\nis the KL divergence (Cover and Thomas, 1991),\n', 6, 0)
(230.42893981933594, 626.6611328125, 305.8935852050781, 647.7047729492188, 'KL(pθA||pθR) =\n', 7, 0)
(308.9179382324219, 617.6184692382812, 324.6706848144531, 659.1600341796875, 'm\nX\n', 8, 0)
(309.9449462890625, 618.646484375, 376.3432922363281, 652.3126220703125, 'i=1\npθA\ni\nlog pθA\ni\n', 9, 0)
(361.3760070800781, 626.6610717773438, 504.00244140625, 651.91357421875, 'pθR\ni\n.\n(20)\n', 10, 0)
(108.00003051757812, 661.8994140625, 503.9953308105469, 678.2235717773438, 'However, the KL divergence is unbounded and goes to inﬁnity whenever pθA\ni\nvanishes\n', 11, 0)
(300.5450439453125, 692.2910766601562, 311.45416259765625, 705.5892944335938, '25\n', 12, 0)

page suivante
(107.99999237060547, 71.8844223022461, 504.0335388183594, 126.33329010009766, 'and pθR\ni\ndoes not, which requires using some kind of smoothing when estimating the\ndistributions. Lin et al. (2006) claims that the measure used here should also be\nsymmetric,25 another thing that the KL divergence is not. Hence, they propose to\nuse the Jensen-Shannon divergence which is bounded and symmetric:26\n', 0, 0)
(186.35995483398438, 134.1419677734375, 356.6895751953125, 162.56568908691406, 'JS(pθA||pθR)\n=\n1\n2KL(pθA||r) + 1\n', 1, 0)
(351.2349853515625, 141.52203369140625, 422.61566162109375, 162.56568908691406, '2KL(pθR||r) =\n', 2, 0)
(257.2140197753906, 160.02105712890625, 319.3085632324219, 188.44471740722656, '=\nH(r) − 1\n', 3, 0)
(313.85400390625, 160.0211181640625, 374.8865661621094, 188.4447784423828, '2H(pθA) − 1\n', 4, 0)
(369.4320068359375, 167.401123046875, 504.00244140625, 188.1822967529297, '2H(pθA),\n(21)\n', 5, 0)
(108.00003051757812, 196.3331298828125, 164.74314880371094, 209.63133239746094, 'where r = 1\n', 6, 0)
(160.51100158691406, 193.71307373046875, 200.66111755371094, 212.65853881835938, '2pθA + 1\n', 7, 0)
(107.99996948242188, 193.71307373046875, 504.0135192871094, 273.2839050292969, '2pθR is the average distribution.\nTo evaluate a summary SA given a reference summary SR, the authors propose\nto use the negative JS divergence between the estimates of pθA and pθR given the\nsummaries,\nscore(SA|SR) = −JS(p\nˆθA||p\nˆθR)\n(22)\n', 8, 0)
(107.99996948242188, 272.2652282714844, 504.0657958984375, 348.4524230957031, 'The parameters are estimated via a posteriori maximization assuming a multi-\nnomial generation model for each summary (which means that they are modeled as\nbags-of-words) and using Dirichlet priors (the conjugate priors of the multinomial\nfamily). So:\nˆθA = arg max\nθA p(SA|θA)p(θA),\n(23)\n', 9, 0)
(108.00003051757812, 351.8992614746094, 504.017333984375, 408.31011962890625, 'where (m being the number of distinct words, a1, . . . , am being the word counts in\nthe summary, a0 = Pm\ni=1 ai)\n', 10, 0)
(220.03700256347656, 391.4072570800781, 347.8372802734375, 440.950927734375, 'p(SA|θA) =\nΓ(a0 + 1)\nQm\ni=1 Γ(ai + 1)\n', 11, 0)
(350.8499755859375, 389.74444580078125, 364.79180908203125, 431.28594970703125, 'm\nY\n', 12, 0)
(350.9679870605469, 398.7880859375, 504.0024108886719, 424.43853759765625, 'i=1\nθA,iai\n(24)\n', 13, 0)
(108.0, 432.4970703125, 125.5854721069336, 445.7952575683594, 'and\n', 14, 0)
(231.2239990234375, 444.2720642089844, 324.8886413574219, 493.8149108886719, 'p(θA) =\nΓ(α0)\nQm\ni=1 Γ(αi)\n', 15, 0)
(327.9010009765625, 442.6084289550781, 341.84283447265625, 484.1499328613281, 'm\nY\n', 16, 0)
(328.0190124511719, 451.50927734375, 504.00244140625, 477.3025207519531, 'i=1\nθA,iαi−1\n(25)\n', 17, 0)
(108.00003051757812, 475.7271728515625, 479.80633544921875, 524.6219482421875, 'where αi are hyper-parameters and α0 = Pm\ni=1 αi. After some algebra, we get\n', 18, 0)
(259.70599365234375, 503.9710693359375, 347.6855163574219, 526.6516723632812, 'ˆθA,i = ai + αi − 1\n', 19, 0)
(292.6440124511719, 511.35107421875, 504.00244140625, 541.5147094726562, 'a0 + α0 − m\n(26)\n', 20, 0)
(108.00001525878906, 538.9220581054688, 504.0330505371094, 596.8262939453125, 'which is similar to MLE with smoothing.27 ˆθR is estimated analogously using the\nreference summary SR. Not surprisingly, if we have more than one reference sum-\nmary, the MAP estimation given all summaries equals MAP estimation given their\nconcatenation into a single summary.\n', 21, 0)
(108.0, 605.561767578125, 504.0550231933594, 671.8865966796875, '25However, the authors do not give much support for this claim. In our view, there is no reason\nto require symmetry.\n26Although this is not mentioned in (Lin et al., 2006), the Jensen-Shannon divergence also satisﬁes\nthe axioms to be a squared metric, as shown by Endres and Schindelin (2003). It has also a plethora\nof properties that are presented elsewhere, but this is out of scope of this survey.\n27In particular if αi = 1 it is just maximum likelihood estimation (MLE).\n', 22, 0)
(300.5459899902344, 692.2910766601562, 311.4551086425781, 705.5892944335938, '26\n', 23, 0)

page suivante
(107.99998474121094, 72.38806915283203, 504.06573486328125, 153.4322967529297, 'The authors experimented three automatic evaluation schemes (JS with smooth-\ning, JS without smoothing, and KL divergence) against manual evaluation; the best\nperformance was achieved by JS without smoothing. This is not surprising since, as\nseen above, the JS divergence is bounded, unlike the KL divergence, and so it does\nnot require smoothing. Smoothing has the eﬀect of pulling the two distributions\nmore close to the uniform distribution.\n', 0, 0)
(107.99998474121094, 174.3173828125, 209.45628356933594, 188.67791748046875, '6\nConclusion\n', 1, 0)
(107.99998474121094, 198.85809326171875, 504.065673828125, 523.7884521484375, 'The rate of information growth due to the World Wide Web has called for a need\nto develop eﬃcient and accurate summarization systems.\nAlthough research on\nsummarization started about 50 years ago, there is still a long trail to walk in\nthis ﬁeld. Over time, attention has drifted from summarizing scientiﬁc articles to\nnews articles, electronic mail messages, advertisements, and blogs. Both abstractive\nand extractive approaches have been attempted, depending on the application at\nhand. Usually, abstractive summarization requires heavy machinery for language\ngeneration and is diﬃcult to replicate or extend to broader domains. In contrast,\nsimple extraction of sentences have produced satisfactory results in large-scale ap-\nplications, specially in multi-document summarization. The recent popularity of\neﬀective newswire summarization systems conﬁrms this claim.\nThis survey emphasizes extractive approaches to summarization using statisti-\ncal methods.\nA distinction has been made between single document and multi-\ndocument summarization. Since a lot of interesting work is being done far from\nthe mainstream research in this ﬁeld, we have chosen to include a brief discussion\non some methods that we found relevant to future research, even if they focus only\non small details related to a general summarization process and not on building an\nentire summarization system.\nFinally, some recent trends in automatic evaluation of summarization systems\nhave been surveyed.\nThe low inter-annotator agreement ﬁgures observed during\nmanual evaluations suggest that the future of this research area heavily depends on\nthe ability to ﬁnd eﬃcient ways of automatically evaluating these systems and on\nthe development of measures that are objective enough to be commonly accepted\nby the research community.\n', 2, 0)
(107.99998474121094, 544.6735229492188, 241.93609619140625, 559.0340576171875, 'Acknowledgements\n', 3, 0)
(107.99998474121094, 569.2142333984375, 504.0876770019531, 609.6104736328125, 'We would like to thank Noah Smith and Einat Minkov for valuable suggestions\nduring the course of the survey. We would also like to thank Alex Rudnicky and\nMohit Kumar for insightful discussions at various points during 2006-2007.\n', 4, 0)
(300.54498291015625, 692.291259765625, 311.4541015625, 705.5894775390625, '27\n', 5, 0)

page suivante
(108.0, 72.19933319091797, 183.51840209960938, 86.55988311767578, 'References\n', 0, 0)
(108.0, 96.74005889892578, 504.03497314453125, 123.5872573852539, 'Amari, S.-I. and Nagaoka, H. (2001). Methods of Information Geometry (Transla-\ntions of Mathematical Monographs). Oxford University Press. [20]\n', 1, 0)
(108.00003051757812, 131.9150390625, 504.07666015625, 185.8602752685547, 'Aone, C., Okurowski, M. E., Gorlinsky, J., and Larsen, B. (1999).\nA trainable\nsummarizer with knowledge acquired from robust nlp techniques.\nIn Mani, I.\nand Maybury, M. T., editors, Advances in Automatic Text Summarization, pages\n71–80. MIT Press. [4, 5]\n', 2, 0)
(108.0, 194.18804931640625, 504.06585693359375, 221.03526306152344, 'Barzilay, R. and Elhadad, M. (1997). Using lexical chains for text summarization.\nIn Proceedings ISTS’97. [8]\n', 3, 0)
(108.0, 229.362060546875, 504.087646484375, 269.7592468261719, 'Barzilay, R., McKeown, K., and Elhadad, M. (1999).\nInformation fusion in the\ncontext of multi-document summarization. In Proceedings of ACL ’99. [12, 13,\n14, 16]\n', 4, 0)
(108.0, 278.0860595703125, 504.0438232421875, 304.9332580566406, 'Baxendale, P. (1958). Machine-made index for technical literature - an experiment.\nIBM Journal of Research Development, 2(4):354–361. [2, 3, 5]\n', 5, 0)
(108.0, 313.26007080078125, 504.0655822753906, 353.6572570800781, 'Brown, F., Pietra, V. J. D., Pietra, S. A. D., and Mercer, R. L. (1993).\nThe\nmathematics of statistical machine translation: parameter estimation. Comput.\nLinguist., 19(2):263–311. [18]\n', 6, 0)
(107.99999237060547, 361.98406982421875, 504.0546875, 415.9302673339844, 'Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., and\nHullender, G. (2005). Learning to rank using gradient descent. In ICML ’05:\nProceedings of the 22nd international conference on Machine learning, pages 89–\n96, New York, NY, USA. ACM. [8]\n', 7, 0)
(108.0, 424.257080078125, 504.06573486328125, 464.6532897949219, 'Carbonell, J. and Goldstein, J. (1998). The use of MMR, diversity-based reranking\nfor reordering documents and producing summaries. In Proceedings of SIGIR ’98,\npages 335–336, New York, NY, USA. [12, 14, 15]\n', 8, 0)
(108.0, 472.9811096191406, 504.0027160644531, 499.82830810546875, 'Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing.\nPhD thesis, University of Pennsylvania. [13, 20]\n', 9, 0)
(108.0, 508.1551513671875, 504.07684326171875, 535.0032958984375, 'Conroy, J. M. and O’leary, D. P. (2001). Text summarization via hidden markov\nmodels. In Proceedings of SIGIR ’01, pages 406–407, New York, NY, USA. [6]\n', 10, 0)
(108.0, 543.330078125, 487.4096374511719, 556.6282958984375, 'Cover, T. and Thomas, J. (1991). Elements of Information Theory. Wiley. [25]\n', 11, 0)
(108.0, 564.955078125, 504.03302001953125, 605.352294921875, 'Daum´e III, H. and Marcu, D. (2002). A noisy-channel model for document com-\npression. In Proceedings of the Conference of the Association of Computational\nLinguistics (ACL 2002). [20]\n', 12, 0)
(108.0, 613.6790771484375, 504.0221252441406, 640.5263061523438, 'Daum´e III, H. and Marcu, D. (2004). A tree-position kernel for document compres-\nsion. In Proceedings of DUC2004. [20]\n', 13, 0)
(108.00001525878906, 648.8541259765625, 504.0010681152344, 675.7012939453125, 'Edmundson, H. P. (1969). New methods in automatic extracting. Journal of the\nACM, 16(2):264–285. [2, 3, 4]\n', 14, 0)
(300.5450439453125, 692.2910766601562, 311.45416259765625, 705.5892944335938, '28\n', 15, 0)

page suivante
(108.0, 72.38806915283203, 504.0984802246094, 99.23526763916016, 'Endres, D. M. and Schindelin, J. E. (2003). A new metric for probability distribu-\ntions. IEEE Transactions on Information Theory, 49(7):1858–1860. [26]\n', 0, 0)
(108.00001525878906, 108.39307403564453, 504.065673828125, 135.2412567138672, 'Evans, D. K. (2005). Similarity-based multilingual multi-document summarization.\nTechnical Report CUCS-014-05, Columbia University. [12, 17]\n', 1, 0)
(108.00001525878906, 144.3990478515625, 364.6692199707031, 157.69725036621094, 'Gous, A. (1999). Spherical subfamily models. [20, 21]\n', 2, 0)
(108.00001525878906, 166.85504150390625, 504.07666015625, 220.80125427246094, 'Hall, K. and Hofmann, T. (2000).\nLearning curved multinomial subfamilies for\nnatural language processing and information retrieval. In Proc. 17th International\nConf. on Machine Learning, pages 351–358. Morgan Kaufmann, San Francisco,\nCA. [20]\n', 3, 0)
(108.00003051757812, 229.95904541015625, 504.0549011230469, 270.3562316894531, 'Hovy, E. and Lin, C. Y. (1999). Automated text summarization in summarist. In\nMani, I. and Maybury, M. T., editors, Advances in Automatic Text Summariza-\ntion, pages 81–94. MIT Press. [17]\n', 4, 0)
(108.00004577636719, 279.5140380859375, 504.0439758300781, 306.3612365722656, 'Knight, K. and Marcu, D. (2000). Statistics-based summarization - step one: Sen-\ntence compression. In AAAI/IAAI, pages 703–710. [19]\n', 5, 0)
(108.00003051757812, 315.5200500488281, 504.05487060546875, 342.36724853515625, 'Kupiec, J., Pedersen, J., and Chen, F. (1995). A trainable document summarizer.\nIn Proceedings SIGIR ’95, pages 68–73, New York, NY, USA. [4]\n', 6, 0)
(108.00003051757812, 351.5250549316406, 504.0548400878906, 378.37225341796875, 'Lebanon, G. (2006). Sequential document representations and simplicial curves. In\nProceedings of the 22nd Conference on Uncertainty in Artiﬁcial Intelligence. [22]\n', 7, 0)
(108.00003051757812, 387.53106689453125, 504.0766906738281, 427.9272766113281, 'Lebanon, G., Mao, Y., and Dillon, J. (2007). The locally weighted bag of words\nframework for document representation. J. Mach. Learn. Res., 8:2405–2441. [21,\n22]\n', 8, 0)
(108.0, 437.0850830078125, 504.0028991699219, 463.9332580566406, 'Lin, C.-Y. (1999). Training a selection function for extraction. In Proceedings of\nCIKM ’99, pages 55–62, New York, NY, USA. [5]\n', 9, 0)
(108.0, 473.091064453125, 504.0330810546875, 527.0372924804688, 'Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In\nMarie-Francine Moens, S. S., editor, Text Summarization Branches Out: Pro-\nceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain.\n[8, 23, 24,\n25]\n', 10, 0)
(108.00003051757812, 536.195068359375, 504.0657958984375, 576.59130859375, 'Lin, C.-Y., Cao, G., Gao, J., and Nie, J.-Y. (2006). An information-theoretic ap-\nproach to automatic evaluation of summaries. In Proceedings of HLT-NAACL\n’06, pages 463–470, Morristown, NJ, USA. [25, 26]\n', 11, 0)
(108.0000228881836, 585.7500610351562, 504.0038757324219, 626.146240234375, 'Lin, C.-Y. and Hovy, E. (1997). Identifying topics by position. In Proceedings of\nthe Fifth conference on Applied natural language processing, pages 283–290, San\nFrancisco, CA, USA. [5]\n', 12, 0)
(108.00003051757812, 635.3040771484375, 504.0657043457031, 675.7012939453125, 'Lin, C.-Y. and Hovy, E. (2002). Manual and automatic evaluation of summaries. In\nProceedings of the ACL-02 Workshop on Automatic Summarization, pages 45–51,\nMorristown, NJ, USA. [23]\n', 13, 0)
(300.5450439453125, 692.291015625, 311.45416259765625, 705.5892333984375, '29\n', 14, 0)

page suivante
(108.0, 72.38806915283203, 504.02020263671875, 99.23526763916016, 'Luhn, H. P. (1958). The automatic creation of literature abstracts. IBM Journal of\nResearch Development, 2(2):159–165. [2, 3, 6, 8]\n', 0, 0)
(107.99999237060547, 108.45307159423828, 504.0657958984375, 135.30027770996094, 'Mani, I. and Bloedorn, E. (1997). Multi-document summarization by graph search\nand matching. In AAAI/IAAI, pages 622–628. [15, 16]\n', 1, 0)
(107.99998474121094, 144.51708984375, 504.0875549316406, 184.91429138183594, 'Marcu, D. (1998a). Improving summarization through rhetorical parsing tuning. In\nProceedings of The Sixth Workshop on Very Large Corpora, pages 206-215, pages\n206–215, Montreal, Canada. [9, 10, 20]\n', 2, 0)
(107.99996948242188, 194.131103515625, 504.0274658203125, 234.52830505371094, 'Marcu, D. C. (1998b). The rhetorical parsing, summarization, and generation of\nnatural language texts. PhD thesis, University of Toronto. Adviser-Graeme Hirst.\n[10]\n', 3, 0)
(107.99996948242188, 243.7451171875, 504.0656433105469, 284.1423034667969, 'McKeown, K., Klavans, J., Hatzivassiloglou, V., Barzilay, R., and Eskin, E.\n(1999). Towards multidocument summarization by reformulation: Progress and\nprospects. In AAAI/IAAI, pages 453–460. [11, 12, 13, 14, 16]\n', 4, 0)
(107.99995422363281, 293.359130859375, 504.032958984375, 333.7563171386719, 'McKeown, K. R. and Radev, D. R. (1995). Generating summaries of multiple news\narticles. In Proceedings of SIGIR ’95, pages 74–82, Seattle, Washington. [8, 11,\n12]\n', 5, 0)
(107.99993896484375, 342.97314453125, 504.0026550292969, 369.8213195800781, 'Miller, G. A. (1995). Wordnet: a lexical database for english. Commun. ACM,\n38(11):39–41. [4, 9]\n', 6, 0)
(107.99993896484375, 379.03814697265625, 504.01104736328125, 419.4343566894531, 'Nenkova, A. (2005). Automatic text summarization of newswire: Lessons learned\nfrom the document understanding conference.\nIn Proceedings of AAAI 2005,\nPittsburgh, USA. [7]\n', 7, 0)
(107.99995422363281, 428.65216064453125, 504.0766296386719, 469.0483703613281, 'Ono, K., Sumita, K., and Miike, S. (1994). Abstract generation based on rhetorical\nstructure extraction. In Proceedings of Coling ’94, pages 344–348, Morristown,\nNJ, USA. [9]\n', 8, 0)
(107.99995422363281, 478.26617431640625, 504.0157775878906, 518.6624145507812, 'Osborne, M. (2002). Using maximum entropy for sentence extraction. In Proceedings\nof the ACL’02 Workshop on Automatic Summarization, pages 1–8, Morristown,\nNJ, USA. [7]\n', 9, 0)
(107.99993896484375, 527.8801879882812, 504.0875244140625, 568.2764282226562, 'Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2001). Bleu: a method for\nautomatic evaluation of machine translation. In Proceedings of ACL ’02, pages\n311–318, Morristown, NJ, USA. [24]\n', 10, 0)
(107.99996948242188, 577.4942016601562, 504.0548400878906, 604.3414306640625, 'Radev, D. R., Hovy, E., and McKeown, K. (2002). Introduction to the special issue\non summarization. Computational Linguistics., 28(4):399–408. [1, 2]\n', 11, 0)
(107.99998474121094, 613.5592041015625, 504.05487060546875, 667.50439453125, 'Radev, D. R., Jing, H., and Budzikowska, M. (2000). Centroid-based summarization\nof multiple documents: sentence extraction, utility-based evaluation, and user\nstudies. In NAACL-ANLP 2000 Workshop on Automatic summarization, pages\n21–30, Morristown, NJ, USA. [12, 16, 17]\n', 12, 0)
(300.54498291015625, 692.2911987304688, 311.4541015625, 705.5894165039062, '30\n', 13, 0)

page suivante
(108.0, 72.38806915283203, 504.0439758300781, 112.7842788696289, 'Radev, D. R., Jing, H., Stys, M., and Tam, D. (2004). Centroid-based summariza-\ntion of multiple documents. Information Processing and Management 40 (2004),\n40:919–938. [16, 17]\n', 0, 0)
(107.99996948242188, 122.00208282470703, 504.0439453125, 148.8492889404297, 'Radev, D. R. and McKeown, K. (1998). Generating natural language summaries\nfrom multiple on-line sources. Computational Linguistics, 24(3):469–500. [12]\n', 1, 0)
(107.99993896484375, 158.06707763671875, 504.0657653808594, 198.4633026123047, 'Salton, G. and Buckley, C. (1988). On the use of spreading activation methods in\nautomatic information. In Proceedings of SIGIR ’88, pages 147–160, New York,\nNY, USA. [15]\n', 2, 0)
(107.99995422363281, 207.68109130859375, 504.05487060546875, 234.52830505371094, 'Salton, G., Wong, A., and Yang, A. C. S. (1975). A vector space model for automatic\nindexing. Communications of the ACM, 18:229–237. [20]\n', 3, 0)
(107.99993896484375, 243.7451171875, 504.0548400878906, 270.5932922363281, 'Selman, B., Levesque, H. J., and Mitchell, D. G. (1992). A new method for solving\nhard satisﬁability problems. In AAAI, pages 440–446. [11]\n', 4, 0)
(107.99992370605469, 279.81011962890625, 504.0548095703125, 320.2063293457031, 'Svore, K., Vanderwende, L., and Burges, C. (2007). Enhancing single-document\nsummarization by combining RankNet and third-party sources. In Proceedings of\nthe EMNLP-CoNLL, pages 448–457. [7, 8]\n', 5, 0)
(107.99990844726562, 329.42413330078125, 504.06573486328125, 369.8203430175781, 'Witbrock, M. J. and Mittal, V. O. (1999). Ultra-summarization (poster abstract):\na statistical approach to generating highly condensed non-extractive summaries.\nIn Proceedings of SIGIR ’99, pages 315–316, New York, NY, USA. [18]\n', 6, 0)
(300.5448913574219, 692.2911376953125, 311.4540100097656, 705.58935546875, '31\n', 7, 0)

page suivante
