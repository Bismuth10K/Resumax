(102.76200103759766, 138.46859741210938, 461.0355224609375, 166.37574768066406, 'An Empirical Evaluation of Text Representation Schemes on\nMultilingual Social Web to Filter the Textual Aggression\n', 0, 0)
(102.76200103759766, 177.13720703125, 306.24212646484375, 192.0052947998047, 'Sandip Modha a and Prasenjit Majumdera\n', 1, 0)
(102.76200866699219, 198.36566162109375, 235.06207275390625, 209.70164489746094, 'aDA-IICT Gandhinagar,India;\n', 2, 0)
(114.71701049804688, 229.37823486328125, 214.3068084716797, 250.29965209960938, 'ARTICLE HISTORY\nCompiled April 19, 2019\n', 3, 0)
(114.71701049804688, 264.24725341796875, 449.54046630859375, 532.242919921875, 'ABSTRACT\nDue to an exponential rise in the social media user base, incidents like hate speech,\ntrolling, cyberbullying are also increasing and that lead hate speech detection prob-\nlem reshaped into diﬀerent tasks like Aggression detection, Fact detection. This\npaper attempt to study the eﬀectiveness of text representation schemes on two\ntasks namely: User Aggression and Fact Detection from the social media contents.\nIn User Aggression detection, The aim is to identify the level of aggression from\nthe contents generated in the Social media and written in the English, Devanagari\nHindi and Romanized Hindi. Aggression levels are categorized into three predeﬁned\nclasses namely: ‘Non-aggressive‘, ‘Overtly Aggressive‘, and ‘Covertly Aggressive‘.\nDuring the disaster-related incident, Social media like, Twitter is ﬂooded with mil-\nlions of posts. In such emergency situations, identiﬁcation of factual posts is impor-\ntant for organizations involved in the relief operation. We anticipated this problem\nas a combination of classiﬁcation and Ranking problem. This paper presents a com-\nparison of various text representation scheme based on BoW techniques, distributed\nword/sentence representation, transfer learning on classiﬁers. Weighted F1 score is\nused as a primary evaluation metric. Results show that text representation using\nBoW performs better than word embedding on machine learning classiﬁers. While\npre-trained Word embedding techniques perform better on classiﬁers based on deep\nneural net. Recent transfer learning model like ELMO, ULMFiT are ﬁne-tuned\nfor the Aggression classiﬁcation task. However, results are not at par with pre-\ntrained word embedding model. Overall, word embedding using fastText produce\nbest weighted F1-score than Word2Vec and Glove. Results are further improved\nusing pre-trained vector model. Statistical signiﬁcance tests are employed to en-\nsure the signiﬁcance of the classiﬁcation results. In the case of lexically diﬀerent\ntest Dataset, other than training Dataset, deep neural models are more robust and\nperform substantially better than machine learning classiﬁers.\n', 4, 0)
(114.7170181274414, 546.1904907226562, 449.4875183105469, 565.118896484375, 'Abbreviations:\nNLP:\nNatural\nLanguage\nProcessing;\nBoW\n:Bag-of-Word;\nCNN:Convolution Neural Network;LSTM :Long Short-Term Memory.\n', 5, 0)
(114.71702575683594, 579.0665283203125, 449.5045166015625, 607.9588623046875, 'KEYWORDS\nAggression, Trolling, Bag-of-Words, Word Embedding, Transfer learning,\nWord2Vec, Glove, fastText, Signiﬁcance Test, Wilcoxon signed-rank, Student t-test\n', 6, 0)
(111.2300033569336, 623.5994262695312, 380.3482971191406, 642.2295532226562, 'CONTACT Sandip Modha. Email: sjmodha@gmail.com\nCONTACT Prasenjit Majumder. Email: prasenjit.majumder@gmail.com\n', 7, 0)
(10.940000534057617, 217.82000732421875, 37.619998931884766, 560.0, 'arXiv:1904.08770v1  [cs.IR]  16 Apr 2019\n', 8, 0)

page suivante
(137.83099365234375, 57.32845687866211, 291.81768798828125, 65.29855346679688, 'Table 1.\nsample post for the each class.\n', 0, 0)
(172.77200317382812, 74.02144622802734, 454.26800537109375, 81.99154663085938, 'Post text\nClass Label\n', 1, 0)
(143.8090057373047, 88.86544036865234, 430.59539794921875, 105.80252075195312, '1\nShe is simple girl and need not know politics. Let her vote to\nher choice\nNAG\n', 2, 0)
(143.80902099609375, 106.79842376708984, 399.4734802246094, 141.66751098632812, '2\nPeople talk about common man suﬀering all the time.This is\nthe same country where thousands laid their lives for free-\ndom.Cant the common man endure little trouble to stand in\nqueues for the greater good\n', 3, 0)
(411.4990234375, 106.79842376708984, 430.3562927246094, 114.76852416992188, 'CAG\n', 4, 0)
(143.80902099609375, 142.66445922851562, 430.59539794921875, 159.60055541992188, '3\nLangove get out from this conversation. U are an uninvited\ndog here\nOAG\n', 5, 0)
(102.76200103759766, 189.87420654296875, 187.94027709960938, 200.78330993652344, '1. Introduction\n', 6, 0)
(102.76199340820312, 215.7772216796875, 509.3223571777344, 679.9862060546875, 'The Social Web is a great source for studying human interaction and behavior. In\nthe last few years, there is an exponential growth in Social Media user base. Sensing\ncontent of Social Media like Facebook, Twitter, by the smart autonomous application\nempower its user community with real-time information which is unfolded across the\ndiﬀerent part of the world. Social media provide the easiest and anonymous platform\nfor common people to voice their opinion or view on a various entity like celebrity,\npolitician, product, stock market etc or any social movement. Sometime such opinions\nmight be aggressive in nature and propagate hate in the social media community.\nWith the unprecedented increase in the user base of the social media and its avail-\nability on the Smartphones, incidents like Hate speech, trolling, Cyberbullying, and\nAggressive posts are increasing exponentially. A smart autonomous system is required\nwhich enable surveillance on the social media platform and detect such incidents.\nSome of the researchers look posts from the aspect like aggression Kumar Ritesh et\nal. (2018) to ﬁlter the contents. some of the posts contain words which might be qual-\niﬁed as either highly or overly aggressive or have hidden aggression. Sometimes posts\ndo not have any aggression. Based on these, posts or comments are categorized into\nthree classes namely: ‘Overtly Aggressive‘, ‘Covertly Aggressive‘ and ‘Non-aggressive‘\nKumar Ritesh et al. (2018). Henceforth, in the rest of the paper, we will denote these\nclasses by these abbreviations namely: OAG, CAG, NAG respectively.Table 1 shows\nthe sample posts belonging to these classes.\nSocial Media, speciﬁcally Microblog has proved its importance during the disaster-\nrelated incidents like an earthquake, Hurricane and ﬂoods 1. Organizations involved\nin relief operation actively track posts related to situational information posted on\nFacebook and Twitter during the disaster. However, At the same time, social media\nis ﬂooded with lots of prayer and condolence messages. Posts which contain factual\ninformation are extremely important for the organization involved in post-disaster\nrelief operations for coordination. Filtering and Ranking of the posts containing factual\ninformation will be very useful to them. We believe that this is the special problem\nof the Sentiment Analysis task. We consider this problem as a combination of two-\nclass classiﬁcation problem: factual posts and nob-factual posts plus Ranking. Table\n2 shows the example of the posts of belong to these class.\nThe Text representation of social web content plays a pivotal role in any NLP task.\nBag-of-word is the oldest and simple technique to represent the document or post into\na ﬁxed length vector. The BoW techniques generate very sparse and high dimensional\nspace vector. Text representation using distributed word/sentence representation or\nword embedding is gain rapid momentum recently. In this paper, one of the objectives\n', 7, 0)
(107.0790023803711, 695.9447631835938, 364.7986755371094, 705.2335815429688, '1https://phys.org/news/2018-08-social-media-bad-disaster-zones.html\n', 8, 0)
(303.27301025390625, 727.0491943359375, 308.7275695800781, 737.9583129882812, '2\n', 9, 0)

page suivante
(137.83099365234375, 57.32845687866211, 291.81768798828125, 65.29855346679688, 'Table 2.\nsample post for the each class.\n', 0, 0)
(172.77200317382812, 74.02144622802734, 454.26800537109375, 81.99154663085938, 'Post text\nClass Label\n', 1, 0)
(143.8090057373047, 88.86544036865234, 399.48162841796875, 114.76852416992188, '1\n#Nepal #Earthquake day four. Slowly in the capital valley\nInternet and electricity beeing restored . A relief for at least\nsome ones\n', 2, 0)
(411.4990234375, 88.86544036865234, 438.88427734375, 96.83554077148438, 'Factual\n', 3, 0)
(143.80902099609375, 115.76546478271484, 454.7607116699219, 132.70156860351562, '2\nPMOIndia Indian Government is doing every possible help to\nthe earthquake victims and they need money so plz contribute\nNon-factual\n', 4, 0)
(102.76200103759766, 162.43719482421875, 509.3005676269531, 406.4712219238281, 'is to ﬁnd the best text representation scheme to model social web content for the\nmachine learning classiﬁer and deep neural net. Various Text representation scheme\nbased on BoW, word embedding and are studied empirically. We have reported result\non popular word embedding technique like Word2vec, Glove and fastText on stan-\ndard machine learning classiﬁer like Multinomial Naive Bayes (MNB), Logistic Re-\ngression(LR), K-Nearest neighbors KNN, Support Vector Classiﬁer (SVC), Decision\nTree (DT),Stochastic Gradient Descent(SGD), Random forest (RF), Ridge, AdaBoost,\nPerceptron, Deep neural net based on LSTM, CNN and Bidirectional LSTM. Results\nare also reported on Doc2vec embedding, a popular sentence or paragraph embedding\ntechnique for the above classiﬁers.\nTransfer Learning is well practiced in the area of computer vision. However, in the\nNLP, transfer learning has limited application in the form of pre-trained word vector\nwhich is used to initialize the weights of the embedding layer of the deep neural net-\nwork. With the advent of transfer learning method like ELMO (Peters et al. , 2018),\nULMFiT (Howaard et al., 2018) claimed substantial improvement in the performance\nof various NLP tasks like Sentiment Analysis, Question/Answering, Textual Entail-\nment empirically. The main idea behind these methods is to train language model\non the large corpus and ﬁne tune on the task-speciﬁc corpus. In this paper, We have\nevaluated the performance of these methods in the Aggression classiﬁcation tasks.\n', 5, 0)
(102.76200103759766, 432.1114807128906, 237.07481384277344, 443.064208984375, '1.1. Research Questions\n', 6, 0)
(102.76200103759766, 452.0801086425781, 509.2569885253906, 475.940185546875, 'In this study, experiments are performed on the benchmark dataset with to answer\nthe following questions\n', 7, 0)
(119.1259994506836, 483.6873474121094, 509.30072021484375, 567.3726196289062, '• Which is the best Text Representation scheme to model text from the Social\nWeb?\n• Does pre-trained language model based on transfer learning better than pre-\ntrained word embedding based on shallow transfer learning on Social media\ndata?\n• Does Making too Deep Neural net make sense?\n', 8, 0)
(102.76200103759766, 567.6470336914062, 509.31146240234375, 708.0701904296875, 'To answer all research question listed above, experiments are performed on two\ntasks namely: Aggression detection (Trolling Aggression and Cyberbullying (TRAC)\ndataset) Kumar et al. (2018) and Fact detection (FIRE iRMDI Dataset)Basu et al.\n(2018). In this paper, we present exhaustive benchmarking of text representation\nschemes on these datasets. Our results reveal that fastText with pre-trained vector\nalong with CNN outperform standard machine learning classiﬁers based on BoW\nModel and marginally perform better than Word2vec and Glove. Paragraph vector\nor Doc2vec Le, Quoc et al. (2014) perform very poor on our dataset and turn out to\nbe the worst text representation scheme among all. We also found that model based\non the deep neural net is more robust than machine learning classiﬁer when tested\non lexically diﬀerent dataset than training Dataset. i.e. deep neural model substan-\n', 9, 0)
(303.27301025390625, 727.049072265625, 308.7275695800781, 737.9581909179688, '3\n', 10, 0)

page suivante
(102.76200103759766, 60.54815673828125, 509.2787170410156, 252.77732849121094, 'tially outperforms machine learning classiﬁer on Twitter test Dataset while trained on\nFacebook Dataset in this evaluation.\nTo validate our claims, statistical signiﬁcance tests are performed on weighted F1-\nscore of the classiﬁer for each text representing scheme. Statistical inference is used\nto check evidence to support or reject these claims. Signiﬁcance tests like Wilcoxon\nsigned-rank and Student t-test were carried out by comparing weighted F1 score all\nthe text representation scheme with the fastText pre-trained vector. In most of the\ncases, p-values are less than 0.05.\nThe rest of the paper is organized as follows: In section 2, we review the relevant\nworks in the area of Sentiment analysis and hate speech detection. Section 3 contains\nthe detail information about the various benchmark Datasets used in the experiments.\nVarious Text Representation schemes are described in section 4. We formally describe\nthe evaluation task and models in section 5. We report results in section 6 and present\ndetail result analysis in section 7. We conclude the discussion and provide insight for\nthe future work in section 8.\n', 0, 0)
(102.76202392578125, 280.7232360839844, 195.2930450439453, 291.63232421875, '2. Related Work\n', 1, 0)
(102.76202392578125, 306.625244140625, 509.3114929199219, 680.1752319335938, 'Bag-of-Words (BoW) (Harris et al. , 1954) is the oldest technique to represent the\ntext of the documents in ﬁxed-length vectors with high dimensionality. Mikolov et.al\n(2013) proposed two architecture namely: skip-gram(SG) and continuous-bag-of word\n(CBOW) to learn high quality low dimensional word embedding. However, to gener-\nate sentence vector often, average or mean of word vector are considered. Doc2vec or\nparagraph vector Le, Quoc et al. (2014) proposed Paragraph2vec (Doc2vec) which is\nthe extension of the Word2vec to learning document level embedding. It is an unsuper-\nvised method which learns document vector from paragraph, sentence or document.\nPennington et.al. (2014) proposed word embedding based on the co-occurrence ma-\ntrix. Lau et al. (2016) have performed a comprehensive evaluation of Doc2Vec on two\ntasks namely: Forum Question Duplication and Semantic Textual Similarity (STS)\ntask. Authors claimed that Doc2Vec performs better than Word2vec provided that\nmodels trained on large external corpora, and can be further improved by using pre-\ntrained word embedding. They have published the hyper-parameter for the Doc2Vec\nembedding. Our work is similar to this but we have reported the evaluation of all the\ntext representation scheme including doc2vec on TRAC dataset(Kumar et al. , 2018)\non each classiﬁer.\nHate speech is a type of language which is used to incite or spread violence to-\nwards the group of people based on the gender, community, race, religion. Sentiment\nanalysis and hate-speech are closely related in fact sentiment analysis techniques are\nused in hate speech detection. Initially, Sentiment Analysis problem is formulated as\na binary classiﬁcation problem for predicting the election results or detecting political\nopinion (Conover et al. , 2011; Conover Micheal et al. , 2011; Maynard et al., 2011;\nTumasjan et al. , 2010) on Twitter. Then after, It turned into the multi-class classi-\nﬁcation problem with the introduction of the neutral label. Soon, Researchers come\nwith diﬀerent notion like aggression (Kumar Ritesh et al., 2018), cyberbullying(xu et\nal. , 2012), sarcasm, trolling. Semeval (International workshop on semantic evalua-\ntion)(Rosenthal et al. , 2017) is one of the popular competition on sentiment analysis\nwhich is started since 2013. TRAC 2(Trolling, aggression, cyberbullying) workshop\n', 2, 0)
(107.0790023803711, 696.7398071289062, 244.22703552246094, 706.0275268554688, '2https://sites.google.com/view/trac1\n', 3, 0)
(303.27301025390625, 727.0491943359375, 308.7275695800781, 737.9583129882812, '4\n', 4, 0)

page suivante
(102.76200103759766, 60.54815673828125, 509.2568359375, 110.31229400634766, '(Kumar Ritesh et al., 2018) co-located with the International Conference of Com-\nputational Linguistics (COLING 2018) redeﬁne hate speech detection task in terms\nof three type of aggression namely: Non-Aggression (NAG), Overly-Aggression(OAG)\nand Covertly Aggression (CAG).\n', 0, 0)
(102.76200103759766, 136.2205810546875, 236.3875274658203, 147.17332458496094, '2.1. Sentiment Analysis\n', 1, 0)
(102.76197814941406, 156.19024658203125, 509.3006286621094, 439.0792541503906, 'During the initial year, there is a lack of standard dataset for comparative perfor-\nmance analysis. International Workshop on Semantic Evaluation 2013 (SemEval-2013)\n(Hltcoe et.al , 2013) was the ﬁrst forum who developed standard tweet dataset for the\nbenchmarking of the various sentiment analysis system. Most of the team who had par-\nticipated in the competition used supervised approaches based on SVM, Naive Bayes,\nand Maximum Entropy. some of the team had used ensemble classiﬁer and rule-based\nclassiﬁer. Mohammad et al. (2013) was the top team of the Semeval-2013 challenge.\nThey have incorporated various semantic and lexicon based sentiment features for the\nexperiment and SVM was used for the classiﬁcation. Deep learning and word embed-\nding had shown its footprints in SemEval-2015 (Rosenthal et al. , 2015). Team UNITN\n(Severyn et al. , 2015) was the second team in the message polarity task. They have\nbuild convolution neural network for the sentiment classiﬁcation. They have used an\nunsupervised neural language model to initialize word embeddings that are further\ntuned by deep learning model on a distant supervised corpus (Severyn et al. , 2015).\nIn fourth edition SemEval-2016 (Nakov et al. , 2016),Team SwissCheese (Deriu et al.\n, 2016) was the ﬁrst ranked team with F1 score around 63.3 %. Their approach was\nbased on 2-layer convolution neural networks whose predictions are combined using a\nrandom forest classiﬁer. SemEval-2017 (Rosenthal et al. , 2017) was the ﬁfth edition,\nTeam DataStories (Baziotis et al. , 2017) was the top-ranked team with AvgRec= 68.1\nand F1 around=67.7 %. They use Long Short-Term Memory (LSTM) networks aug-\nmented with two kinds of attention mechanisms, on top of word embedding pre-trained\non a big collection of Twitter messages without using any hand-crafted features.\n', 2, 0)
(102.76197814941406, 464.9875183105469, 398.9549255371094, 475.94024658203125, '2.2. Hate Speech/Cyberbullying/Aggression Detection\n', 3, 0)
(102.76197814941406, 484.9571533203125, 509.321044921875, 703.0892333984375, 'Hate Speech Detection research attracts researchers from the diverse background\nlike Computational linguistic, computer science, social science. The actual term hate\nspeech was coined by Warner et al.\n(2012). Various Authors used diﬀerent notion\nlike oﬀensive language (Razavi et.al , 2010), Cyberbullying (xu et al. , 2012), Aggres-\nsion (Kumar et al. , 2018). Davidson et al. (2017) studied tweet classiﬁcation of hate\nspeech and oﬀensive language and deﬁned hate speech as following: language that is\nused to expresses hatred towards a targeted group or is intended to be derogatory,\nto humiliate, or to insult the members of the group. Authors observed that oﬀensive\nlanguage often miss-classiﬁed as hate speech. They have trained a multi-class classi-\nﬁer on N-gram features weighted by its TF-IDF weights and PoS tags. In addition\nto these, features like sentiment score of each tweet, no of hashtags, URLS, mentions\nare considered. Authors concluded that Logistic regression and Linear SVM perform\nbetter than NB, Decision Tree, Random Forests. Schmidt et al. (2017) perform com-\nprehensive survey on hate speech. They have identiﬁed features like Surface features,\nsentiment, word generalization,lexical, linguistics etc. can be used by classiﬁer.\nCyberbullying is the type bullying that occurs on social media platform or app via\ncellphone or any internet enabled device. xu et al. (2012) introduces Cyberbullying to\n', 4, 0)
(303.2729797363281, 727.0491333007812, 308.7275390625, 737.958251953125, '5\n', 5, 0)

page suivante
(102.76200103759766, 60.54815673828125, 509.30047607421875, 460.0002136230469, 'the NLP community. They have performed various binary classiﬁcation on tweets text\nwith bullying perspective to determine whether the user is cyberbully or not. They\nreported binary classiﬁcation accuracy around 81%. Kwok et al. (2013(@), authors\nperformed classiﬁcation using NB classiﬁer on tweets based on two classes :racist and\nnon-racists and achieved accuracy around 76 %. Burnap et al. (2015), authors studied\ncyber hate on Twitter. They have used various classiﬁer like SVM, BLR, RFDT, Voting\nbase ensemble for the binary classiﬁcation achieved best F1-score of 0.77 in the voted\nensemble.Malmasi, et al.\n(2017), authors have used NLP based lexical approach to\naddress the multi-class classiﬁcation problem. They have used character N-gram, word\nN-gram and word skip-gram feature for the classiﬁcation.\nSchmidt et al. (2017), have described the key areas that have been explored to detect\nhate speech. They have surveyed diﬀerent types of features used for hate speech classi-\nﬁcation. They have categorized features in Simple surface features, word generalization\nfeatures, sentiment features, linguistic features, lexical resources features, Knowledge-\nbased features, and Meta-Information features Simple surface features include features\nlike character level unigram/n-gram, word generalization features include features like\nthe bag-of-words, clustering, word embedding, paragraph embedding. Linguistic fea-\ntures include PoS tag of tokens. list of bad words or hate words can be considered as\na lexical resource. Malmasi et al.\n(2018), tried to address the problem of discrimi-\nnating profanity from the hate speech in the social media posts. n-grams, skip-gram\nand clustering based word representation features are considered for the 3-class classi-\nﬁcation.The Author use SVM and advance ensemble based classiﬁer for this task and\nachieved 80 % accuracy.\nAroyehun et al. (2018) performed translation as data augmentation strategy. TRAC\nDataset (Kumar et al. , 2018) was also augmented using translation and pseudo labeled\nusing an external dataset on hate speech. they have reported best performance with\nLSTM and F1 score around 0.6415 on TRAC English dataset (Kumar et al. , 2018).\nArroyo et al.\n(2018) implement ensemble of the Passive-Aggressive (PA) and SVM\nclassiﬁers with character n-grams. TF-IDF weighting used for feature representation.\nFIRE initiative also gave importance text representation in Indian language since its\ninception. (Majumder et al. , 2008)(Majumder et al. , 2007).\n', 0, 0)
(102.76200103759766, 487.94512939453125, 161.0602569580078, 498.8542175292969, '3. Dataset\n', 1, 0)
(102.76200103759766, 513.84814453125, 509.28955078125, 576.563232421875, 'Experiments are performed on standard benchmarked Datasets to evaluate the perfor-\nmance of various text representation scheme. For User Aggression detection problem,\nTrolling, Aggression and Cyberbullying TRAC (Kumar et al. , 2018) is considered for\nthe experiments which contain post in English and code-mixed Hindi. For the Factual\nDetection task, experiments are performed on FIRE IRMiDis Dataset.\n', 2, 0)
(102.76200103759766, 602.4725341796875, 212.5620574951172, 613.42529296875, '3.1. TRAC Dataset\n', 3, 0)
(102.76200103759766, 622.441162109375, 509.27874755859375, 659.2532958984375, 'TRAC (Trolling, Aggresion and Cyberbullying) consist of 15,001 aggression-annotated\nFacebook Posts and Comments each in Hindi (Romanized and Devanagari script) and\nEnglish for training and validation Kumar et al. (2018).\n', 4, 0)
(303.27301025390625, 727.0491333007812, 308.7275695800781, 737.958251953125, '6\n', 5, 0)

page suivante
(163.96400451660156, 57.32845687866211, 360.6225891113281, 65.29855346679688, 'Table 3.\nClass distribution in the Training Dataset\n', 0, 0)
(230.25599670410156, 74.02144622802734, 409.69482421875, 81.99154663085938, 'English Corpus\nHindi Corpus\n', 1, 0)
(201.42100524902344, 88.24642181396484, 442.0862121582031, 96.21652221679688, '# Training\n# Validation\n# Training\n# Validation\n', 2, 0)
(169.9409942626953, 112.0573959350586, 424.8613586425781, 146.92648315429688, 'NAG\n5,052\n1, 233\n2, 275\n538\nCAG\n4240\n1, 057\n4, 869\n1, 246\nOAG\n2, 708\n711\n4, 856\n1217\nTotal\n12, 000\n3, 001\n12, 000\n3, 001\n', 3, 0)
(203.77999877929688, 166.01040649414062, 346.40234375, 173.98049926757812, 'Table 4.\nTest Data Corpus statistics\n', 4, 0)
(209.75799560546875, 182.70346069335938, 402.2438659667969, 190.67355346679688, 'Test Dataset\n# of posts\n', 5, 0)
(209.75799560546875, 197.54843139648438, 378.349365234375, 232.41751098632812, 'Facebook English Corpus\n916\nTwitter English Corpus\n1, 257\nFacebook mixed script Hindi Corpus\n970\nTwitter mixed script Hindi Corpus\n1, 194\n', 6, 0)
(102.76200103759766, 262.64752197265625, 260.4530334472656, 273.6002502441406, '3.2. FIRE IRMiDis Dataset\n', 7, 0)
(102.76200103759766, 282.61614990234375, 509.31134033203125, 410.0881652832031, 'Forum for Information Retrieval Evaluation, have introduced Microblog track since\n2016 as Information Retrieval from Microblogs during Disasters (IRMiDis). IRMiDis\ntrack (Basu et al. , 2018) of FIRE is organized with the objective to extract factual\nor fact-checkable tweets during the disaster which might be helpful to the victims\nor the people who are involved in the relief operation. Dataset contain tweets which\nare downloaded from the Twitter during Nepal earthquake 2015. Following are the\nexample of factual or fact-checkable and non-fact-checkable tweet.Table 5 shows a\ndetail statistics of FIRE IRMiDis Dataset. As we look at the table, There are only\n83 tweets is annotated with objective class. not a single tweet is annotated from the\nsubjective class.\n', 8, 0)
(102.76200103759766, 438.0340881347656, 281.0931091308594, 448.94317626953125, '4. Text Representation Schemes\n', 9, 0)
(102.76200103759766, 463.93609619140625, 509.2896728515625, 578.4571533203125, 'The main objective of this paper is an identiﬁcation of the best text representation\nscheme for the Social media text which is very sparse and noisy in nature. Text rep-\nresentation is about representing documents in a numerical way so that they can be\nfeed as an input to the classiﬁer. This numerical representation is in the form of the\nvectors which together form matrices. Essentially, There are two types of text repre-\nsentation scheme :(i) Bag-of-words(BoW) (ii) Distributed Word/sentence representa-\ntion. BoW with count vector and TF/IDF weighting , various word embedding tech-\nniques(Word2Vec, Glove, fastText), and sentence or paragraph embedding (Doc2Vec)\nare studied.\n', 10, 0)
(172.74700927734375, 595.2122802734375, 335.693115234375, 603.1824340820312, 'Table 5.\nFIRE IRMiDis Dataset statistics\n', 11, 0)
(178.72500610351562, 610.3554077148438, 333.1405334472656, 618.3255615234375, 'Particulars\n# tweets\nRemark\n', 12, 0)
(178.72500610351562, 625.1994018554688, 433.20233154296875, 651.1025390625, 'Number of Tweets\n50000+\nLabelled Tweets\n83\nonly tweets belong to Factual class\nClasses\n2\n', 13, 0)
(303.27301025390625, 727.0491943359375, 308.7275695800781, 737.9583129882812, '7\n', 14, 0)

page suivante
(102.76200103759766, 60.504520416259766, 374.57318115234375, 71.4572525024414, '4.1. Bag-of-Word Model for Text Representation\n', 0, 0)
(102.76200103759766, 80.47418212890625, 509.26788330078125, 182.04331970214844, 'The Bag-of-words is the simple technique to represent the document or social media\nposts in the vector form and also a very common feature extraction method from the\ntext. Word count or TF/IDF weight of each n-gram word can be used as a features.\nThe dimension of the vector is equal to the size of vocabulary of the text corpus\nor dataset which results in very high dimensional sparse document vector. It is the\ncommon method used for the text representation in order to perform various NLP task\nlike text classiﬁcation, clustering. However,the BoW methods ignore the word order\nwhich may lead to loss of the context.\n', 1, 0)
(102.76200103759766, 206.1585693359375, 355.2204284667969, 217.11131286621094, '4.2. Word Embedding for Text representation\n', 2, 0)
(102.76200103759766, 226.127197265625, 509.28961181640625, 288.8423156738281, 'Word Embedding is the text representation technique to represent the word in the low\ndimensional space so that semantically similar word have similar representation. Major\nword embedding techniques like Word2vec learn word embedding using shallow neural\nnetwork. The fastText, extension of Word2vec, consider the morphological structure\nof the word.\n', 3, 0)
(102.76200103759766, 307.02423095703125, 183.52206420898438, 317.9333190917969, '4.2.1. Word2Vec\n', 4, 0)
(102.76200103759766, 325.9532470703125, 509.30072021484375, 401.6192932128906, 'Word2vec (Mikolov et.al , 2013) is the unsupervised and predictive neural word em-\nbedding technique to learn the word representation in the low dimensional space.\nWord2vec is a two-layer neural net that take text corpus as an input and output is a\nset of vectors. two novel model architectures: Skipgram and CBOW(Continuous bag\nof words) are proposed for computing continuous vector representations of words from\nvery large data sets.\n', 5, 0)
(102.76200103759766, 419.80120849609375, 161.95477294921875, 430.7102966308594, '4.2.2. Glove\n', 6, 0)
(102.76200103759766, 438.730224609375, 509.3114318847656, 501.4452819824219, 'GloVe stands for Global vector for [Word Representation] (Pennington et.al. , 2014)is\nan unsupervised method for learning word embedding. A Co-occurrence word matrix\nis created from the text corpus for the training and is reduced in low dimensional space\nwhich explain the variance of high dimensional data and provide word vector for each\nword.\n', 7, 0)
(102.76200103759766, 519.627197265625, 172.7875213623047, 530.5363159179688, '4.2.3. fastText\n', 8, 0)
(102.76200103759766, 538.5562133789062, 509.3115234375, 640.1253051757812, 'fastText (Bojanowski et al. , 2017) is the neural word embedding technique which\nlearn distributed low dimensional word embedding. Word2vec, Glove consider each\nword as single unit and ignore the morphological structure of the word. They are not\nable generate word embedding for the unseen or out of vocabulary word during the\ntraining. fastText overcome this limitation of Word2vec and GLOVE by considering\neach word as N-gram of characters. A word vector for a word is computed from the\nsum of the n-gram characters. The range of N is typically 3 to 6. Since user on social\nmedia often make spelling error, typos, fastText will be more eﬀective then rest of two.\n', 9, 0)
(102.76200103759766, 664.2405395507812, 274.4057922363281, 675.1932983398438, '4.3. Paragraph vector/Doc2vec\n', 10, 0)
(102.76200103759766, 684.210205078125, 509.3004455566406, 708.0703125, 'Paragraph Vector is an unsupervised algorithm that learns ﬁxed-length feature rep-\nresentations from variable-length pieces of texts, such as sentences, paragraphs, and\n', 11, 0)
(303.27301025390625, 727.0491943359375, 308.7275695800781, 737.9583129882812, '8\n', 12, 0)

page suivante
(102.76200103759766, 60.54815673828125, 509.3440856933594, 226.8743133544922, 'documents (Le, Quoc et al. , 2014). Paragraph vector represents each document by\na dense vector which is trained to predict words in the document. Authors believe\nthat Paragraph vector have the potential to overcome the weaknesses of bag-of-words\nmodels and claimed that Paragraph Vectors outperform bag-of-words models as well\nas other techniques for text representations. Paragraph vector model is also referred\nas doc2vec model. Henceforth, we will refer paragraph and Doc2vec interchangeably.\nDoc2vec model have two architecture namely : (i) DM: This is the Doc2Vec model\nanalogous to CBOW model in Word2vec. The paragraph vectors are obtained by train-\ning a neural network on the task of inferring a center word based on context words and\na context paragraph. (ii) DBOW: This is the Doc2Vec model analogous to Skip-gram\nmodel in Word2Vec. The paragraph vectors are obtained by training a neural network\non the task of predicting a probability distribution of words in a paragraph given a\nrandomly-sampled word from the paragraph.\n', 0, 0)
(102.76200103759766, 252.7835693359375, 229.362060546875, 263.7362976074219, '4.4. Transfer Learning\n', 1, 0)
(102.76200103759766, 272.752197265625, 509.300537109375, 594.4961547851562, 'Transfer Learning in NLP is not as matured as compare to in Computer Vision. Trans-\nfer learning is a method in which model is trained on large corpus for a particular task\nand use this pre-trained model for the similar task. There are two way to use transfer\nlearning in NLP (i) Use of Pre-trained word embedding to initialize ﬁrst layer of neural\nnetwork model which can be called as shallow representation. (ii) Use the full model\nand ﬁne tune for the task speciﬁc in supervise learning way.\nWord2vec, Glove and fastText provide pre-trained word vector trained on the large\ncorpus. Google Word2vec pre-trained model have word vector for 3 million words with\nsize 300 and trained on Google news. Glove pre-trained model available with diﬀerent\nembed size and trained on common crawl, Twitter. We have use Glove pre-trained\nmodel with vocabulary size 2.2 million and trained on common crawl. fastText pre-\ntrained models are available in 157 language. We have use fasttext pre-trained vector\nfor Englsih and Hindi language trained on commnon crawl and wikipedia.\nRecently, transfer learning in NLP done in new way; First language model is trained\non large text corpus in unsupervise way and ﬁne tune on speciﬁc task like text classiﬁ-\ncation on labeled data. Peters et al. (2018) author argued that word representation is\ndepend upon the context. So each word has diﬀerent word vector depending upon the\nposition of the word in the sentence. Essentially Each word has dynamic word vector\nwith respect to the context as opposed to the traditional word embedding techniques\nwhich always give same word vector ignoring the context. Embedding from Language\nModels (ELMos) use languge model for the word embedding. Howaard et al. (2018)\nauthor propose Universal Language Model Fine-Tuning for Text Classiﬁcation (ULM-\nFiT) which is bi-LSTM model that is trained on a general language modeling (LM)\ntask and then ﬁne tuned on text classiﬁcation. Results are reported on both transfer\nlearning model on TRAC dataset (Kumar et al. , 2018).\n', 2, 0)
(102.76200103759766, 622.4410400390625, 211.6893768310547, 633.3501586914062, '5. Evaluation Tasks\n', 3, 0)
(102.76200103759766, 648.3440551757812, 509.2350158691406, 685.1561889648438, 'We have benchmarked various text representation scheme on two specialized NLP\ntask namely: aggression detection and fact detection. Text Representation scheme are\nevaluated on machine learning and deep neural model.\n', 4, 0)
(303.27301025390625, 727.049072265625, 308.7275695800781, 737.9581909179688, '9\n', 5, 0)

page suivante
(102.76200103759766, 60.504520416259766, 271.91845703125, 71.4572525024414, '5.1. Aggression Detection task\n', 0, 0)
(102.76200103759766, 80.47418212890625, 509.2896423339844, 207.94529724121094, 'The objective of this task is to identify type of aggression present in the text in\nboth Englsih and code-mixed Hindi language. Aggression are classiﬁed into three\nlevel namely: ‘Overtly Aggressive‘ (OAG), ‘Covertly Aggressive‘ (CAG) and ‘Non-\naggressive‘ (NAG). We have implemented all standard machine learning classiﬁers\nlike Multinomial Naive Bayes (MNB), Logistic Regression(LR), K-Nearest neighbors\n(KNN), Support Vector Classiﬁer (SVC), Decision Tree (DT),Stochastic Gradient De-\nscent(SGD), Random forest (RF), Ridge, AdaBoost, Perceptron, and various voting\nbased ensemble with diﬀerent text representation schemes like count based, TF/IDF\nand word embedding to prepare baseline results. Various word embedding techniques\nlike Word2Vec, Glove, fastText, Paragraph2Vec are studied.\n', 1, 0)
(102.76200103759766, 227.92120361328125, 223.77667236328125, 238.83030700683594, '5.1.1. Problem statement\n', 2, 0)
(102.76200103759766, 246.8502197265625, 509.23504638671875, 283.66229248046875, 'Basically Aggression detection is a Text classiﬁcation problem. Formally, the task of\nText Classiﬁcation is stated as follows. Given a set of social media feed and a set of\nclasses, We need to compute a function of the form:\n', 3, 0)
(276.9330139160156, 298.65521240234375, 335.05511474609375, 309.5643005371094, 'C = f(T, Ω)\n', 4, 0)
(102.76200866699219, 324.5582275390625, 509.2569580078125, 361.37030029296875, 'where f is the multi-class classiﬁer that is computed using training data, T is the\nnumeric representation of the text of the dataset, Ω is the set of parameters of the\nclassiﬁer and C is the pre-deﬁne class-labels.\n', 5, 0)
(102.76200866699219, 381.34521484375, 334.10040283203125, 392.2543029785156, '5.1.2. Model Architectures and Hyperparameters\n', 6, 0)
(102.76200866699219, 400.27423095703125, 509.2896423339844, 450.0382995605469, 'In this subsection, we will discuss the architecture and hyperparameters of our deep\nneural model used for the classiﬁcation. Model learns feature from the input texts\nTher is no need to design hand-crafted features which used to encode text into feature\nvector.\n', 7, 0)
(102.76199340820312, 469.9695739746094, 509.3005676269531, 610.436279296875, '5.1.2.1. Bidirectional LSTM. The ﬁrst model is based on the Bidirectional LSTM\ninclude embedding layer with embed size 300, convert each word from the post into\na ﬁxed length vector. short posts are padded with zero values. Subsequent layers\nincludes Bidirectional LSTM layer with 50 memory units followed by one-dimensional\nglobal max pooling layer, a hidden layer with size 50 and output layer with softmax\nactivations. ReLU activation function is used for the hidden layer activation. A drop\nout layer is added between the last two layers to counter the overﬁtting with parameter\n0.1. Hyperparameters are as follows: Sequence length is ﬁxed at 1073 word; maximum\nlength of posts in the dataset. No of features is equal to half of total vocabulary size.\nModels are trained for 10 epoch with batch size 128. Adam optimization algorithm is\nused to update network weights.\n', 8, 0)
(102.76199340820312, 630.3675537109375, 509.31146240234375, 706.0773315429688, '5.1.2.2. Single LSTM with higher dropout. This model is based on the Long\nShort Term Memory, a type of recurrent neural network with higher dropout. This\nmodel is having one embedding layer, one LSMT layer with a size 64 memory unit,\nand one fully connected hidden layer with Relu activation and size 256 and an output\nlayer with softmax activation. Hyperparameters are same as discussed in the previous\nmodel. A dropout layer is added between the hidden layer and an output layer with\n', 9, 0)
(300.54498291015625, 727.0491943359375, 311.4541015625, 737.9583129882812, '10\n', 10, 0)

page suivante
(102.76200103759766, 60.54815673828125, 336.7840270996094, 71.4572525024414, 'drop out rate 0.2 to address the overﬁtting issue.\n', 0, 0)
(102.76199340820312, 91.38848876953125, 509.3114929199219, 180.05027770996094, '5.1.2.3. CNN Model. This model includes one embedding layer whose weights\nare initialized with fastText pre-trained vector with embed size is 300, followed by\none-dimensional convolution layer with 100 ﬁlters of height 2 and stride 1 to target\nbiagrams. In addition to this, Global Max Pooling layer added to fetch the maximum\nvalue from the ﬁlters which are feed to the fully connected hidden layer with size\n256, followed by output layer. ReLU and softmax activation function are used for the\nhidden layer and output layer respectively.\n', 1, 0)
(102.76199340820312, 199.98150634765625, 509.3004150390625, 314.5462341308594, '5.1.2.4. CNN model with Multiple Convolution layer. This model includes\nembedding layer with embed size 300. Three one dimensional convolution layers with\nsize 100 and diﬀerent ﬁlters with height 2,3,4 to target bigrams, trigrams, and four-\ngrams features, followed by max pooling layer which concatenate max pooled result\nfrom each of one-dimensional convolution layer. The ﬁnal two layers include a fully con-\nnected hidden layer with size 250 and output layer with ReLu and softmax activation.\nA Drop out layer is added between the last two layer with rate 0.2. Hyperparameters\nare same as discussed in the ﬁrst model. This model is similar to proposed by (Zhang\net al. , 2015).\n', 2, 0)
(102.76199340820312, 340.4544982910156, 399.57672119140625, 351.4072265625, '5.2. Factual Post/Tweet Detection from Social Media\n', 3, 0)
(102.76199340820312, 360.42413330078125, 509.30059814453125, 526.7501220703125, 'During the emergency situation like earthquake or ﬂoods, Microblog plays a very\nimportant role as an anonymous communication medium. The various entity like,\nVolunteers, NGOs involved in relief operation always look for real-time information\nwhich contains facts instead of prayer and condolence messages. In more technical\nterm, these agencies are looking for factual information from Microblog instead of the\nsubjective information. In addition to this, the system should generate rank-list of the\ntweets based upon the worthiness of facts. we considered this problem as a binary\nclassiﬁcation problem plus pure IR Ranking problem. two classes can be labeled as\nfactual and non-factual.\nthe IRMiDis dataset 3,which was prepared from the tweet posted during Nepal\nearthquake 2015 Basu et al. (2018) is considered for the experiment. There are only\n83 fact checkable tweets in the dataset. Non-factual tweets are not available. Total no\nof tweets in the dataset is more than 50000.\n', 4, 0)
(102.76199340820312, 546.7250366210938, 274.6676330566406, 557.6341552734375, '5.2.1. Preparation of Training Data\n', 5, 0)
(102.76199340820312, 565.654052734375, 509.3114318847656, 680.1751708984375, 'Due to the unavailability of adequate training data, The ﬁrst task is to prepare\ntraining data to train the deep neural model. We randomly choose 100 tweets from\nthe dataset and labeled as a non-fact-checkable tweet and 83 fact-checkable tweets\npresent in the dataset labeled as fact-checkable. We have trained our Convolution\nneural network on these training data and tested the model on the remaining 50000\ntweets. At this stage we are not interested in the class but, we have sorted all the\ntweets based upon the predicted probability of the fact-checkable class and selected\ntop 2000 tweets. We have randomly selected tweets and gave relevance judgment\nbased upon availability of factual information in ﬁrst 1000 tweets and manually\n', 6, 0)
(107.0790023803711, 696.1337890625, 279.8294982910156, 705.4215698242188, '3https://sites.google.com/site/irmidisﬁre2018/\n', 7, 0)
(300.5450134277344, 727.0491943359375, 311.4541320800781, 737.9583129882812, '11\n', 8, 0)

page suivante
(102.76200103759766, 60.54815673828125, 509.2569274902344, 123.2632827758789, 'extracted 300 tweets as non-fact-checkable tweets to minimize the false positives.\nRemaining 1700 tweets labeled as fact-checkable tweets. We selected the last 1700\ntweets with the least probability of the class fact-checkable and labeled them as\nnon-fact-checkable tweets. So our Training corpus has 1783 fact-checkable and 2000\nnon-fact-checkable tweets.\n', 0, 0)
(102.76200103759766, 155.85821533203125, 225.59848022460938, 166.76731872558594, '5.2.2. Proposed Approach\n', 1, 0)
(102.76200103759766, 174.7872314453125, 509.28948974609375, 263.4043273925781, 'We have used word embedding to represent the text instead of bags-of-words. fastText\n(Mikolov et al. , 2018) pre-trained vector with 300 dimensions is used to initialize the\nweight matrix of the embedding layer of the network. We trained our CNN model on\nthis training corpus with 10-fold cross-validation.The Model gives validation accuracy\naround 94%. Finally, we run the model on the entire corpus and sorted the tweet based\nupon the predicted probability of the Fact-checkable class. Essentially this approach\ntermed as weakly-supervise classiﬁcation.\n', 2, 0)
(102.76200103759766, 291.0172424316406, 159.17295837402344, 301.92633056640625, '6. Results\n', 3, 0)
(102.76200103759766, 316.92022705078125, 509.2787170410156, 418.4892578125, 'In this section, we ﬁrst present results of classiﬁers TRAC dataset Kumar et al.\n(2018) with diﬀerent text representation scheme. Latter we present result on FIRE\nIRMiDis 2018 Dataset. Tweets are very noisy in nature contains user mentions, Hash-\ntags, Emojis, and URLs. We do not perform any kind of text pre-processing on tweets\nin experiments with deep neural models. In experiments with machine learning classi-\nﬁer, before classiﬁcation, Hashtag symbol # and User mentions are dropped from the\ntweets. Non-ASCII characters and stop-words are removed from tweet text (Modha et\nal. , 2016).\n', 4, 0)
(102.76200103759766, 444.0665283203125, 277.3948669433594, 455.0192565917969, '6.1. Results On TRAC Dataset\n', 5, 0)
(102.76200103759766, 464.03515625, 509.3005676269531, 708.0702514648438, 'Precision, Recall, and F1-score are the standard metrics which are used to evaluate the\nclassiﬁer performance. We have evaluated 16 classiﬁers performance on 4 Datasets (2\nEnglish+2 Hindi) with 10 Text Representation scheme(8 in the case of Hindi Dataset).\nLooking at such massive experiment, it is diﬃcult to report results in all the above\nmetrics. Therefore, Results are reported in terms of weighted F1-score only which is\nthe function of Precision and Recall. Classiﬁers results based on LSTM and CNN\non BoW text representation schemes are not possible due to the high dimensionality.\nBernoulli classiﬁer is used instead of Naive Bayes Classiﬁer in case of text represen-\ntation schemes other than BoW. Since word vectors might have negative weights, it\nis impossible to calculate probabilities with negative weights. Skip-gram variant of\nWord2Vec and fastText is used in this experiment instead of continuous bag-of-word.\nTable 6 and 7 shows results on Facebook and Twitter English Dataset with BoW and\nword embedding while Table 8 present result with pre-trained word embedding with\nsame dataset. Table 9 and 10 shows results on Facebook and Twitter code-mixed Hindi\nDataset. Only fastText provide pre-trained word vector (Mikolov et al. , 2018) for the\nHindi language. Exhaustive evaluation is performed with all classiﬁers with respect\nto each text representation schemes. Experiments are also performed with the new\ntransfer learning model like ELMO and ULMFIT. Table 11 presents results on both\nFacebook and Twitter English Datasets. Figure 1 and ﬁgure 2 display the heatmap of\n', 6, 0)
(300.5450134277344, 727.0491333007812, 311.4541320800781, 737.958251953125, '12\n', 7, 0)

page suivante
(124.95099639892578, 57.32845687866211, 335.4297180175781, 65.29855346679688, 'Table 6.\nF1-score on TRAC Facebook English Dataset\n', 0, 0)
(130.9290008544922, 74.02144622802734, 483.8061218261719, 90.95755004882812, 'Classiﬁer\nCount-\nvector\nTF/IDF W2Vec\nGlove\nFasttext\ndoc2vec-\ndmc\ndoc2vec-\ndbow\n', 1, 0)
(130.92898559570312, 97.8324203491211, 470.5139465332031, 240.29745483398438, 'NB\n0.5571\n0.5596\n0.4870\n0.3873\n0.5035\n0.4585\n0.4634\nLR\n0.5953\n0.6046\n0.5675\n0.5358\n0.5400\n0.5266\n0.5139\nKNN\n0.5466\n0.5428\n0.5061\n0.5130\n0.5113\n0.5114\n0.5095\nSVC\n0.5801\n0.5902\n0.5369\n0.5037\n0.5137\n0.5388\n0.5033\nDT\n0.5269\n0.5055\n0.4468\n0.4067\n0.5002\n0.4198\n0.4198\nSGD\n0.5706\n0.5938\n0.4647\n0.3571\n0.5167\n0.5060\n0.3521\nRF\n0.5621\n0.5582\n0.5199\n0.4752\n0.5513\n0.4230\n0.4210\nRidge\n0.6009\n0.5999\n0.5347\n0.5336\n0.5225\n0.5385\n0.5083\nAdaB\n0.6210\n0.6141\n0.5491\n0.4932\n0.5644\n0.4689\n0.4852\nPerce.\n0.5387\n0.5491\n0.5230\n0.4020\n0.4848\n0.3800\n0.3253\nANN\n0.5703\n0.5350\n0.5350\n0.5037\n0.5380\n0.4980\n0.4401\nEnsemble\n0.58\n0.5900\n0.5558\n0.4067\n0.5617\n0.4980\n0.4401\nLSTM\n0.5649\n0.5454\n0.5062\nBLSTM\n0.5759\n0.4760\n0.5641\nCNN\n0.5515\n0.5365\n0.5638\nNCNN\n0.5919\n0.5488\n0.4849\n', 2, 0)
(124.95099639892578, 259.2584533691406, 328.96600341796875, 267.2285461425781, 'Table 7.\nF1-score on TRAC Twitter English Dataset\n', 3, 0)
(130.9290008544922, 275.950439453125, 483.8061218261719, 292.8875427246094, 'Classiﬁer\nCount-\nvector\nTF/IDF W2Vec\nGlove\nFasttext\ndoc2vec-\ndmc\ndoc2vec-\ndbow\n', 4, 0)
(130.92898559570312, 299.7614440917969, 470.5523986816406, 442.2276306152344, 'NB\n0.5102\n0.4528\n0.5551\n0.3936\n0.5495\n0.3254\n0.3536\nLR\n0.4849\n0.4890\n0.3457\n0.3959\n0.3871\n0.3041\n0.3274\nKNN\n0.3539\n0.2891\n0.3843\n0.3607\n0.3997\n0.3225\n0.3191\nSVC\n0.4642\n0.4853\n0.3078\n0.3627\n0.2858\n0.3019\n0.3274\nDT\n0.4229\n0.4111\n0.3884\n0.3673\n0.3948\n0.3326\n0.3326\nSGD\n0.4682\n0.5020\n0.4512\n0.4182\n0.3838\n0.3251\n0.3350\nRF\n0.4199\n0.3917\n0.4333\n0.3634\n0.4069\n0.3301\n0.3293\nRidge\n0.4703\n0.5003\n0.3352\n0.3877\n0.3180\n0.2994\n0.3243\nAdaB\n0.3343\n0.3696\n0.4485\n0.3552\n0.4215\n0.3288\n0.3223\nPerce.\n0.4930\n0.4778\n0.3521\n0.3938\n0.3340\n0.3015\n0.2990\nANN\n0.4912\n0.5164\n0.5111\n0.3552\n0.4532\n0.3230\n0.3281\nEnsemble\n0.495\n0.4842\n0.4500\n0.3938\n0.4471\n0.3230\n0.3281\nLSTM\n0.5385\n0.5156\n0.5335\nBLSTM\n0.5314\n0.3860\n0.4985\nCNN\n0.5012\n0.5377\n0.4849\nNCNN\n0.5120\n0.4984\n0.5179\n', 5, 0)
(102.76200103759766, 472.253173828125, 433.8968200683594, 483.1622619628906, 'the results achieved by classiﬁers on each text representation scheme.\n', 6, 0)
(102.76200103759766, 508.947509765625, 503.8241271972656, 532.851318359375, '6.2. Information Retrieval from Microblogs during Disasters (IRMiDis)\nDataset\n', 7, 0)
(102.76200866699219, 541.8681640625, 509.2678527832031, 604.582275390625, 'As discussed in the previous section 1, This task is classiﬁcation plus Ranking task.\nTable 13 shows our system results on IRMiDis dataset (Basu et al. , 2018) along with\nthe rest of teams. nDCG overall is the primary metric for the evaluation. Our system\nsubstantially outperforms rest of team in the most of the metrics which justiﬁes our\nclaim established on TRAC dataset (Kumar et al. , 2018)\n', 8, 0)
(102.76200866699219, 632.4041748046875, 204.44573974609375, 643.3132934570312, '7. Result Analysis\n', 9, 0)
(102.76200866699219, 658.3071899414062, 509.3115234375, 708.0703125, 'In this section, we will present the comprehensive result analysis and try to answer\nthe research questions which framed before the experiments were performed. As we\nlook at the table 6 7, and 8, Overall, LSTM and CNN with pre-trained fastText word\nembedding marginally outperform (around 2 % to 4%) standard machine learning\n', 10, 0)
(300.5450134277344, 727.0491943359375, 311.4541320800781, 737.9583129882812, '13\n', 11, 0)

page suivante
(117.25700378417969, 67.1914291381836, 494.639404296875, 75.16152954101562, 'Table 8.\nF1-score on TRAC Facebook and Twitter English Dataset:Using Pre-trained word vectors.\n', 0, 0)
(201.11500549316406, 83.8844223022461, 458.14263916015625, 91.85452270507812, 'Facebook Test Dataset\nTwitter Test Dataset\n', 1, 0)
(123.23500061035156, 98.1093978881836, 488.7914733886719, 106.07949829101562, 'Classiﬁer\np-Word2vec\np-Glove\np-Fasttext\np-Word2vec\np-Glove\np-Fasttext\n', 2, 0)
(123.23492431640625, 112.95442962646484, 480.5322265625, 255.41946411132812, 'NB\n0.5342\n0.5373\n0.5519\n0.4152\n0.4527\n0.4276\nLR\n0.5799\n0.6050\n0.6045\n0.4197\n0.4527\n0.4441\nKNN\n0.4981\n0.5103\n0.4819\n0.3405\n0.3959\n0.3912\nSVC\n0.5832\n0.5678\n0.6120\n0.4446\n0.4581\n0.4350\nDT\n0.4700\n0.4515\n0.4900\n0.3640\n0.3949\n0.3632\nSGD\n0.5019\n0.5521\n0.5360\n0.3692\n0.3793\n0.3852\nRF\n0.5402\n0.5338\n0.5505\n0.3394\n0.3716\n0.3687\nRidge\n0.5829\n0.5952\n0.6140\n0.4092\n0.4530\n0.4461\nAdaB\n0.5713\n0.5781\n0.5907\n0.4241\n0.4261\n0.4033\nPerce.\n0.5114\n0.5201\n0.5660\n0.4224\n0.4118\n0.4049\nANN\n0.5025\n0.5498\n0.5722\n0.3728\n0.3722\n0.4842\nEnsemble\n0.5300\n0.5500\n0.5558\n0.3728\n0.3722\n0.4500\nLSTM\n0.4979\n0.4979\n0.6178\n0.5537\n0.5518\n0.5541\nBLSTM\n0.5501\n0.6062\n0.6000\n0.5359\n0.5466\n0.5423\nCNN\n0.4749\n0.5405\n0.6407\n0.5226\n0.5667\n0.5520\nNCNN\n0.5169\n0.5883\n0.5600\n0.5384\n0.5067\n0.5407\n', 3, 0)
(101.96600341796875, 290.2444152832031, 352.478515625, 298.2145080566406, 'Table 9.\nF1-score on TRAC Facebook Code-mixed Hindi Dataset\n', 4, 0)
(107.94300079345703, 305.3874206542969, 506.7911682128906, 322.32452392578125, 'Classiﬁer\nCount-\nvector\nTF/IDF W2Vec\nGlove\nFasttext\np-\nfastText\ndoc2vec-\ndmc\ndoc2vec-\ndbow\n', 5, 0)
(107.94296264648438, 329.19842529296875, 493.5299377441406, 471.66461181640625, 'NB\n0.5535\n0.6031\n0.3001\n0.372\n0.2959\n0.3176\n0.3459\n0.4736\nLR\n0.5855\n0.6134\n0.5779\n0.464\n0.5457\n0.5518\n0.3894\n0.4380\nKNN\n0.3340\n0.1721\n0.4998\n0.425\n0.5106\n0.4909\n0.3768\n0.4038\nSVC\n0.5556\n0.5862\n0.4806\n0.373\n0.5186\n0.5442\n0.3879\n0.4344\nDT\n0.5307\n0.5025\n0.4629\n0.388\n0.4392\n0.4288\n0.3485\n0.3485\nSGD\n0.5533\n0.5922\n0.3912\n0.393\n0.3670\n0.4746\n0.3331\n0.4134\nRF\n0.5473\n0.5473\n0.5374\n0.440\n0.5047\n0.4788\n0.3512\n0.3477\nRidge\n0.5780\n0.5850\n0.5293\n0.381\n0.5092\n0.5544\n0.3866\n0.4292\nAdaB\n0.5373\n0.5233\n0.5342\n0.479\n0.5336\n0.4913\n0.3751\n0.4214\nPerce.\n0.5213\n0.5598\n0.4232\n0.364\n0.3763\n0.4873\n0.2661\n0.3282\nANN\n0.5703\n0.5350\n0.5455\n0.5037\n0.5842\n0.5190\n0.4091\n0.4440\nEnsemble\n0.5700\n0.6087\n0.5558\n0.4067\n0.534\n0.5612\n0.4980\n0.4401\nLSTM\n0.5649\n0.590\n0.6021\n0.5916\nBLSTM\n0.5759\n0.527\n0.5770\n0.5900\nCNN\n0.5515\n0.566\n0.5950\n0.6081\nNCNN\n0.5919\n0.573\n0.5912\n0.5965\n', 6, 0)
(101.96600341796875, 506.4884338378906, 350.90478515625, 514.4585571289062, 'Table 10.\nF1-score on TRAC Twitter Code-mixed Hindi Dataset\n', 7, 0)
(107.94300079345703, 521.6323852539062, 506.7911376953125, 538.5685424804688, 'Classiﬁer\nCount-\nvector\nTF/IDF W2Vec\nGlove\nFasttext\np-fastText\ndoc2vec-\ndmc\ndoc2vec-\ndbow\n', 8, 0)
(107.94297790527344, 545.4423828125, 493.5252990722656, 687.9085693359375, 'NB\n0.2970\n0.2902\n0.3215\n0.273\n0.3359\n0.2897\n0.3270\n0.3205\nLR\n0.3787\n0.3724\n0.2819\n0.279\n0.3184\n0.3524\n0.2438\n0.2833\nKNN\n0.2527\n0.2553\n0.3704\n0.334\n0.3381\n0.2917\n0.3051\n0.3299\nSVC\n0.3781\n0.3886\n0.2821\n0.261\n0.3087\n0.3472\n0.2580\n0.2905\nDT\n0.3685\n0.3936\n0.3572\n0.326\n0.3475\n0.3473\n0.2988\n0.2988\nSGD\n0.3996\n0.3993\n0.2822\n0.287\n0.2739\n0.3163\n0.2605\n0.2588\nRF\n0.3585\n0.3737\n0.3286\n0.344\n0.3449\n0.3288\n0.2981\n0.2988\nRidge\n0.3616\n0.3872\n0.2811\n0.242\n0.3346\n0.3361\n0.2549\n0.2875\nAdaB\n0.1886\n0.1903\n0.3256\n0.362\n0.3261\n0.3441\n0.2614\n0.2933\nPerce.\n0.3931\n0.3868\n0.2802\n0.329\n0.2787\n0.3835\n0.2616\n0.2752\nANN\n0.430\n0.44\n0.3163\n0.3552\n0.2399\n0.3593\n0.2419\n0.3132\nEnsemble\n0.4400\n0.4600\n0.4500\n0.3938\n0.3426\n0.3555\n0.3230\n0.3281\nLSTM\n0.3840\n0.376\n0.3667\n0.4600\nBLSTM\n0.2846\n0.318\n0.3005\n0.4600\nCNN\n0.3323\n0.317\n0.2669\n0.4992\nNCNN\n0.3338\n0.380\n0.3494\n0.4600\n', 9, 0)
(300.5450134277344, 727.0491943359375, 311.4541320800781, 737.9583129882812, '14\n', 10, 0)

page suivante
(180.66799926757812, 60.50442123413086, 431.27001953125, 78.43753051757812, 'Table 11.\nF1 score on TRAC Facebook English Test Dataset using\nTransfer Learning methods\n', 0, 0)
(186.64599609375, 87.1604232788086, 370.15753173828125, 95.13052368164062, 'Transfer learning Model\nEnglish\n', 1, 0)
(287.5790100097656, 101.3853988647461, 425.3103332519531, 109.35549926757812, 'Facebook Dataset\nTwitter Dataset\n', 2, 0)
(186.64598083496094, 116.2293930053711, 407.2664489746094, 133.16647338867188, 'ELMO\n0.3699\n0.3854\nULMFiT\n0.4725\n0.4664\n', 3, 0)
(120.20700073242188, 154.61740112304688, 398.2127380371094, 162.58749389648438, 'Table 12.\nweighted F1-score TRAC Test Dataset: comparison with peers\n', 4, 0)
(126.18399810791016, 171.31045532226562, 433.3516540527344, 179.28054809570312, 'System\nEnglish\nHindi\n', 5, 0)
(223.1790008544922, 185.53543090820312, 457.7356872558594, 202.47250366210938, 'Facebook\nDataset\nTwitter\nDataset\nFacebook\nDataset\nTwitter\nDataset\n', 6, 0)
(126.18399047851562, 209.34640502929688, 456.2532653808594, 280.0804748535156, 'Our system result\n0.6407\n0.5541\n0.6081\n0.4992\nDA-LD-hildesheim\n0.6178\n0.552\n0.6081\n0.4992\nsaroyehun\n0.6425\n0.5920\nNA\nNA\nEBSI-LIA-UNAM\n0.6315\n0.5715\nNA\nNA\nTakeLab\n0.5920\n0.5651\nNA\nNA\ntaraka rama\n0.6008\n0.5656\n0.6420\n0.40\nvista.ue\n0.5812\n0.6008\n0.5951\n0.4829\nna14\n0.5920\n0.5663\n0.6450\n0.4853\n', 7, 0)
(187.031005859375, 588.8494262695312, 424.9109802246094, 596.819580078125, 'Figure 1. Heatmap on English Facebook Test Dataset Results.\n', 8, 0)
(135.50399780273438, 610.2564086914062, 445.3023986816406, 618.2265625, 'Table 13.\nResults Comparison with rest of team on FIRE 2018 IRMiDis Dataset.\n', 9, 0)
(141.48199462890625, 626.9494018554688, 470.5263366699219, 634.9195556640625, 'System\np@100\nR@1000\nMAP@100\nMAP\nnDCG @100\nnDCG\n', 10, 0)
(141.48199462890625, 641.7933959960938, 466.4174499511719, 694.5955810546875, 'Our System\n0.4\n0.2002\n0.0129\n0.1471\n0.4021\n0.7492\nMIDAS-semiauto\n0.9600\n0.1148\n0.0740\n0.1345\n0.6007\n0.6899\nMIDAS-1\n0.8800\n0.1292\n0.0581\n0.1329\n0.5649\n0.6835\nFAST NU Run2\n0.7000\n0.0885\n0.0396\n0.0801\n0.5723\n0.6676\nUEM DataMining\n0.6800\n0.1427\n0.0378\n0.1178\n0.5332\n0.6396\niitbhu irlab2\n0.3900\n0.0447\n0.0144\n0.0401\n0.3272\n0.6200\n', 11, 0)
(300.5450134277344, 727.0491943359375, 311.4541320800781, 737.9583129882812, '15\n', 12, 0)

page suivante
(190.25999450683594, 344.6454162597656, 421.67620849609375, 352.6155090332031, 'Figure 2. Heatmap on English Twitter Test Dataset Results.\n', 0, 0)
(155.406005859375, 363.31341552734375, 456.5217590332031, 381.2455139160156, 'Table 14.\nResults Comparison with CNN model and Logistic Regression TRAC\nFacebook English Test Dataset\n', 1, 0)
(161.38299560546875, 389.96844482421875, 450.5780944824219, 397.93853759765625, 'Class\nCNN model\nLogistic Regression\n#Posts\n', 2, 0)
(197.57400512695312, 404.1944274902344, 411.85015869140625, 412.1645202636719, 'P\nR\nWeighted F1\nP\nR\nWeighted F1\n', 3, 0)
(161.38299560546875, 419.0384216308594, 436.47882080078125, 444.9415283203125, 'NAG\n0.86\n0.64\n0.73\n0.83\n0.60\n0.70\n630\nCAG\n0.28\n0.46\n0.35\n0.23\n0.54\n0.32\n142\nOAG\n0.42\n0.61\n0.50\n0.46\n0.39\n0.42\n144\n', 4, 0)
(161.38299560546875, 447.8304443359375, 436.4707946777344, 455.800537109375, 'overall\n0.70\n0.61\n0.64\n0.68\n0.56\n0.60\n916\n', 5, 0)
(102.76199340820312, 485.32916259765625, 509.3223571777344, 612.80126953125, 'classiﬁers and ensemble of classiﬁer with respect to weighted F1- score on Facebook\nEnglish corpus and substantially outperforms on Twitter English corpus. By and large\nsimilar results observed on code-mixed Hindi corpus as shown in table 9 and 10.\nTable 14 present the detailed comparative results of two classiﬁers: CNN model\nwith fastText pre-trained vector and the logistic regression with TF/IDF weighting\non TRAC Facebook English dataset. The CNN Model classify Facebook posts better\nthan logistic regression at the individual class level and overall. It has been quite\nevident that posts belong to CAG class are hard to classify and Malmasi, et al. (2017)\nreported that the same observation. Table 15 show posts which are miss-classiﬁed by\nlogistic regression however, CNN model correctly classiﬁed them into the CAG class.\n', 6, 0)
(102.76199340820312, 638.3375244140625, 222.1947784423828, 649.290283203125, '7.1. Signiﬁcance Test\n', 7, 0)
(102.76199340820312, 658.30712890625, 509.26763916015625, 708.0702514648438, 'To support our claim drawn in the previous section, signiﬁcance tests, like Wilcoxon\nsigned-rank test and Student t-test were carried out by comparing Weighted F1 score\nof each classiﬁer for each text representation scheme with fastText pre-trained vector\nscheme. Table 16 and 17 summarizes the p-values of statistical signiﬁcance tests on\n', 8, 0)
(300.54498291015625, 727.0491333007812, 311.4541015625, 737.958251953125, '16\n', 9, 0)

page suivante
(128.7100067138672, 57.32845687866211, 290.2247619628906, 65.29855346679688, 'Table 15.\nsample post for the CAG class.\n', 0, 0)
(134.68800354003906, 74.02144622802734, 471.8072814941406, 81.99154663085938, 'no\nPost text\nGold Label\nCNN\nLR\n', 1, 0)
(134.68800354003906, 88.86544036865234, 479.4037170410156, 105.80252075195312, '1\nMauni singh trying very hard to convince himself what\nis written in script... body language says it all\nCAG\nCAG\nNAG\n', 2, 0)
(134.68800354003906, 106.79842376708984, 362.01885986328125, 150.63449096679688, '2\nIndian govt is all Abt giving money to Bangladesh on\nthe terms of Bangladesh ll give that all projects to\namabani n adani for thier beneﬁts lol who cares Abt\nsoldiers or India they r just puppets of thier owners\nfeku or pappu\n', 3, 0)
(374.031005859375, 106.7983627319336, 479.4037170410156, 114.76846313476562, 'CAG\nCAG\nNAG\n', 4, 0)
(134.68800354003906, 151.63034057617188, 362.01904296875, 186.49942016601562, '3\nWhen asked to speak in Parliament ran away. Speaks\nonly in TV,radio or in election rally. Can we expect\nAnother crying drama after Demonetisation disaster ?\n#cryBaby”\n', 5, 0)
(374.031005859375, 151.63034057617188, 479.4037170410156, 159.60043334960938, 'CAG\nCAG\nNAG\n', 6, 0)
(120.20700073242188, 205.56344604492188, 452.09136962890625, 214.1424102783203, 'Table 16.\np-values of Signiﬁcance test on F1-score on TRAC Facebook English Dataset\n', 7, 0)
(126.18399810791016, 222.25643920898438, 452.34442138671875, 230.22653198242188, 'Text Rep. scheme\nFacebook English\nTwitter English\n', 8, 0)
(223.1790008544922, 236.48245239257812, 451.713623046875, 244.45254516601562, 'Wilcoxon\nT-test\nWilcoxon\nT-test\n', 9, 0)
(126.18399810791016, 251.32644653320312, 461.0316467285156, 331.0275573730469, 'Count Vector\n0.001\n0.12\n0.004\n0.016\nTF/IDF\n0.0001\n0.1465\n0.0061\n0.0391\nWord2vec\n0.00002\n0.0001\n0.69\n0.30\nGlove\n0.00001\n0.000002\n0.01\n0.0009\nfastText\n0.00003\n0.0004\n0.12\n0.08\ndoc2vec-dmc\n0.000009\n0.0001\n0.0004\n0.000004\ndoc2vec-dbow\n0.000009\n0.00003\n0.0004\n0.000006\np-Word2vec\n0.00001\n0.0006\n0.02\n0.01\nP-Glove\n0.0002\n0.02\n0.08\n0.30\n', 10, 0)
(102.76200103759766, 361.2621765136719, 509.28948974609375, 449.87921142578125, 'English and Hindi Dataset respectively. In Wilcoxon signed-rank test, p-values of the\nresults is less than 0.05 for Facebook English dataset and Twitter Hindi Dataset.\nHowever, On Twitter English dataset and Facebook Hindi Dataset, some of the p-\nvalues are higher than 0.05. In student t-test, we get mixed bag results. By and large,\nour results are statistically signiﬁcant.\nIn the following subsection, we will try to answer all the research questions framed\nduring the experiments were planned.\n', 11, 0)
(102.76200103759766, 475.76849365234375, 506.453125, 486.7212219238281, '7.2.\nBest Text Representation scheme to model the text from Social web\n', 12, 0)
(102.76200103759766, 495.73712158203125, 509.3004455566406, 571.4031982421875, 'Text Representation is the primary task for to address any NLP task like Ques-\ntion/answering, classiﬁcation etc. As dicusses in section 4, There are basically two\ntext representing scheme:Bag-of-Word(BoW) with countvector, TF/IDF weighting\nand word embedding. Word2Vec (Mikolov et.al , 2013), Glove (Pennington et.al. ,\n2014), and fastText (Mikolov et al. , 2018), an extension of Word2vec are popular\nword embedding techniques.\n', 13, 0)
(120.20700073242188, 590.2402954101562, 470.90081787109375, 598.8193359375, 'Table 17.\np-values of Signiﬁcance test on F1-score on TRAC code-mixed Hindi Test Dataset\n', 14, 0)
(126.18399810791016, 606.9334106445312, 472.34136962890625, 614.903564453125, 'Text Rep. scheme\nFacebook Code-mixed Hindi\nTwitter Code-mixed Hindi\n', 15, 0)
(223.1790008544922, 621.1583862304688, 451.713623046875, 629.1285400390625, 'Wilcoxon\nT-test\nWilcoxon\nT-test\n', 16, 0)
(126.18399047851562, 636.00341796875, 452.5754089355469, 697.7715454101562, 'Count Vector\n0.003\n0.05\n0.01\n0.20\nTF/IDF\n0.003\n0.14\n0.007\n0.12\nWord2vec\n0.40\n0.17\n0.043\n0.017\nGlove\n0.001\n0.0005\n0.015\n0.004\nfastText\n0.35\n0.15\n0.022\n0.006\ndoc2vec-dmc\n0.005\n0.00001\n0.001\n0.0008\ndoc2vec-dbow\n0.003\n0.002\n0.001\n0.002\n', 17, 0)
(300.5450134277344, 727.0491943359375, 311.4541320800781, 737.9583129882812, '17\n', 18, 0)

page suivante
(102.76200103759766, 60.54815673828125, 509.3115539550781, 265.7293395996094, 'Results clearly show that models with fastText pre-trained vector outperform Glove\npre-trained vector on Facebook test dataset as well as the Twitter test dataset. the\nmain reason behind the outperformance of fastText over Glove and Word2vec is that\nThe fastText consider each word as N-gram characters. A word vector for a word\nis computed from the sum of the n-gram characters. Glove and Word2vec consider\neach word as a single unit and provide a word vector for each word. Since Facebook\nusers make a lot of mistakes in spelling, typos, fastText is more convenient than Glove\n(Majumder et al., 2018). from Figure 1 and 2 shows that BoW is still eﬀective text\nrepresentation scheme for the standard machine learning classier which takes hand-\ncrafted feature and n-grams as inputs. Logistic Regression and Support Vector perform\nbetter than other classiﬁers in English as well as Hindi Dataset. Adaboost performs\nbetter than LR and SVC on Facebook English Dataset but substantially underperform\nthem on rest of three Datsets. Our participation (Majumder et al., 2018) in TRAC\ncompetition (Kumar Ritesh et al., 2018) FIRE Information Retrieval from Microblogs\nduring Disasters (Basu et al. , 2018) track where our team performed well and secured\ntop position.\n', 0, 0)
(102.76200103759766, 291.6376037597656, 484.3403625488281, 302.59033203125, '7.3. Transfer Learning Model vs Pre-trained Word Embedding Model\n', 1, 0)
(102.76200103759766, 311.60723876953125, 509.3113708496094, 581.5442504882812, 'Transfer learning is focused on storing knowledge gained while solving one problem and\napplying it to a diﬀerent but related problem. On many occasion, NLP researchers face\nthe problem of unavailability of suﬃcient labeled data to train the model. With the\nadvent of new transfer learning method like ELMO (Peters et al. , 2018) and Universal\nlanguage model ﬁne-tuning for Text Classiﬁcation (ULMFiT) (Howaard et al., 2018)\nattract interest among NLP Researchers. These models are trained or large text corpus.\nHowaard et al. (2018) claimed that these model can be ﬁne-tuned on the task-speciﬁc\ncorpus. We have used these transfer learning model on TRAC English dataset Kumar\net al. (2018) and results are presented in table 11. one can observe that results are\nsubstantially lower than the results reported in Table 6, 7, and 8 where pre-trained\nword vectors are used to initialize the ﬁrst layer of deep neural model and rest of the\nnetwork is trained from scratch achieve better results than transfer learning model.\nHowaard et al. (2018) termed use of pre-trained vector as shallow representation.\nIn these experiments, we trained diﬀerent classiﬁer models on Facebook posts. Table\n7 10 shows the results on Twitter dataset Kumar et al.\n(2018). There is lexical\ndiﬀerence between Facebook and Twitter posts. From the results shown in Table 7 8\nand table 10, one can conclude that weighted F1 score of standard machine learning\nclassiﬁers are substantially lower in Twitter Dataset as compare to Facebook Dataset.\nWhile deep learning models perform better than machine learning classiﬁers for the\nTwitter Dataset. Thus, Deep learning models are more robust than machine learning\nclassiﬁer across diverse datasets.\n', 2, 0)
(102.76202392578125, 607.4534912109375, 332.67132568359375, 618.40625, '7.4. Does Deeper Neural Net make Sense\n', 3, 0)
(102.76202392578125, 627.422119140625, 509.2786865234375, 690.13720703125, 'To answer this question, we designed ﬁrst CNN model with one convolution layer and\nother CNN model with 3 convolution layer with diﬀerent ﬁlters height. As we look\nat results shown in Table 6,7,8, and 9, one can conclude that by and large weighted\nF1 score lower for CNN model with multiple convolution layer than CNN model with\nsingle convolution layer.\n', 4, 0)
(300.5450439453125, 727.0491333007812, 311.45416259765625, 737.958251953125, '18\n', 5, 0)

page suivante
(102.76200103759766, 60.54815673828125, 178.7875213623047, 71.4572525024414, '8. Conclusion\n', 0, 0)
(102.76200103759766, 86.451171875, 509.300537109375, 382.2922668457031, 'In this Paper, Multilingual Social media stream is studied with special kind of text\nfeatures: Aggression and fact perspective. Exhaustive experiments are performed to\nbenchmark the text representation scheme on machine learning classiﬁers and deep\nneural nets. From the results, we conclude that deep Neural model with pre-trained\nword embedding is the better choice than machine earning classiﬁer and transfer learn-\ning model. Word embedding is the better text representative scheme than Bag-of-words\nfor the deep neural models. In fact, performance can be improved with the help of fast-\nText pre-trained vector. However, machine learning classiﬁers perform better in BoW\nwith TF/IDF weighting than word embedding. We also concluded that higher drop\nout will help to counter model overﬁtting and improvise a standard evaluation metrics.\nCNN and LSTM are the better models for these datasets. On the English test corpus,\nwe obtained a better weighted F1 score for NAG class and poor weighted F1 score\nfor CAG class which supports the previous (Malmasi, et al. , 2017) ﬁndings. For the\nFacebook Hindi test corpus, the same seems not to be true. We obtained a better F1\nscore for CAG class than NAG class. It is also to be noted that the model leads to\npoor result on Twitter test data since the training corpus was created from Facebook.\nIn such cases, deep neural models substantially outperform machine learning classi-\nﬁers. Signiﬁcance test conﬁrms these claims with 95 % conﬁdence interval in most the\ncases. Our work shows what kind of problems are moving into the center of attention\nfor research in machine learning. Using deep learning models, there is great potential\nto solve some of these problems, yet still, the performance is far from perfect. Model\ntransfer between problems and the application of derived knowledge in user interfaces\nare areas directions for future work.\n', 1, 0)
(102.76202392578125, 409.2411804199219, 161.66026306152344, 420.1502685546875, 'References\n', 2, 0)
(102.76202392578125, 434.8570556640625, 509.285888671875, 707.8336181640625, 'Aroyehun, Segun Taofeek and Gelbukh, Alexander (2018). Aggression detection in social media:\nUsing deep neural networks, data augmentation, and pseudolabeling.Proceedings of the First\nWorkshop on Trolling, Aggression and Cyberbullying (TRAC-2018), pp.90–97.\nArroyo-Fern´andez, Ignacio and Forest, Dominic and Torres-Moreno, Juan-Manuel and\nCarrasco-Ruiz, Mauricio and Legeleux, Thomas and Joannette,Karen (2018). Cyberbullying\nDetection Task: the EBSI-LIA-UNAM System (ELU) at COLING’18 TRAC-1.Proceedings\nof the First Workshop on Trolling,Aggression and Cyberbullying (TRAC-2018), pp 140–149.\nBasu, Moumitaand Ghosh, Saptarshi and Ghosh, Kripabandhu (2018). Overview of the FIRE\n2018 track: Information Retrieval from Microblogs during Disasters (IRMiDis). Proceedings\nof FIRE 2018 - Forum for Information Retrieval Evaluation, Gujrat, India, December .\nBaziotis, Christos and Pelekis, Nikos and Doulkeridis, Christos (2017). Datastories at semeval-\n2017 task 4: Deep lstm with attention for message-level and topic-based sentiment analysis.\nProceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017),\npp.747–754\nBojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas (2017) En-\nriching word vectors with subword information, Transactions of the Association for Compu-\ntational Linguistics, vol-5,pp.135–146, MIT Press.\nBurnap, Pete and Williams, Matthew Ltitle (2015). Cyber hate speech on twitter: An applica-\ntion of machine classiﬁcation and statistical modeling for policy and decision makingPolicy\n& Internet, vol-7 number 2, pp.223–242, Wiley Online Library.\nConover, Michael and Ratkiewicz, Jacob and Francisco, Matthew R and Gon¸calves , Bruno and\nMenczer, Filippo and Flammini, Alessandro (2011). Political polarization on twitter.,Icwsm,\nvol-133, pp.89–96.\n', 3, 0)
(300.5450439453125, 727.0491333007812, 311.45416259765625, 737.958251953125, '19\n', 4, 0)

page suivante
(102.76187133789062, 61.25802993774414, 509.3455810546875, 704.8446044921875, 'Conover, Michael D and Gon¸calves, Bruno and Ratkiewicz, Jacob and Flammini, Alessandro\nand Menczer, Filippo (2011). Predicting the political alignment of twitter users, Privacy,\nSecurity, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social\nComputing (SocialCom), 2011 IEEE Third International Conference on, pp.192–199.\nDavidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar (2017). Au-\ntomated Hate Speech Detection and the Problem of Oﬀensive Langemuage. Proceedings of\nICWSM.\nDeriu, Jan and Gonzenbach, Maurice and Uzdilli, Fatih and Lucchi, Aurelien and Luca, Valeria\nDe and Jaggi, Martin (2016). Swisscheese at semeval-2016 task 4: Sentiment classiﬁcation\nusing an ensemble of convolutional neural networks with distant supervision. Proceedings of\nthe 10th international workshop on semantic evaluation,pp.1124–1128.\nHltcoe, J (2013).Semeval-2013 task 2: Sentiment analysis in Twitter,vol-312 Atlanta, Georgia,\nUSA.\nHoward, Jeremy & Ruder, Sebastian 2018. Universal language model ﬁne-tuning for text clas-\nsiﬁcation”,arXiv preprint arXiv:1801.06146,\nHarris, Zellig S (1954),Distributional structure Word,10, number=2-3,pp.146–162, 1954, Taylor\n& Francis.\nKumar, Ritesh and Reganti, Aishwarya N. and Bhatia, Akshit and Maheshwari,Tushar (2018),\nAggression-annotated Corpus of Hindi-English Code-mixed Data, Proceedings of the 11th\nLanguage Resources and Evaluation Conference (LREC), Miyazaki, Japan.\nKumar, Ritesh and Ojha, Atul Kr. and Malmasi, Shervin and Zampieri Marcos (2018). Bench-\nmarking Aggression Identiﬁcation in Social Media, Proceedings of the First Workshop on\nTrolling, Aggression and Cyberbulling (TRAC), Santa Fe, USA\nKwok, Irene and Wang, Yuzhou (2013),Locate the hate: Detecting Tweets Against Blacks\n,Twenty-Seventh AAAI Conference on Artiﬁcial Intelligence.\nLau, Jey Han and Baldwin, Timothy (2016). An empirical evaluation of doc2vec with practical\ninsights into document embedding generation. arXiv preprint arXiv:1607.05368.\nLe, Quoc and Mikolov, Tomas (2014) Distributed representations of sentences and docu-\nments.International Conference on Machine Learning, pp.1188–1196\nMajumder, Prasenjit and Mandl, Thomas and Modha Sandip (2018)Filtering Aggression from\nthe Multilingual Social Media Feed Proceedings of the First Workshop on Trolling, Aggres-\nsion and Cyberbullying (TRAC-2018), pp. 199–207\nMalmasi, Shervin and Zampieri, Marcos (2017)Detecting Hate Speech in Social Media Pro-\nceedings of the International Conference Recent Advances in Natural Language Processing\n(RANLP), pp.467–472.\nMalmasi, Shervin and Zampieri, Marcos (2018).Challenges in Discriminating Profanity from\nHate Speech Journal of Experimental & Theoretical Artiﬁcial Intelligence pp.1–16, vol-30,\nissue-2,Taylor & Francis.\nMaynard, Diana and Funk, Adam (2011) Automatic detection of political opinionsin tweets,\nExtended Semantic Web Conference,pp. 88–99, Springer.\nMikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeﬀrey (2013). Eﬃcient estima-\ntion of word representations in vector space,arXiv preprint arXiv:1301.3781.\nMikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin,\nArmand (2018). Advances in Pre-Training Distributed Word Representations,Proceedings of\nthe International Conference on Language Resources and Evaluation (LREC 2018).\nModha, Sandip and Agrawal, Krati and Verma, Deepali and Majumder, Prasenjit and Man-\ndalia, Chintak 2016. DAIICT at TREC RTS 2016: Live Push Notiﬁcation and Email Di-\ngest.,TREC.\nMohammad, Saif M and Kiritchenko, Svetlana and Zhu, Xiaodan (2013). NRC-Canada: Build-\ning the state-of-the-art in sentiment analysis of tweets.arXiv preprint arXiv:1308.6242.\nNakov, Preslav and Ritter, Alan and Rosenthal, Sara and Sebastiani, Fabrizio and Stoyanov,\nVeselin (2016). SemEval-2016 task 4: Sentiment analysis in Twitter,Proceedings of the 10th\ninternational workshop on semantic evaluation (semeval-2016), pp. 1–18.\nPennington, Jeﬀrey and Socher, Richard and Manning, Christopher Glove: Global vectors for\n', 0, 0)
(300.54486083984375, 727.0491333007812, 311.4539794921875, 737.958251953125, '20\n', 1, 0)

page suivante
(102.76195526123047, 61.25802993774414, 509.34576416015625, 549.4276123046875, 'word representation Proceedings of the 2014 conference on empirical methods in natural\nlanguage processing (EMNLP), pp.1532–1543.\nPeters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark,\nChristopher and Lee, Kenton and Zettlemoyer, Luke (2018). Deep contextualized word rep-\nresentations arXiv preprint arXiv:1802.05365.\nRazavi, Amir H and Inkpen, Diana and Uritsky, Sasha and Matwin, Stan (2010) Oﬀensive\nlanguage detection using multi-level classiﬁcation, Canadian Conference on Artiﬁcial Intel-\nligence pp.16–27, Springer\nRosenthal, Sara and Nakov, Preslav and Kiritchenko, Svetlana and Mohammad, Saif and Rit-\nter, Alan and Stoyanov, Veselin (2015). Semeval-2015 task 10: Sentiment analysis in twit-\nter Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015),\npp.451–463.\nRosenthal, Sara and Farra, Noura and Nakov, Preslav (2017). SemEval-2017 task 4: Sentiment\nanalysis in Twitter Proceedings of the 11th International Workshop on Semantic Evaluation\n(SemEval-2017), pp.502–518.\nSchmidt, Anna and Wiegand, Michael (2017).A Survey on Hate Speech Detection Using Natural\nLanguage Processing,Proceedings of the Fifth International Workshop on Natural Language\nProcessing for Social Media. Association for Computational Linguistics Valencia, Spain,\npp.1–10,\nSeveryn, Aliaksei and Moschitti, Alessandro (2015) Unitn: Training deep convolutional neural\nnetwork for twitter sentiment classiﬁcation, Proceedings of the 9th international workshop\non semantic evaluation (SemEval 2015), pp. 464–469.\nTumasjan, Andranik and Sprenger, Timm Oliver and Sandner, Philipp G and Welpe, Isabell\nM (2010). Predicting elections with twitter: What 140 characters reveal about political sen-\ntiment.,Icwsm, vol-10, number-1, pp. 178–185.\nWarner, William and Hirschberg, Julia (2012) Detecting hate speech on the world wide web,\nProceedings of the Second Workshop on Language in Social Media,pp 19–26,Association for\nComputational Linguistics.\nXu, Jun-Ming and Jun, Kwang-Sung and Zhu, Xiaojin and Bellmore, Amy (2012). Learning\nfrom bullying traces in social media, Proceedings of the 2012 conference of the North Amer-\nican chapter of the association for computational linguistics: Human language technologies,\npp.656–666, Association for Computational Linguistics\nZhang, Ye and Wallace, Byron (2015). A Sensitivity Analysis of (and Practitioners’ Guide to)\nConvolutional Neural Networks for Sentence Classiﬁcation,arXiv preprint arXiv:1510.03820.\nMajumder, Prasenjit and Mitra, Mandar and Pal, Dipasree and Bandyopadhyay, Ayan and\nMaiti, Samaresh and Mitra, Sukanya and Sen, Aparajita and Pal, Sukomal.Text collections\nfor FIRE, Proceedings of the 31st annual international ACM SIGIR conference on research\nand development in information retrieval,pp.699–700,ACM\nMajumdar, P and Mitra, Mandar and Parui, Swapan K and Bhattacharya, Initiative for indian\nlanguage ir evaluation, The First International Workshop on Evaluating Information Access\n(EVIA)\n', 0, 0)
(300.54498291015625, 727.0491333007812, 311.4541015625, 737.958251953125, '21\n', 1, 0)

page suivante
