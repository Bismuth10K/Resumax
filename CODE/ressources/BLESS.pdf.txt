(70.15640258789062, 754.6322021484375, 525.1163940429688, 779.9996948242188, 'Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 1–10,\nEdinburgh, Scotland, UK, July 31, 2011. c⃝2011 Association for Computational Linguistics\n', 0, 0)
(144.3857879638672, 60.1028938293457, 467.6201477050781, 78.76729583740234, 'How we BLESSed distributional semantic evaluation\n', 1, 0)
(168.1887969970703, 113.61126708984375, 239.81239318847656, 129.16497802734375, 'Marco Baroni\n', 2, 0)
(155.30780029296875, 127.9896469116211, 252.6948699951172, 142.39566040039062, 'University of Trento\n', 3, 0)
(174.32879638671875, 141.93667602539062, 233.67440795898438, 156.34268188476562, 'Trento, Italy\n', 4, 0)
(128.68280029296875, 157.23561096191406, 279.3184509277344, 169.76466369628906, 'marco.baroni@unitn.it\n', 5, 0)
(363.7727966308594, 113.61126708984375, 452.2293395996094, 142.39566040039062, 'Alessandro Lenci\nUniversity of Pisa\n', 6, 0)
(384.0907897949219, 141.93667602539062, 431.9115905761719, 156.34268188476562, 'Pisa, Italy\n', 7, 0)
(300.40478515625, 157.23561096191406, 515.5985717773438, 169.76466369628906, 'alessandro.lenci@ling.unipi.it\n', 8, 0)
(163.15879821777344, 238.72528076171875, 207.6440887451172, 254.27899169921875, 'Abstract\n', 9, 0)
(93.81880187988281, 265.35882568359375, 276.9812316894531, 420.8267822265625, 'We introduce BLESS, a data set speciﬁcally\ndesigned for the evaluation of distributional\nsemantic models. BLESS contains a set of tu-\nples instantiating different, explicitly typed se-\nmantic relations, plus a number of controlled\nrandom tuples. It is thus possible to assess the\nability of a model to detect truly related word\npairs, as well as to perform in-depth analy-\nses of the types of semantic relations that a\nmodel favors. We discuss the motivations for\nBLESS, describe its construction and struc-\nture, and present examples of its usage in the\nevaluation of distributional semantic models.\n', 10, 0)
(72.00080108642578, 440.26727294921875, 154.81446838378906, 455.82098388671875, '1\nIntroduction\n', 11, 0)
(72.00080108642578, 462.57525634765625, 298.8053283691406, 706.0567016601562, 'In NLP, it is customary to distinguish between in-\ntrinsic evaluations, testing a system in itself, and\nextrinsic evaluations, measuring its performance in\nsome task or application (Sparck Jones and Galliers,\n1996). For instance, the intrinsic evaluation of a de-\npendency parser will measure its accuracy in identi-\nfying speciﬁc syntactic relations, while its extrinsic\nevaluation will focus on the impact of the parser on\ntasks such as question answering or machine trans-\nlation. Current approaches to the evaluation of Dis-\ntributional Semantic Models (DSMs, also known\nas semantic spaces, vector-space models, etc.; see\nTurney and Pantel (2010) for a survey) are task-\noriented. Model performance is evaluated in “se-\nmantic tasks”, such as detecting synonyms, recog-\nnizing analogies, modeling verb selectional prefer-\nences, ranking paraphrases, etc. Measuring the per-\nformance of DSMs on such tasks represents an in-\n', 12, 0)
(313.2008056640625, 240.1222686767578, 540.0064086914062, 348.1117248535156, 'direct test of their ability to capture lexical mean-\ning. The task-oriented benchmarks adopted in dis-\ntributional semantics have not speciﬁcally been de-\nsigned to evaluate DSMs. For instance, the widely\nused TOEFL synonym detection task was designed\nto test the learners’ proﬁciency in English as a sec-\nond language, and not to investigate the structure of\ntheir semantic representations (cf. Section 2).\n', 13, 0)
(324.10980224609375, 348.7512512207031, 540.000732421875, 361.896728515625, 'To gain a real insight into the abilities of DSMs to\n', 14, 0)
(313.2008056640625, 362.3002624511719, 540.0060424804688, 605.78173828125, 'address lexical semantics, existing benchmarks must\nbe complemented with a more intrinsically oriented\napproach, to perform direct tests on the speciﬁc as-\npects of lexical knowledge captured by the models.\nIn order to achieve this goal, three conditions must\nbe met: (i) to single out the particular aspects of\nmeaning that we want to focus on in the evaluation\nof DSMs; (ii) to design a data set that is able to ex-\nplicitly and reliably encode the target semantic infor-\nmation; (iii) to specify the evaluation criteria of the\nsystem performance on the data set, in order to get\nan estimate of the intrinsic ability of DSMs to cope\nwith the selected semantic aspects. In this paper, we\naddress these three conditions by presenting BLESS\n(Baroni and Lenci Evaluation of Semantic Spaces),\na new data set speciﬁcally geared towards the in-\ntrinsic evaluation of DSMs, downloadable from:\nhttp://clic.cimec.unitn.it/distsem.\n', 15, 0)
(313.2008056640625, 615.8212890625, 521.5201416015625, 631.375, '2\nDistributional semantics benchmarks\n', 16, 0)
(313.2008056640625, 638.7142333984375, 540.0009155273438, 706.0567016601562, 'There are several benchmarks that have been widely\nadopted for the evaluation of DSMs, all of them cap-\nturing interesting challenges a DSM should meet.\nWe brieﬂy review here some commonly used and\nrepresentative benchmarks, and discuss why we felt\n', 17, 0)
(293.2098388671875, 724.5390014648438, 298.6643981933594, 737.6844482421875, '1\n', 18, 0)

page suivante
(72.00080108642578, 55.87024688720703, 298.801025390625, 163.85971069335938, 'the need to add BLESS to the set. We notice at the\noutset of this discussion that we want to carve out a\nspace for BLESS, and not to detract from the impor-\ntance and usefulness of other data sets. We further\nremark that we focus on data sets that, like BLESS,\nare monolingual English and, while task-oriented,\nnot aimed at a speciﬁc application setting (such as\nmachine translation or ontology population).\n', 0, 0)
(82.90979766845703, 164.3782501220703, 298.80078125, 177.52371215820312, 'Probably the most commonly used benchmark in\n', 1, 0)
(72.00080108642578, 177.53453063964844, 298.8064270019531, 489.15472412109375, 'distributional semantics is the TOEFL synonym de-\ntection task introduced to computational linguis-\ntics by Landauer and Dumais (1997). It consists of\n80 multiple-choice questions, each made of a target\nword (a noun, verb, adjective or adverb) and 4 re-\nsponse words, 1 of them a synonym of the target.\nFor example, given the target levied, the matched\nwords are imposed, believed, requested, correlated,\nthe ﬁrst one being the correct choice. The task for\na system is then to pick the true synonym among\nthe responses. The TOEFL task focuses on a single\nsemantic relation, namely synonymy. Synonymy is\nactually not a common semantic relation and one of\nthe hardest to deﬁne, to the point that many lexi-\ncal semanticists have concluded that true synonymy\ndoes not exist (Cruse, 1986). Just looking at a few\nexamples of synonym pairs from the TOEFL set will\nillustrate the problem: discrepancy/difference, pro-\nliﬁc/productive, percentage/proportion, to market/to\nsell, color/hue. Moreover, the criteria adopted to\nchoose the distractors (probably motivated by the\nlanguage proﬁciency testing purposes of TOEFL)\nare not known.\nBy looking at the set, it is hard\n', 2, 0)
(72.00080108642578, 489.5582580566406, 298.8043518066406, 692.3927001953125, 'to discern a coherent pattern. In certain cases, the\ndistractors are semantically close to the target word\n(volume, sample and proﬁt for percentage), whereas\nin other cases they are not (home, trail, and song for\nannals). It it thus not clear whether we are asking the\nmodels to distinguish a semantically related word\n(the synonym) from random elements, or a more\ntightly related word (the synonym, again) from other\nrelated words. The TOEFL task, ﬁnally, is based on\na discrete choice (either you get the right word, or\nyou don’t), with the result that evaluation is “quan-\ntized”, leading to large accuracy gains for small ac-\ntual differences (one model that guesses one more\nsynonym right than another gets 1.25% more points\nin percentage accuracy).\n', 3, 0)
(82.90979766845703, 692.9112548828125, 298.8007507324219, 706.0567016601562, 'The WordSim 353 data set (Finkelstein et al.,\n', 4, 0)
(313.2008056640625, 55.47751998901367, 540.0010375976562, 163.85971069335938, '2002) is a widely used example of semantic simi-\nlarity rating set (see also Rubenstein and Goode-\nnough (1965) and Miller and Charles (1991)). Sub-\njects were asked to rate a set of 353 word pairs on a\n“similarity” scale and average ratings for each pair\nwere computed. Models are then evaluated in terms\nof correlation of their similarity scores with aver-\nage ratings across pairs.\nFrom the point of view\n', 5, 0)
(313.2008056640625, 164.2632598876953, 540.0059814453125, 502.5897216796875, 'of assessing the performance of a DSM, the Word-\nSim (and related) similarity ratings are a mixed bag,\nin two senses. First, the data set contains a vari-\nety of different semantic relations. In a recent se-\nmantic annotation of the WordSim performed by\nAgirre et al. (2009) we ﬁnd that, among the 174\npairs with above-median score (and thus presum-\nably related), there is 1 identical pair, 17 synonym\npairs, 28 hyper-/hyponym pairs, 30 coordinate pairs,\n6 holo-/meronym pairs and 92 (more than half) pairs\nthat are “topically related, but none of the above”.\nSecond, the scores are a mixture of intuitions about\nwhich of these relations are more semantically tight\nand intuitions about more or less connected pairs\nwithin each of the relations. For example, among\nthe top-rated scores we ﬁnd synonyms such as jour-\nney/voyage and coordinate concepts (king/queen).\nIf we look at the relations characterizing pairs\naround the median rating, we ﬁnd both less “per-\nfect” synonyms (monk/brother, that are synonymous\nonly under an unusual sense of brother) and less\nclose coordinates (skin/eye), as well as pairs in-\nstantiating other, less taxonomically tight relations,\nsuch as many syntagmatically connected items (fam-\nily/planning, disaster/area, bread/butter).\nAppar-\n', 6, 0)
(313.2008056640625, 502.99322509765625, 540.0009155273438, 543.2366943359375, 'ently, a single scale is merging intuitions about se-\nmantic similarity of speciﬁc pairs and semantic sim-\nilarity of different relations.\n', 7, 0)
(324.10980224609375, 543.8692626953125, 540.000732421875, 557.0147094726562, 'A perhaps more principled way to evaluate DSMs\n', 8, 0)
(313.2008056640625, 557.0265502929688, 540.003662109375, 706.0567016601562, 'that has recently gained some popularity is the con-\ncept categorization task, where a DSM has to clus-\nter a set of nouns expressing basic-level concepts\ninto gold standard categories. A particularly care-\nfully constructed example is the Almuhareb-Poesio\n(AP) set of 402 concepts introduced in Almuhareb\n(2006). Concept categorization sets also include the\nBattig (Baroni et al., 2010) and ESSLLI 2008 (Ba-\nroni et al., 2008) lists. The AP concepts must be\nclustered into 21 classes, each represented by be-\ntween 13 and 21 nouns. Examples include the ve-\n', 9, 0)
(293.2098388671875, 724.5390014648438, 298.6643981933594, 737.6844482421875, '2\n', 10, 0)

page suivante
(72.00080108642578, 55.87024688720703, 298.8038330078125, 163.85971069335938, 'hicle class (helicopter, motorcycle. . . ), the motiva-\ntion class (ethics, incitement, . . . ), and the social\nunit class (platoon, branch). The concepts are bal-\nanced in terms of frequency and ambiguity, so that,\ne.g., the tree class contains a common concept such\nas pine but also the casuarina tree, as well as the\nsamba tree, that is not only an ambiguous term, but\none where the non-arboreal sense dominates.\n', 0, 0)
(82.90979766845703, 165.44129943847656, 298.8008117675781, 178.58676147460938, 'Concept categorization data sets, while interest-\n', 1, 0)
(72.00080108642578, 178.99024963378906, 298.801025390625, 368.27471923828125, 'ing to simulate one of the basic aspects of human\ncognition, are limited to one kind of semantic re-\nlation (discovering coordinates). More importantly,\nthe quality of the results will depend not only on the\nunderlying DSMs, but also on the clustering algo-\nrithm being used (and on how this interacts with the\noverall structure of the DSM), thus making it hard\nto interpret the performance of DSMs. The forced\n“hard” category choice is also problematic, and ex-\naggerates performance differences between models\nespecially in the presence of ambiguous terms (a\nmodel that puts samba in the occasion class with\ndance and ball might be penalized as much as a\nmodel that puts it in the monetary currency class).\n', 2, 0)
(82.90979766845703, 369.8552551269531, 298.80084228515625, 383.000732421875, 'A more general issue with all benchmarks is that\n', 3, 0)
(72.00080108642578, 383.4052429199219, 298.8042297363281, 613.3377075195312, 'tasks are based on comparing a single quality score\nfor each considered model (accuracy for TOEFL,\ncorrelation for WordSim, a clustering quality mea-\nsure for AP, etc.). This gives little insight into how\nand why the models differ. Moreover, there is no\nwell-established statistical procedure to assess sig-\nniﬁcance of differences for most commonly used\nmeasures. Finally, either because the data sets were\nnot originally intended as standard benchmarks, or\neven on purpose, they all are likely to cause coverage\nproblems even for DSMs trained on very large cor-\npora. Think of the presence of extremely rare nouns\nlike casuarina in AP, of proper nouns in WordSim (it\nis not clear to us that DSMs are adequate semantic\nmodels for referring expressions – at the very least\nthey should not be mixed up lightly with common\nnouns), or multi-word expressions in other data sets.\n', 4, 0)
(72.00080108642578, 626.6382446289062, 291.85693359375, 656.1389770507812, '3\nHow we intend to BLESS distributional\nsemantic evaluation\n', 5, 0)
(72.00080108642578, 665.812255859375, 298.8009948730469, 706.0567016601562, 'DSMs measure the distributional similarity between\nwords, under the assumption that proximity in distri-\nbutional space models semantic relatedness, includ-\n', 6, 0)
(313.2008056640625, 55.87024688720703, 540.0040893554688, 245.15475463867188, 'ing, as a special case, semantic similarity (Budanit-\nsky and Hirst, 2006). However, semantically related\nwords in turn differ for the type of relation hold-\ning between them: e.g., dog is strongly related to\nboth animal and tail, but with different types of re-\nlations. Therefore, evaluating the intrinsic ability of\nDSMs to represent the semantic space of a word en-\ntails both (i) determining to what extent words close\nin semantic space are actually semantically related,\nand (ii) analyzing, among related words, which type\nof semantic relation they tend to instantiate. Two\nmodels can be equally very good in identifying se-\nmantically related words, while greatly differing for\nthe type of related pairs they favor.\n', 7, 0)
(324.10980224609375, 245.63526916503906, 540.000732421875, 258.7807312011719, 'The BLESS data set complies with both these\n', 8, 0)
(313.2008056640625, 259.18426513671875, 540.0009765625, 272.3297424316406, 'constraints.\nThe set is populated with tuples ex-\n', 9, 0)
(313.2008056640625, 272.34051513671875, 540.0050659179688, 516.2147216796875, 'pressing a relation between a target concept (hence-\nforth referred to as concept) and a relatum concept\n(henceforth referred to as relatum). For instance, in\nthe BLESS tuple coyote-hyper-animal, the concept\ncoyote is linked to the relatum animal via the hy-\npernymy relation (the relatum is a hypernym of the\nconcept). BLESS focuses on a coherent set of basic-\nlevel nominal concrete concepts and a small but ex-\nplicit set of semantic relations, each instantiated by\nmultiple relata. Depending on the type of relation,\nrelata can be nouns, verbs or adjectives. Moreover,\nBLESS also contains, for each concept, a number of\nrandom “relatum” words that are not semantically\nrelated to the concept. Thus, it also allows to evalu-\nate a model in terms of its ability to harvest related\nwords given a concept (by comparing true and ran-\ndom relata), and to identify speciﬁc types of relata,\nboth in terms of semantic relation and part of speech.\n', 10, 0)
(324.10980224609375, 516.6952514648438, 540.0008544921875, 529.8406982421875, 'A data set intending to represent a gold standard\n', 11, 0)
(313.2008056640625, 530.2442626953125, 540.0008544921875, 556.938720703125, 'for evaluation should include tests items that are as\nlittle controversial as possible.\nThe choice of re-\n', 12, 0)
(313.2008056640625, 557.3422241210938, 540.0008544921875, 624.6846923828125, 'stricting BLESS to concrete concepts is motivated\nby the fact that they are by far the most studied ones,\nand there is better agreement about the relations that\ncharacterize them (Murphy, 2002; Rogers and Mc-\nClelland, 2004).\n', 13, 0)
(324.10980224609375, 625.1652221679688, 540.0007934570312, 638.3106689453125, 'As for the types of relation to include, we are\n', 14, 0)
(313.2008056640625, 638.7142333984375, 540.0009155273438, 706.0567016601562, 'faced with a dilemma. On the one hand, there is\nwide evidence that taxonomic relations, the best un-\nderstood type, only represent a tiny portion of the\nrich spectrum covered by semantic relatedness. On\nthe other hand, most of these wider semantic rela-\n', 15, 0)
(293.2098388671875, 724.5390014648438, 298.6643981933594, 737.6844482421875, '3\n', 16, 0)

page suivante
(72.00080108642578, 55.87024688720703, 298.8009338378906, 82.56472778320312, 'tions are also highly controversial, and may easily\nlead to questionable classiﬁcations.\nFor instance,\n', 0, 0)
(72.00080108642578, 82.96826934814453, 298.801025390625, 258.7047424316406, 'concepts are related to events, but often it is not clear\nhow to distinguish the events expressing a typical\nfunction of nominal concepts (e.g., car and trans-\nport), from those events that are also strongly re-\nlated to them but without representing their typical\nfunction sensu stricto (e.g., car and ﬁx). As will be\nshown in Section 4, the BLESS data set tries to over-\ncome this dilemma by attempting a difﬁcult com-\npromise: Semantic relations are not limited to tax-\nonomic types and also include attributes and events\nstrongly related to a concept, but in these cases we\nhave resorted to underspeciﬁcation, rather than com-\nmitting ourselves to questionable granular relations.\n', 1, 0)
(82.90979766845703, 259.2222595214844, 298.8008728027344, 272.36773681640625, 'BLESS strives to capture those differences and\n', 2, 0)
(72.00080108642578, 272.771240234375, 298.8047180175781, 651.7457275390625, 'similarities among DSMs that do not depend on\ncoverage, processing choices or lexical preferences.\nBLESS has been constructed using a publicly avail-\nable collection of corpora for reference (see Section\n4.4 below), which means that anybody can train a\nDSM on the same data and be sure to have perfect\ncoverage (but this is not strictly necessary). For each\nconcept and relation, we pick a variety of relata (see\nnext section) in order to abstract away from inciden-\ntal gaps of models or different lexical/topical prefer-\nences. For example, the concept robin has 7 hyper-\nnyms including the very general and non-technical\nanimal and bird and the more speciﬁc and techni-\ncal passerine. A model more geared toward techni-\ncal terminology might assign a high similarity score\nto the latter, whereas a commonsense-knowledge-\noriented DSM might pick bird. Both models have\ncaptured similarity with a hypernym, and we have\nno reason, in general semantic terms, to penalize one\nor the other. To maximize coverage, we also make\nsure that, for each concept and relation, a reason-\nable number of relata are frequently attested in our\nreference corpora (see statistics below), we only in-\nclude single-word relata and, where appropriate, we\ninclude multiple forms for the same relatum (both\nsock and socks as coordinates of scarf – as discussed\nin Section 4.1, we avoided similar ambiguous items\nas target concepts).\n', 3, 0)
(82.90979766845703, 652.2632446289062, 298.8008117675781, 665.40869140625, 'Currently, distributional models for attributional\n', 4, 0)
(72.00080108642578, 665.812255859375, 298.801025390625, 706.0567016601562, 'similarity and relational similarity (Turney, 2006)\nare tested on different data sets, e.g., TOEFL and\nSAT respectively (brieﬂy, attributional similarity\n', 5, 0)
(313.2008056640625, 55.87024688720703, 540.0018310546875, 258.7037048339844, 'pertains to similarity between a pair of concepts in\nterms of shared properties, whereas relational sim-\nilarity measures the similarity of the relations in-\nstantiated by couples of concept pairs). Conversely,\nBLESS is not biased towards any particular type of\nsemantic similarity and thus allows both families of\nmodels to be evaluated on the same data set. Given\na concept, we can analyze the types of relata that are\nselected by a model as more attributionally similar\nto the target. Alternatively, given a concept-relatum\npair instantiating a speciﬁc semantic relation (e.g.,\nhypernymy) we can evaluate a model ability to iden-\ntify analogically similar pairs, i.e., others concept-\nrelatum pairs instantiating the same relation (we do\nnot illustrate this possibility here).\n', 6, 0)
(324.10980224609375, 259.6942443847656, 540.0007934570312, 272.8397216796875, 'Finally, by collecting distributions of 200 similar-\n', 7, 0)
(313.2008056640625, 273.2432556152344, 540.0009155273438, 367.6837463378906, 'ity values for each relation, BLESS allows reliable\nstatistical testing of the signiﬁcance of differences\nin similarity within a DSM (for example, using the\nprocedure we present in Section 5 below), as well\nas across DSMs (for example, via a linear/ANOVA\nmodel with relations and DSMs as factors – not il-\nlustrated here).\n', 8, 0)
(313.2008056640625, 378.93829345703125, 398.2142333984375, 394.49200439453125, '4\nConstruction\n', 9, 0)
(313.2008056640625, 402.3075256347656, 380.7717590332031, 416.5002746582031, '4.1\nConcepts\n', 10, 0)
(313.2008056640625, 421.34124755859375, 540.0009155273438, 448.0357360839844, 'BLESS includes 200 distinct English concrete\nnouns as target concepts,\nequally divided be-\n', 11, 0)
(313.2008056640625, 448.43927001953125, 540.0007934570312, 461.5847473144531, 'tween living and non-living entities.\nConcepts\n', 12, 0)
(313.2008056640625, 461.9882507324219, 539.728515625, 475.13372802734375, 'have been grouped into 17 broader classes:\nAM-\n', 13, 0)
(313.2008056640625, 475.53826904296875, 539.9976806640625, 502.23272705078125, 'PHIBIAN REPTILE (including amphibians and rep-\ntiles:\nalligator),\nAPPLIANCE\n(toaster),\nBIRD\n', 14, 0)
(313.2008056640625, 502.63629150390625, 540.0008544921875, 515.78173828125, '(crow), BUILDING (cottage), CLOTHING (sweater),\n', 15, 0)
(313.4737854003906, 516.1852416992188, 539.728515625, 529.3306884765625, 'CONTAINER (bottle),\nFRUIT (banana),\nFURNI-\n', 16, 0)
(313.4737854003906, 529.7342529296875, 539.728515625, 542.8796997070312, 'TURE (chair),\nGROUND MAMMAL (beaver),\nIN-\n', 17, 0)
(313.4737854003906, 543.2842407226562, 540.0014038085938, 556.4296875, 'SECT (cockroach),\nMUSICAL INSTRUMENT (vio-\n', 18, 0)
(313.2008056640625, 556.833251953125, 540.0009155273438, 583.5277099609375, 'lin), TOOL (i.e., manipulable tools or devices: ham-\nmer), TREE (birch), VEGETABLE (cabbage), VEHI-\n', 19, 0)
(313.2008056640625, 583.9312744140625, 540.0027465820312, 610.625732421875, 'CLE (bus), WATER ANIMAL (including ﬁsh and sea\nmammals: herring), WEAPON (dagger).\n', 20, 0)
(324.10980224609375, 611.6162719726562, 540.0007934570312, 624.76171875, 'All 200 BLESS concepts are single-word nouns\n', 21, 0)
(313.2008056640625, 625.1652221679688, 540.0009155273438, 706.0567016601562, 'in the singular form (we avoided concepts such as\nsocks whose surface form might change depending\non lemmatization choices). The major source we\nused to select the concepts were the McRae Norms\n(McRae et al., 2005), a collection of living and non-\nliving basic-level concepts described by 725 sub-\n', 22, 0)
(293.2098388671875, 724.5390014648438, 298.6643981933594, 737.6844482421875, '4\n', 23, 0)

page suivante
(72.00080108642578, 55.87024688720703, 298.8009948730469, 82.56472778320312, 'jects with semantic features, each tagged with its\nproperty type.\nAs further constraints guiding our\n', 0, 0)
(72.00080108642578, 82.96826934814453, 298.8009948730469, 245.15475463867188, 'selection, we wanted concepts with a reasonably\nhigh frequency (cf. Section 4.4), we avoided am-\nbiguous or highly polysemous concepts and we bal-\nanced inter- and intra-class composition. Classes in-\nclude both prototypical and atypical instances (e.g.,\nrobin and penguin for BIRD), and have a wide spec-\ntrum of internal variation (e.g., the class VEHICLE\ncontains wheeled, air and sea vehicles). 175 BLESS\nconcepts are attested in the McRae Norms, while the\nremnants were selected by the authors according to\nthe above constraints. The average number of con-\ncepts per class is 11.76 (median 11; min. 5 AMPHIB-\n', 1, 0)
(72.2738037109375, 245.5592803955078, 270.3147888183594, 258.7047424316406, 'IAN REPTILE; max. 21 GROUND MAMMAL).\n', 2, 0)
(72.00080108642578, 268.0965270996094, 140.1826629638672, 282.2892761230469, '4.2\nRelations\n', 3, 0)
(72.00080108642578, 286.27825927734375, 298.8044128417969, 326.521728515625, 'For each concept noun, BLESS includes several\nrelatum words, linked to the concept by one of\nthe following 5 relations.\nCOORD:\nthe relatum\n', 4, 0)
(72.00080108642578, 326.9252624511719, 298.805419921875, 394.2677307128906, 'is a noun that is a co-hyponym (coordinate) of\nthe concept, i.e., they belong to the same (nar-\nrowly or broadly deﬁned) semantic class: alligator-\ncoord-lizard; HYPER: the relatum is a noun that\nis a hypernym of the concept:\nalligator-hyper-\n', 5, 0)
(72.00080108642578, 394.6712646484375, 298.7972717285156, 407.8167419433594, 'animal;\nMERO:\nthe relatum is a noun referring\n', 6, 0)
(72.00080108642578, 408.22125244140625, 298.8009338378906, 475.562744140625, 'to a part/component/organ/member of the concept,\nor something that the concept contains or is made\nof: alligator-mero-mouth; ATTRI: the relatum is\nan adjective expressing an attribute of the concept:\nalligator-attri-aquatic;\nEVENT:\nthe relatum is a\n', 7, 0)
(72.00080108642578, 475.9672546386719, 298.8042297363281, 516.210693359375, 'verb referring to an action/activity/happening/event\nthe concept is involved in or is performed by/with\nthe concept:\nalligator-event-swim.\nBLESS also\n', 8, 0)
(72.00080108642578, 516.6142578125, 298.8010559082031, 570.40771484375, 'includes the relations RAN.N, RAN.J and RAN.V,\nwhich relate the target concepts to control tuples\nwith random noun, adjective and verb relata, respec-\ntively.\n', 9, 0)
(82.90979766845703, 570.96826171875, 298.8008117675781, 584.1137084960938, 'The BLESS relations cover a wide spectrum of\n', 10, 0)
(72.00080108642578, 584.5172729492188, 298.80584716796875, 706.0567016601562, 'information useful to describe a target concept and\nto qualify the notion of semantic relatedness: taxo-\nnomically related entities (hyper and coord), typical\nattributes (attri), components (mero), and associated\nevents (event). However, except for hyper and co-\nord (corresponding to the standard relations of class\ninclusion and co-hyponymy respectively), the other\nBLESS relations are highly underspeciﬁed. For in-\nstance, mero corresponds to a very broad notion of\n', 11, 0)
(313.2008056640625, 55.87024688720703, 540.0052490234375, 272.25372314453125, 'meronymy, including not only parts (dog-tail), but\nalso the material (table-wood) as well as the mem-\nbers (hospital-patient) of the entity the target con-\ncept refers to (Winston et al., 1987); event is used to\nrepresent the behaviors of animals (dog-bark), typi-\ncal functions of instruments (violin-play), and events\nthat are simply associated with the target concept\n(car-park); attri captures a large range of attributes,\nfrom physical (elephant-big) to evaluative ones (car-\nexpensive). As we said in section 3, we did not at-\ntempt to further specify these relations to avoid any\ncommitment to controversial ontologies of property\ntypes. Note that we exclude synonymy both because\nof the inherent problems in this very notion (Cruse,\n1986), and because it is impossible to ﬁnd convinc-\ning synonyms for 200 concrete concepts.\n', 12, 0)
(324.10980224609375, 272.7932434082031, 540.0007934570312, 285.938720703125, 'In BLESS, we have adopted the simplifying as-\n', 13, 0)
(313.2008056640625, 286.3422546386719, 540.0010986328125, 380.7827453613281, 'sumption that each relation type has relata belonging\nto the same part of speech: nouns for hyper, coord\nand mero, verbs for event, and adjectives for attri.\nTherefore, we abstract away from the fact that the\nsame semantic relation can be realized with different\nparts of speech, e.g., a related event can be expressed\nby a verb (transport) or by a noun (transportation).\n', 14, 0)
(313.2008056640625, 390.1025390625, 368.04083251953125, 404.2952880859375, '4.3\nRelata\n', 15, 0)
(313.2008056640625, 408.2412414550781, 540.0022583007812, 638.1746826171875, 'The relata of the non-random relations are English\nnouns, verbs and adjectives selected and validated\nby both authors using two types of sources: se-\nmantic sources (the McRae Norms (McRae et al.,\n2005), WordNet (Fellbaum, 1998) and ConceptNet\n(Liu and Singh, 2004)) and text sources (Wikipedia\nand the Web-derived ukWaC corpus, see Section 4.4\nbelow). These resources greatly differ in dimension,\norigin and content and therefore provide comple-\nmentary views on relata. Their relative contribution\nto BLESS also depends on the type of relation and\nthe target concept. For instance, the rich taxonomic\nstructure of WordNet has been the main source of in-\nformation for many technical hypernyms (e.g. gym-\nnosperm, oscine), which instead are missing from\nmore commonsense-oriented resources such as the\nMcRae Norms and ConceptNet.\nMeronyms are\n', 16, 0)
(313.2008056640625, 638.5782470703125, 540.0009155273438, 692.3707275390625, 'rarer in WordNet, and were collected mainly from\nthe latter two resources, with many technical terms\n(e.g., parts of ships, weapons) harvested from the\nWikipedia entries for the target concepts.\n', 17, 0)
(324.10980224609375, 692.9112548828125, 540.000732421875, 706.0567016601562, 'Attributes and events were collected from McRae\n', 18, 0)
(293.2098388671875, 724.5390014648438, 298.6643981933594, 737.6844482421875, '5\n', 19, 0)

page suivante
(72.00080108642578, 55.87024688720703, 298.8010559082031, 258.7047424316406, 'Norms, ConceptNet and ukWaC. In the McRae\nNorms, the number of features per concept is fairly\nlimited, but they correspond to highly distinctive,\nprototypical and cognitively salient properties. Con-\nceptNet instead provides a much wider array of as-\nsociated events and attributes that are part of our\ncommonsense knowledge about the target concepts\n(e.g., the events park, steal and break, etc. for car).\nConceptNet relations such as Created by, Used for,\nCapable of etc. have been analyzed to identify po-\ntential event relata, while the Has property relation\nhas been inspected to look for attributes. The most\nsalient adjectival and verbal collocates of the tar-\nget nouns in the ukWaC corpus were also used to\nidentify associated attributes and events.\nFor in-\n', 0, 0)
(72.00080108642578, 259.1082458496094, 298.801025390625, 502.5897216796875, 'stance, the target concept elephant is not attested in\nthe McRae Norms and has few properties in Con-\nceptNet. Thus, many of its related events have been\nharvested from ukWaC. They include verbs such as\nhunt, kill, etc. which are quite salient and frequent\nwith respect to elephants, although they can hardly\nbe deﬁned as prototypical properties of this animal.\nAs a result of the combined use of such different\ntypes of sources, the BLESS relata are representative\nof a wide spectrum of semantic information about\nthe target concepts: they include domain-speciﬁc\nterms side by side to commonsense ones, very dis-\ntinctive features of a concept (e.g., hoot for owl)\ntogether with attributes and events that are instead\nshared by a whole class of concepts (e.g., all animals\nhave relata such as eat, feed, and live), prototypical\nfeatures as well as events and attributes that are sta-\ntistically salient for the target, etc.\n', 1, 0)
(82.90979766845703, 503.10821533203125, 298.80084228515625, 516.253662109375, 'In many cases, the concept properties contained\n', 2, 0)
(72.00080108642578, 516.6572875976562, 298.8061828613281, 597.5487060546875, 'in semantic sources are expressed with phrases, e.g.,\nlay eggs, eat grass, live in Africa, etc. We decided,\nhowever, to keep only single-word relata in BLESS,\nbecause DSMs are typically populated with single\nwords, and, when they are not, they differ in the\nkinds of multi-word elements they store.\nThere-\n', 3, 0)
(72.00080108642578, 597.9522705078125, 298.8009948730469, 665.294677734375, 'fore, phrasal relata have always been reduced to\ntheir head: a verb for properties expressed by a verb\nphrase, and a noun for properties expressed by a\nnoun phrase. For instance, from the property lay\neggs, we derived the event relatum lay.\n', 4, 0)
(82.90979766845703, 665.812255859375, 298.80084228515625, 678.9577026367188, 'To extract the random relata, we adopted the fol-\n', 5, 0)
(72.00080108642578, 679.3622436523438, 298.8009948730469, 706.0567016601562, 'lowing procedure. For each relatum that instantiates\na true relation with the concept, we also randomly\n', 6, 0)
(313.2008056640625, 55.87024688720703, 540.0057983398438, 380.646728515625, 'picked from our combined corpus (cf. Section 4.4)\nanother lemma with the same part of speech, and\nfrequency within 1 absolute logarithmic unit from\nthe frequency of the corresponding true relatum.\nSince picking a random term does not guarantee\nthat it will not be related to the concept, we ﬁltered\nthe extracted list by crowdsourcing, using the Ama-\nzon Mechanical Turk via the CrowdFlower interface\n(CF).1 We presented CF workers with the list of\nabout 15K concept+random-term pairs selected with\nthe procedure we just described, plus a manually\nchecked validation set (a “gold set” in CF terminol-\nogy) comprised of 500 concept+true-relatum pairs\nand 500 concept+random-term pairs (these elements\nare used by CF to determine the reliability of work-\ners, and discard the ratings of unreliable ones), plus a\nfurther set of 1.5K manually checked concept+true-\nrelatum pairs to make the random-true distribution\nless skewed. The workers’ task was, for each pair,\nto check a YES radio button if they thought there is\na relation between the words, NO otherwise. The\nwords were annotated with their part of speech, and\nworkers were instructed to pay attention to this in-\nformation when making their choices.\nExtensive\n', 7, 0)
(313.2008056640625, 381.0502624511719, 540.0009155273438, 489.0407409667969, 'commented examples of both related pairs and un-\nrelated ones were also provided in the instruction\npage. A minimum of 2 CF workers rated each pair,\nand, conservatively, we preserved only those items\n(about 12K) that were unanimously rated as unre-\nlated to their concept by the judges. See Table 1 for\nsummary statistics about the preserved random sets\n(nouns: RAND.N, adjectives: RAN.J, verbs:RAN.V).\n', 8, 0)
(313.2008056640625, 501.6255187988281, 414.4263000488281, 515.8182983398438, '4.4\nBLESS statistics\n', 9, 0)
(313.2008056640625, 521.6372680664062, 540.0009155273438, 575.4296875, 'For frequency information, we rely on the combi-\nnation of the freely available ukWaC and Wackype-\ndia corpora (size: 1.915B and 820M tokens, respec-\ntively).2\nThe data set contains 200 concepts that\n', 10, 0)
(313.2008056640625, 575.833251953125, 540.0013427734375, 629.626708984375, 'have a mean corpus frequency of 53K occurrences\n(min. 1416 chisel, max. 793K car). The relata of\nthese concepts (26,554 in total) are distributed as re-\nported in Table 1.\n', 11, 0)
(324.10980224609375, 631.1092529296875, 540.0006713867188, 644.2546997070312, 'Note that the distributions reﬂect certain “natural”\n', 12, 0)
(313.2008056640625, 644.6582641601562, 540.0009765625, 671.3536987304688, 'differences between relations (hypernyms tend to be\nmore frequent words than coordinates, but there are\n', 13, 0)
(325.852783203125, 682.4859619140625, 485.3554992675781, 705.1163330078125, '1http://crowdflower.com/\n2http://wacky.sslmit.unibo.it/\n', 14, 0)
(293.2098388671875, 724.5390014648438, 298.6643981933594, 737.6844482421875, '6\n', 15, 0)

page suivante
(143.831787109375, 54.15484619140625, 274.531005859375, 66.0501937866211, 'frequency\ncardinality\n', 0, 0)
(79.00080108642578, 66.10980224609375, 289.2811584472656, 78.0051498413086, 'relation\nmin\navg\nmax\nmin\navg\nmax\n', 1, 0)
(79.66280364990234, 78.46282958984375, 289.3104248046875, 90.46776580810547, 'COORD\n0\n37K\n1.7M\n6\n17.1\n35\n', 2, 0)
(80.5498046875, 90.4188232421875, 289.3104248046875, 102.42375946044922, 'HYPER\n31\n138K\n1.9M\n2\n6.7\n15\n', 3, 0)
(82.50180053710938, 102.37384033203125, 289.3104248046875, 114.37877655029297, 'MERO\n0\n133K\n2M\n2\n14.7\n53\n', 4, 0)
(82.32279968261719, 114.328857421875, 289.3104248046875, 126.33379364013672, 'ATTRI\n0\n501K\n3.7M\n4\n13.6\n27\n', 5, 0)
(80.55480194091797, 126.2838134765625, 289.3104248046875, 138.2887420654297, 'EVENT\n0\n517K\n5.4M\n6\n19.1\n40\n', 6, 0)
(81.07279968261719, 138.23883056640625, 289.3106994628906, 150.24375915527344, 'RAN.N\n0\n92K\n2.4M\n16\n32.9\n67\n', 7, 0)
(82.3978042602539, 150.19384765625, 289.3104248046875, 162.1987762451172, 'RAN.J\n1\n472K\n4.5M\n3\n10.9\n24\n', 8, 0)
(81.07279968261719, 162.14984130859375, 289.3104248046875, 174.15476989746094, 'RAN.V\n1\n508K\n7.7M\n4\n16.3\n34\n', 9, 0)
(72.00080108642578, 183.48980712890625, 298.801025390625, 273.9903564453125, 'Table 1: Distribution (minimum, mean and maximum) of\nthe relata of all BLESS concepts: the frequency columns\nreport summary statistics for corpus counts across relata\ninstantiating a relation; the cardinality columns report\nsummary statistics for number of relata instantiating a\nrelation across the 200 concepts, only considering relata\nwith corpus frequency ≥ 100.\n', 10, 0)
(72.00080108642578, 289.54425048828125, 298.80096435546875, 383.9847412109375, 'more coordinates than hypernyms, etc.). Instead of\ntrying to artiﬁcially control for these differences, we\nassess their impact in Section 5 by looking at the\nbehavior of baselines that exploit the frequency and\ncardinality of relations as proxies to semantic simi-\nlarity (such factors could also be entered as regres-\nsors in a linear model).\n', 11, 0)
(72.00080108642578, 396.94427490234375, 145.6209259033203, 412.49798583984375, '5\nEvaluation\n', 12, 0)
(72.00080108642578, 421.92724609375, 298.8050537109375, 706.0567016601562, 'This section illustrates one possible way to use\nBLESS to explore and evaluate DSMs. Given the\nsimilarity scores provided by a model for a concept\nwith all its relata across all relations, we pick the re-\nlatum with the highest score (nearest neighbour) for\neach relation (see discussion in Section 3 above on\nwhy we allow models to pick their favorite from a\nset of relata instantiating the same relation). In this\nway, for each of the 200 BLESS concepts, we obtain\n8 similarity scores, one per relation. In order to fac-\ntor out concept-speciﬁc effects that might add to the\noverall score variance (for example, a frequent con-\ncept might have a denser neighborhood than a rarer\none, and consequently the nearest relatum scores of\nthe former are trivially higher than those of the lat-\nter), we transform the 8 similarity scores of each\nconcept onto standardized z scores (mean: 0; s.d: 1)\nby subtracting from each their mean, and dividing by\ntheir standard deviation. After this transformation,\nwe produce a boxplot summarizing the distribution\nof scores per relation across the 200 concepts (i.e.,\n', 13, 0)
(313.2008056640625, 55.87024688720703, 540.0010375976562, 190.95870971679688, 'each box of the plot summarizes the distribution of\nthe 200 standardized scores picked for each rela-\ntion). Our boxplots (see examples in Fig. 1 below)\ndisplay the median of a distribution as a thick hori-\nzontal line within a box extending from the ﬁrst to\nthe third quartile, with whiskers covering 1.5 of the\ninterquartile range in each direction from the box,\nand values outside this extended range – extreme\noutliers – plotted as circles (these are the default\nboxplotting option of the R statistical package).3\n', 14, 0)
(313.2008056640625, 191.3622589111328, 540.0056762695312, 448.3927307128906, 'While the boxplots are extremely informative about\nthe relation types that are best captured by models,\nwe expect some degree of overlap among the distri-\nbutions of different relations, and in such cases we\nmight want to ask whether a certain model assigns\nsigniﬁcantly higher scores to one relation rather than\nanother (for example, to coordinates rather than ran-\ndom nouns). It is difﬁcult to decide a priori which\npairwise statistical comparisons will be interesting.\nWe thus take a conservative approach in which we\nperform all pairwise comparisons using the Tukey\nHonestly Signiﬁcant Difference test, that is simi-\nlar to the standard t test, but accounts for the greater\nlikelihood of Type I errors when multiple compar-\nisons are performed (Abdi and Williams, 2010). We\nonly report the Tukey test results for those com-\nparisons that are of interest in the analysis of the\nboxplots, using the standard α = 0.05 signiﬁcance\nthreshold.\n', 15, 0)
(313.2008056640625, 457.65252685546875, 371.6844787597656, 471.84527587890625, '5.1\nModels\n', 16, 0)
(313.2008056640625, 475.75726318359375, 540.0010375976562, 570.1976928710938, 'Occurrence and co-occurrence statistics for all mod-\nels are extracted from the combined ukWaC and\nWackypedia corpora (see Section 4.4 above). We ex-\nploit the automated morphosyntactic annotation of\nthe corpora by building our DSMs out of lemmas\n(instead of inﬂected words), and relying on part of\nspeech information.\n', 17, 0)
(313.2008056640625, 577.9845581054688, 539.9969482421875, 592.1773071289062, 'Baselines.\nThe RelatumFrequency baseline uses\n', 18, 0)
(313.2008056640625, 591.92626953125, 540.0009155273438, 686.36669921875, 'the frequency of occurrence of a relatum as a sur-\nrogate of its cosine with the concept. With this ap-\nproach, we want to verify that the unequal frequency\ndistribution across relations (see Table 1 above) is\nnot trivially sufﬁcient to differentiate relation classes\nin a semantically interesting way. For our second\nbaseline, we assign a random number as cosine sur-\n', 19, 0)
(325.852783203125, 693.658935546875, 464.33819580078125, 705.1163330078125, '3http://www.r-project.org/\n', 20, 0)
(293.2098388671875, 724.5390014648438, 298.6643981933594, 737.6844482421875, '7\n', 21, 0)

page suivante
(72.00080108642578, 55.87024688720703, 298.802001953125, 204.50772094726562, 'rogate to each relatum (to smooth these random val-\nues, we generate them by ﬁrst sampling, for each\nrelatum, 10K random variates from a uniform distri-\nbution, and then averaging them). If the set of relata\ninstantiating a certain relation is larger, it is more\nlikely that it will contain the highest random value.\nThus, this RelationCardinality baseline will favor\nrelations that tend to have large relata set across con-\ncepts, controlling for effects due to different cardi-\nnalities across semantic relations (again, see Table 1\nabove).\n', 0, 0)
(72.00080108642578, 218.2965545654297, 298.80096435546875, 232.48928833007812, 'DSMs.\nWe choose a few ways to construct DSMs\n', 1, 0)
(72.00080108642578, 232.23829650878906, 298.801513671875, 422.17828369140625, 'for illustrative purposes only. All the models contain\nvector representations for the same words, namely,\napproximately, the top 20K most frequent nouns, 5K\nmost frequent adjectives and 5K most frequent verbs\nin the combined corpora. All the models use Local\nMutual Information (Evert, 2005; Baroni and Lenci,\n2010) to weight raw co-occurrence counts (this asso-\nciation measure is obtained by multiplying the raw\ncount by Pointwise Mutual Information, and it is a\nclose approximation to the Log-Likelihood Ratio).\nThree DSMs are based on counting co-occurrences\nwith collocates within a window of ﬁxed width,\nin the tradition of HAL (Lund and Burgess, 1996)\nand many later models.\nThe ContentWindow2\n', 2, 0)
(72.00080108642578, 421.92724609375, 298.8009948730469, 489.9242858886719, 'model records sentence-internal co-occurrence with\nthe nearest 2 content words to the left and right\nof each target concept (the same 30K target nouns,\nverbs and adjectives are also employed as context\ncontent words).\nContentWindow20 is like Con-\n', 3, 0)
(72.00080108642578, 489.6732482910156, 298.8012390136719, 557.6702880859375, 'tentWindow2, but considers a larger window of 20\nwords to the left and right of the target. AllWin-\ndow2 adopts the same window of ContentWindow2,\nbut considers all co-occurrences, not only those with\ncontent words.\nThe Document model, ﬁnally, is\n', 4, 0)
(72.00080108642578, 557.4192504882812, 298.801025390625, 706.0567016601562, 'based on a (Local-Mutual-Information transformed)\nword-by-document matrix, recording the distribu-\ntion of the 30K target words across the documents in\nthe concatenated corpus. This DSM is thus akin to\ntraditional Latent Semantic Analysis (Landauer and\nDumais, 1997), without dimensionality reduction.\nThe content-window-based models have, by con-\nstruction, about 30K dimensions. The other models\nare much larger, and for practical reasons we only\nkeep 1 million dimensions (those that account, cu-\nmulatively, for the largest proportion of the overall\n', 5, 0)
(313.2008056640625, 55.87024688720703, 458.0298767089844, 69.01571655273438, 'Local Mutual Information mass).\n', 6, 0)
(313.2008056640625, 78.40751647949219, 371.6844787597656, 92.60025024414062, '5.2\nResults\n', 7, 0)
(313.2008056640625, 96.58924102783203, 540.0009155273438, 150.38174438476562, 'The concept-by-concept z-normalized distributions\nof cosines of relata instantiating each of our rela-\ntions are presented, for each of the example mod-\nels, in Fig. 1.\nThe RelatumFrequency baseline\n', 8, 0)
(313.2008056640625, 150.78627014160156, 540.0009155273438, 326.521728515625, 'shows a preference for adjectives and verbs in gen-\neral, independently of whether they are meaningful\n(attributes, events) or not (random adjectives and\nverbs), reﬂecting the higher frequencies of adjec-\ntives and verbs in BLESS (Table 1). The Relation-\nCardinality baseline produces even less interesting\nresults, with a strong preference for random nouns,\nfollowed by coordinates, events and random verbs\n(as predicted by the distribution in Table 1). We can\nconclude that the semantically meaningful patterns\nproduced by the other models cannot be explained\nby trivial differences in relatum frequency or rela-\ntion cardinality in the BLESS data set.\n', 9, 0)
(324.10980224609375, 327.0822448730469, 540.000732421875, 340.22772216796875, 'Moving then to the real DSMs, ContentWindow2\n', 10, 0)
(313.2008056640625, 340.63226318359375, 540.0010986328125, 435.0727233886719, 'essentially partitions the relations into 3 groups: co-\nordinates are the closest relata, which makes sense\nsince they are, taxonomically, the most similar en-\ntities to target concepts. They are followed by (but\nsigniﬁcantly closer to the concept than) events, hy-\npernyms and meronyms (events and hypernyms sig-\nniﬁcantly above meronyms).\nNext come the at-\n', 11, 0)
(313.2008056640625, 435.47625732421875, 540.0010375976562, 706.0567016601562, 'tributes (signiﬁcantly lower cosines than all relation\ntypes above). All the meaningful relata are signif-\nicantly closer to the concepts than the random re-\nlata. Similar patterns can be observed in the Con-\ntentWindow20 distribution, however in this case the\nevents, while still signiﬁcantly below the coordi-\nnates, are signiﬁcantly above the (statistically in-\ndistinguishable) hypernym, meronym and attribute\nset. Again, all meaningful relata are above the ran-\ndom ones. Both content-window-based models pro-\nvide reasonable results, with ContentWindow2 be-\ning probably closer to our “ontological” intuitions.\nThe high ranking of events is probably explained\nby the fact that a nominal concept will often ap-\npear as subject or object of verbs expressing asso-\nciated events (dog barks, ﬁshing tuna), and thus the\ncorresponding verbs will share even relatively nar-\nrow context windows with the concept noun. The\nAllWindow2 distribution probably reﬂects the fact\nthat many contexts picked by this DSM are function\n', 12, 0)
(293.2098388671875, 724.5390014648438, 298.6643981933594, 737.6844482421875, '8\n', 13, 0)

page suivante
(114.41139221191406, 154.47193908691406, 115.8314208984375, 156.2671356201172, '●\n', 0, 0)
(114.41139221191406, 157.55352783203125, 115.8314208984375, 161.0623321533203, '●\n●\n', 1, 0)
(114.41139221191406, 123.33433532714844, 115.8314208984375, 125.1295394897461, '●\n', 2, 0)
(114.41139221191406, 156.35592651367188, 115.8314208984375, 158.151123046875, '●\n', 3, 0)
(129.41139221191406, 106.93513488769531, 130.8314208984375, 108.73033905029297, '●\n', 4, 0)
(129.41139221191406, 99.12793731689453, 130.8314208984375, 102.65113830566406, '●\n●\n', 5, 0)
(129.41139221191406, 95.94313049316406, 130.8314208984375, 97.73833465576172, '●\n', 6, 0)
(129.41139221191406, 113.3431396484375, 130.8314208984375, 115.13834381103516, '●\n', 7, 0)
(129.41139221191406, 107.38153076171875, 130.8314208984375, 109.1767349243164, '●\n', 8, 0)
(129.41139221191406, 100.59193420410156, 130.8314208984375, 102.38713836669922, '●\n', 9, 0)
(129.41139221191406, 111.05353546142578, 130.8314208984375, 112.84873962402344, '●\n', 10, 0)
(129.41139221191406, 97.27033233642578, 130.8314208984375, 99.06553649902344, '●\n', 11, 0)
(129.41139221191406, 113.08393859863281, 130.8314208984375, 114.87914276123047, '●\n', 12, 0)
(129.41139221191406, 100.49353790283203, 130.8314208984375, 102.28874206542969, '●\n', 13, 0)
(129.41139221191406, 106.81273651123047, 130.8314208984375, 108.60794067382812, '●\n', 14, 0)
(129.41139221191406, 116.28793334960938, 130.8314208984375, 118.08313751220703, '●\n', 15, 0)
(129.41139221191406, 91.25593566894531, 130.8314208984375, 93.05113983154297, '●\n', 16, 0)
(129.41139221191406, 103.59193420410156, 130.8314208984375, 105.63673400878906, '●●\n', 17, 0)
(129.41139221191406, 97.52953338623047, 130.8314208984375, 99.32473754882812, '●\n', 18, 0)
(129.41139221191406, 103.6975326538086, 130.8314208984375, 105.49273681640625, '●\n', 19, 0)
(129.41139221191406, 114.04153442382812, 130.8314208984375, 115.83673858642578, '●\n', 20, 0)
(129.41139221191406, 95.35033416748047, 130.8314208984375, 97.14553833007812, '●\n', 21, 0)
(129.41139221191406, 104.71273040771484, 130.8314208984375, 106.5079345703125, '●\n', 22, 0)
(129.41139221191406, 108.70153045654297, 130.8314208984375, 110.49673461914062, '●\n', 23, 0)
(129.41139221191406, 104.67433166503906, 130.8314208984375, 106.46953582763672, '●\n', 24, 0)
(144.41139221191406, 111.50713348388672, 145.8314208984375, 113.30233764648438, '●\n', 25, 0)
(144.41139221191406, 86.25914001464844, 145.8314208984375, 88.0543441772461, '●\n', 26, 0)
(144.41139221191406, 104.89032745361328, 145.8314208984375, 106.68553161621094, '●\n', 27, 0)
(144.41139221191406, 114.14713287353516, 145.8314208984375, 115.94233703613281, '●\n', 28, 0)
(144.41139221191406, 117.0703353881836, 145.8314208984375, 120.96073913574219, '●\n●\n', 29, 0)
(144.41139221191406, 110.89033508300781, 145.8314208984375, 112.68553924560547, '●\n', 30, 0)
(144.41139221191406, 118.54633331298828, 145.8314208984375, 120.34153747558594, '●\n', 31, 0)
(144.41139221191406, 144.74473571777344, 145.8314208984375, 146.53993225097656, '●\n', 32, 0)
(144.41139221191406, 108.01753234863281, 145.8314208984375, 109.81273651123047, '●\n', 33, 0)
(189.41139221191406, 95.75833129882812, 190.8314208984375, 100.02313995361328, '●\n●\n', 34, 0)
(189.41139221191406, 99.87433624267578, 190.8314208984375, 101.66954040527344, '●\n', 35, 0)
(189.41139221191406, 96.4903335571289, 190.8314208984375, 98.28553771972656, '●\n', 36, 0)
(189.41139221191406, 82.82713317871094, 190.8314208984375, 85.57513427734375, '●●\n', 37, 0)
(189.41139221191406, 92.95993041992188, 190.8314208984375, 94.75513458251953, '●\n', 38, 0)
(189.41139221191406, 82.47193908691406, 190.8314208984375, 84.26714324951172, '●\n', 39, 0)
(189.41139221191406, 97.33993530273438, 190.8314208984375, 99.13513946533203, '●\n', 40, 0)
(189.41139221191406, 74.60713195800781, 190.8314208984375, 76.40233612060547, '●\n', 41, 0)
(189.41139221191406, 102.58153533935547, 190.8314208984375, 104.37673950195312, '●\n', 42, 0)
(189.41139221191406, 82.31593322753906, 190.8314208984375, 84.11113739013672, '●\n', 43, 0)
(189.41139221191406, 98.30233764648438, 190.8314208984375, 100.83193969726562, '●●●\n', 44, 0)
(189.41139221191406, 76.61833190917969, 190.8314208984375, 78.41353607177734, '●\n', 45, 0)
(109.7625961303711, 167.8603973388672, 224.52145385742188, 170.74038696289062, 'COORD\nHYPER\nMERO\nATTRI\nEVENT\nRAN.N\nRAN.J\nRAN.V\n', 46, 0)
(96.45693969726562, 73.6715316772461, 99.33694458007812, 161.1188507080078, '−2\n−1\n0\n1\n2\n', 47, 0)
(141.72579956054688, 56.60113525390625, 193.5205841064453, 62.36113357543945, 'RelatumFrequency\n', 48, 0)
(255.3813934326172, 147.2479248046875, 256.8014221191406, 149.04312133789062, '●\n', 49, 0)
(255.3813934326172, 133.82473754882812, 256.8014221191406, 135.61993408203125, '●\n', 50, 0)
(255.3813934326172, 150.3799285888672, 256.8014221191406, 152.1751251220703, '●\n', 51, 0)
(255.3813934326172, 140.3119354248047, 256.8014221191406, 143.72952270507812, '●\n●\n', 52, 0)
(255.3813934326172, 131.8663330078125, 256.8014221191406, 135.09432983398438, '●●\n', 53, 0)
(255.3813934326172, 140.36473083496094, 256.8014221191406, 142.15992736816406, '●\n', 54, 0)
(255.3813934326172, 144.46392822265625, 256.8014221191406, 147.8359375, '●\n●\n', 55, 0)
(300.3813781738281, 129.49032592773438, 301.8014221191406, 135.49273681640625, '●●\n●\n●\n', 56, 0)
(315.3813781738281, 113.56874084472656, 316.8014221191406, 117.9007339477539, '●●\n●\n', 57, 0)
(315.3813781738281, 119.08633422851562, 316.8014221191406, 120.88153839111328, '●\n', 58, 0)
(315.3813781738281, 115.05673217773438, 316.8014221191406, 116.85193634033203, '●\n', 59, 0)
(360.38140869140625, 127.54873657226562, 361.80145263671875, 129.34393310546875, '●\n', 60, 0)
(360.38140869140625, 130.56072998046875, 361.80145263671875, 132.35592651367188, '●\n', 61, 0)
(360.38140869140625, 124.15032958984375, 361.80145263671875, 128.13433837890625, '●\n●\n', 62, 0)
(360.38140869140625, 131.90713500976562, 361.80145263671875, 133.70233154296875, '●\n', 63, 0)
(250.7325897216797, 167.8603973388672, 365.4914245605469, 170.74038696289062, 'COORD\nHYPER\nMERO\nATTRI\nEVENT\nRAN.N\nRAN.J\nRAN.V\n', 64, 0)
(237.42694091796875, 64.73153686523438, 240.3069305419922, 152.3540496826172, '−2\n−1\n0\n1\n2\n', 65, 0)
(282.4029846191406, 56.60113525390625, 334.779541015625, 62.36113357543945, 'RelationCardinality\n', 66, 0)
(396.35137939453125, 114.07513427734375, 397.77142333984375, 118.22713470458984, '●\n●\n', 67, 0)
(396.35137939453125, 120.6247329711914, 397.77142333984375, 122.41993713378906, '●\n', 68, 0)
(396.35137939453125, 110.88313293457031, 397.77142333984375, 112.67833709716797, '●\n', 69, 0)
(396.35137939453125, 116.69833374023438, 397.77142333984375, 120.92713928222656, '●\n●\n', 70, 0)
(426.35137939453125, 69.56233978271484, 427.77142333984375, 71.3575439453125, '●\n', 71, 0)
(441.35137939453125, 93.00553131103516, 442.77142333984375, 94.94474029541016, '●●\n', 72, 0)
(441.35137939453125, 84.68953704833984, 442.77142333984375, 86.4847412109375, '●\n', 73, 0)
(456.35137939453125, 75.17353820800781, 457.77142333984375, 77.68634033203125, '●●\n', 74, 0)
(471.35137939453125, 114.50233459472656, 472.77142333984375, 116.29753875732422, '●\n', 75, 0)
(471.35137939453125, 101.14153289794922, 472.77142333984375, 102.93673706054688, '●\n', 76, 0)
(471.35137939453125, 110.13433074951172, 472.77142333984375, 111.92953491210938, '●\n', 77, 0)
(471.35137939453125, 142.6327362060547, 472.77142333984375, 144.4279327392578, '●\n', 78, 0)
(471.35137939453125, 156.5503387451172, 472.77142333984375, 158.3455352783203, '●\n', 79, 0)
(471.35137939453125, 102.99673461914062, 472.77142333984375, 104.79193878173828, '●\n', 80, 0)
(471.35137939453125, 110.69593048095703, 472.77142333984375, 115.01113891601562, '●\n●●●\n', 81, 0)
(471.35137939453125, 159.2671356201172, 472.77142333984375, 161.0623321533203, '●\n', 82, 0)
(471.35137939453125, 99.41353607177734, 472.77142333984375, 101.208740234375, '●\n', 83, 0)
(471.35137939453125, 85.04473114013672, 472.77142333984375, 86.83993530273438, '●\n', 84, 0)
(471.35137939453125, 111.46873474121094, 472.77142333984375, 113.2639389038086, '●\n', 85, 0)
(471.35137939453125, 115.46953582763672, 487.77142333984375, 117.76153564453125, '●\n●\n', 86, 0)
(486.35137939453125, 93.283935546875, 487.77142333984375, 95.07913970947266, '●\n', 87, 0)
(486.35137939453125, 115.27273559570312, 487.77142333984375, 117.06793975830078, '●\n', 88, 0)
(486.35137939453125, 111.32233428955078, 487.77142333984375, 113.92633819580078, '●●\n', 89, 0)
(486.35137939453125, 104.69833374023438, 487.77142333984375, 106.49353790283203, '●\n', 90, 0)
(486.35137939453125, 114.07752990722656, 487.77142333984375, 115.87273406982422, '●\n', 91, 0)
(486.35137939453125, 152.27113342285156, 487.77142333984375, 154.0663299560547, '●\n', 92, 0)
(501.35137939453125, 113.91433715820312, 502.77142333984375, 117.52153778076172, '●●\n●\n', 93, 0)
(501.35137939453125, 79.45993041992188, 502.77142333984375, 81.25513458251953, '●\n', 94, 0)
(501.35137939453125, 105.01993560791016, 502.77142333984375, 106.81513977050781, '●\n', 95, 0)
(501.35137939453125, 142.3375244140625, 502.77142333984375, 144.13272094726562, '●\n', 96, 0)
(501.35137939453125, 112.88233947753906, 502.77142333984375, 114.67754364013672, '●\n', 97, 0)
(501.35137939453125, 105.97513580322266, 502.77142333984375, 108.45673370361328, '●●\n', 98, 0)
(501.35137939453125, 103.33513641357422, 502.77142333984375, 105.13034057617188, '●\n', 99, 0)
(501.35137939453125, 142.67832946777344, 502.77142333984375, 144.47352600097656, '●\n', 100, 0)
(501.35137939453125, 105.7423324584961, 502.77142333984375, 107.53753662109375, '●\n', 101, 0)
(391.7026062011719, 167.8603973388672, 506.46142578125, 170.74038696289062, 'COORD\nHYPER\nMERO\nATTRI\nEVENT\nRAN.N\nRAN.J\nRAN.V\n', 102, 0)
(378.3969421386719, 74.82593536376953, 381.2769470214844, 155.17164611816406, '−2\n−1\n0\n1\n2\n', 103, 0)
(426.08740234375, 56.60113525390625, 473.03802490234375, 62.36113357543945, 'ContentWindow2\n', 104, 0)
(114.41139221191406, 267.0781555175781, 115.8314208984375, 268.87335205078125, '●\n', 105, 0)
(114.41139221191406, 254.36294555664062, 115.8314208984375, 261.61334228515625, '●●\n●\n●\n', 106, 0)
(159.41139221191406, 191.7757568359375, 160.8314208984375, 194.09414672851562, '●●\n', 107, 0)
(189.41139221191406, 210.90615844726562, 190.8314208984375, 212.70135498046875, '●\n', 108, 0)
(189.41139221191406, 236.0773468017578, 190.8314208984375, 237.87254333496094, '●\n', 109, 0)
(189.41139221191406, 231.89654541015625, 190.8314208984375, 233.69174194335938, '●\n', 110, 0)
(189.41139221191406, 227.88134765625, 190.8314208984375, 229.67654418945312, '●\n', 111, 0)
(189.41139221191406, 233.82135009765625, 190.8314208984375, 237.21253967285156, '●\n●\n', 112, 0)
(189.41139221191406, 222.6925506591797, 190.8314208984375, 224.4877471923828, '●\n', 113, 0)
(189.41139221191406, 234.4117431640625, 190.8314208984375, 237.81015014648438, '●●\n●\n', 114, 0)
(204.41139221191406, 222.61334228515625, 205.8314208984375, 224.40853881835938, '●\n', 115, 0)
(204.41139221191406, 225.74295043945312, 205.8314208984375, 227.53814697265625, '●\n', 116, 0)
(204.41139221191406, 222.74295043945312, 205.8314208984375, 224.53814697265625, '●\n', 117, 0)
(204.41139221191406, 214.8229522705078, 205.8314208984375, 216.61814880371094, '●\n', 118, 0)
(204.41139221191406, 223.61175537109375, 205.8314208984375, 225.40695190429688, '●\n', 119, 0)
(204.41139221191406, 210.47415161132812, 205.8314208984375, 212.26934814453125, '●\n', 120, 0)
(219.41140747070312, 217.820556640625, 220.83143615722656, 219.61575317382812, '●\n', 121, 0)
(219.41140747070312, 202.72695922851562, 220.83143615722656, 204.52215576171875, '●\n', 122, 0)
(219.41140747070312, 217.77975463867188, 220.83143615722656, 220.9261474609375, '●●●\n', 123, 0)
(219.41140747070312, 226.38613891601562, 220.83143615722656, 228.18133544921875, '●\n', 124, 0)
(219.41140747070312, 218.11575317382812, 220.83143615722656, 219.91094970703125, '●\n', 125, 0)
(219.41140747070312, 224.5765380859375, 220.83143615722656, 226.37173461914062, '●\n', 126, 0)
(219.41140747070312, 211.70054626464844, 220.83143615722656, 213.49574279785156, '●\n', 127, 0)
(219.41140747070312, 274.03094482421875, 220.83143615722656, 275.8261413574219, '●\n', 128, 0)
(109.7625961303711, 289.81939697265625, 224.52145385742188, 292.69940185546875, 'COORD\nHYPER\nMERO\nATTRI\nEVENT\nRAN.N\nRAN.J\nRAN.V\n', 129, 0)
(96.45693969726562, 195.09054565429688, 99.33694458007812, 277.373046875, '−2\n−1\n0\n1\n2\n', 130, 0)
(142.54660034179688, 178.5601348876953, 192.69979858398438, 184.3201446533203, 'ContentWindow20\n', 131, 0)
(255.3813934326172, 231.58694458007812, 256.8014221191406, 233.90533447265625, '●●\n', 132, 0)
(270.3813781738281, 251.10855102539062, 271.8014221191406, 252.90374755859375, '●\n', 133, 0)
(270.3813781738281, 255.262939453125, 271.8014221191406, 257.0581359863281, '●\n', 134, 0)
(270.3813781738281, 258.81976318359375, 271.8014221191406, 260.6149597167969, '●\n', 135, 0)
(270.3813781738281, 250.60455322265625, 271.8014221191406, 252.39974975585938, '●\n', 136, 0)
(270.3813781738281, 196.81814575195312, 271.8014221191406, 198.61334228515625, '●\n', 137, 0)
(285.3813781738281, 251.51414489746094, 286.8014221191406, 253.30934143066406, '●\n', 138, 0)
(285.3813781738281, 268.9237365722656, 286.8014221191406, 270.71893310546875, '●\n', 139, 0)
(285.3813781738281, 256.6381530761719, 286.8014221191406, 258.433349609375, '●\n', 140, 0)
(285.3813781738281, 271.1005554199219, 286.8014221191406, 272.895751953125, '●\n', 141, 0)
(285.3813781738281, 249.98294067382812, 286.8014221191406, 251.77813720703125, '●\n', 142, 0)
(285.3813781738281, 192.4165496826172, 286.8014221191406, 194.2117462158203, '●\n', 143, 0)
(285.3813781738281, 267.3133544921875, 286.8014221191406, 269.1085510253906, '●\n', 144, 0)
(285.3813781738281, 260.85736083984375, 286.8014221191406, 264.1309509277344, '●\n●\n', 145, 0)
(285.3813781738281, 253.71255493164062, 286.8014221191406, 255.50775146484375, '●\n', 146, 0)
(285.3813781738281, 261.3469543457031, 286.8014221191406, 263.14215087890625, '●\n', 147, 0)
(300.3813781738281, 251.28854370117188, 301.8014221191406, 253.083740234375, '●\n', 148, 0)
(300.3813781738281, 257.67254638671875, 301.8014221191406, 259.4677429199219, '●\n', 149, 0)
(300.3813781738281, 261.51495361328125, 301.8014221191406, 263.3101501464844, '●\n', 150, 0)
(300.3813781738281, 251.4877471923828, 301.8014221191406, 253.28294372558594, '●\n', 151, 0)
(330.3813781738281, 249.86294555664062, 331.8014221191406, 251.65814208984375, '●\n', 152, 0)
(330.3813781738281, 254.3389434814453, 331.8014221191406, 256.1341552734375, '●\n', 153, 0)
(330.3813781738281, 247.77255249023438, 331.8014221191406, 249.5677490234375, '●\n', 154, 0)
(250.7325897216797, 289.81939697265625, 365.4914245605469, 292.69940185546875, 'COORD\nHYPER\nMERO\nATTRI\nEVENT\nRAN.N\nRAN.J\nRAN.V\n', 155, 0)
(237.42694091796875, 191.17373657226562, 240.3069305419922, 276.2258605957031, '−2\n−1\n0\n1\n2\n', 156, 0)
(292.31500244140625, 178.5601348876953, 324.87139892578125, 184.3201446533203, 'AllWindow2\n', 157, 0)
(441.35137939453125, 220.04295349121094, 442.77142333984375, 221.83815002441406, '●\n', 158, 0)
(441.35137939453125, 205.99575805664062, 442.77142333984375, 207.79095458984375, '●\n', 159, 0)
(441.35137939453125, 200.3461456298828, 442.77142333984375, 202.14134216308594, '●\n', 160, 0)
(441.35137939453125, 220.3717498779297, 442.77142333984375, 224.358154296875, '●\n●\n', 161, 0)
(441.35137939453125, 189.90615844726562, 442.77142333984375, 191.70135498046875, '●\n', 162, 0)
(441.35137939453125, 217.69573974609375, 442.77142333984375, 219.49093627929688, '●\n', 163, 0)
(441.35137939453125, 203.56214904785156, 442.77142333984375, 205.3573455810547, '●\n', 164, 0)
(441.35137939453125, 219.7021484375, 442.77142333984375, 221.49734497070312, '●\n', 165, 0)
(441.35137939453125, 187.80615234375, 442.77142333984375, 189.60134887695312, '●\n', 166, 0)
(441.35137939453125, 191.79495239257812, 442.77142333984375, 193.59014892578125, '●\n', 167, 0)
(441.35137939453125, 200.0365447998047, 442.77142333984375, 201.8317413330078, '●\n', 168, 0)
(441.35137939453125, 221.41815185546875, 442.77142333984375, 223.21334838867188, '●\n', 169, 0)
(441.35137939453125, 208.30215454101562, 442.77142333984375, 210.09735107421875, '●\n', 170, 0)
(471.35137939453125, 276.4909362792969, 472.77142333984375, 278.2861328125, '●\n', 171, 0)
(471.35137939453125, 281.22613525390625, 472.77142333984375, 283.0213317871094, '●\n', 172, 0)
(471.35137939453125, 239.55494689941406, 472.77142333984375, 241.3501434326172, '●\n', 173, 0)
(471.35137939453125, 271.6021423339844, 472.77142333984375, 275.017333984375, '●\n●\n', 174, 0)
(471.35137939453125, 242.43495178222656, 472.77142333984375, 244.29495239257812, '●●\n', 175, 0)
(471.35137939453125, 246.65655517578125, 472.77142333984375, 248.45175170898438, '●\n', 176, 0)
(471.35137939453125, 271.5949401855469, 472.77142333984375, 273.39013671875, '●\n', 177, 0)
(486.35137939453125, 244.38134765625, 487.77142333984375, 246.17654418945312, '●\n', 178, 0)
(486.35137939453125, 278.40374755859375, 487.77142333984375, 280.1989440917969, '●\n', 179, 0)
(486.35137939453125, 241.17735290527344, 487.77142333984375, 245.2021484375, '●\n●●\n', 180, 0)
(486.35137939453125, 230.14215087890625, 487.77142333984375, 231.93734741210938, '●\n', 181, 0)
(486.35137939453125, 244.97174072265625, 487.77142333984375, 247.34774780273438, '●●\n', 182, 0)
(486.35137939453125, 237.970947265625, 487.77142333984375, 239.76614379882812, '●\n', 183, 0)
(486.35137939453125, 240.7261505126953, 487.77142333984375, 245.8885498046875, '●●\n●●\n', 184, 0)
(501.35137939453125, 225.51014709472656, 502.77142333984375, 227.3053436279297, '●\n', 185, 0)
(501.35137939453125, 236.0533447265625, 502.77142333984375, 237.84854125976562, '●\n', 186, 0)
(501.35137939453125, 241.38134765625, 502.77142333984375, 243.17654418945312, '●\n', 187, 0)
(501.35137939453125, 223.97174072265625, 502.77142333984375, 225.76693725585938, '●\n', 188, 0)
(501.35137939453125, 234.7069549560547, 502.77142333984375, 236.5021514892578, '●\n', 189, 0)
(501.35137939453125, 239.2237548828125, 502.77142333984375, 241.01895141601562, '●\n', 190, 0)
(501.35137939453125, 212.01014709472656, 502.77142333984375, 213.8053436279297, '●\n', 191, 0)
(501.35137939453125, 237.78614807128906, 502.77142333984375, 239.5813446044922, '●\n', 192, 0)
(501.35137939453125, 230.14454650878906, 502.77142333984375, 231.9397430419922, '●\n', 193, 0)
(501.35137939453125, 238.77255249023438, 502.77142333984375, 240.5677490234375, '●\n', 194, 0)
(501.35137939453125, 231.04934692382812, 502.77142333984375, 234.60614013671875, '●\n●●\n', 195, 0)
(391.7026062011719, 289.81939697265625, 506.46142578125, 292.69940185546875, 'COORD\nHYPER\nMERO\nATTRI\nEVENT\nRAN.N\nRAN.J\nRAN.V\n', 196, 0)
(378.3969421386719, 197.7377471923828, 381.2769470214844, 284.4938659667969, '−2\n−1\n0\n1\n2\n', 197, 0)
(435.4809875488281, 178.5601348876953, 463.64166259765625, 184.3201446533203, 'Document\n', 198, 0)
(72.00080108642578, 310.62481689453125, 540.00390625, 334.58477783203125, 'Figure 1: Distribution of relata cosines across concepts (values on ordinate are cosines after concept-by-concept z-\nnormalization).\n', 199, 0)
(72.00080108642578, 354.3582458496094, 298.8014221191406, 692.6837158203125, 'words, and thus they capture syntactic, rather than\nsemantic distributional properties. As a result, ran-\ndom nouns are as high (statistically indistinguish-\nable from) hypernyms and meronyms. Interestingly,\nattributes also belong to this subset of relations –\nprobably due to the effect of determiners, quantiﬁers\nand other DP-initial function words, that will often\noccur both before nouns and before adjectives. In-\ndeed, even random adjectives, although signiﬁcantly\nbelow the other relations we discussed, are signif-\nicantly above both random and meaningful verbs\n(i.e., events). For the Document model, all mean-\ningful relations are signiﬁcantly above the random\nones. However, coordinates, while still the nearest\nneighbours (signiﬁcantly closer than all other rela-\ntions) are much less distinct than in the window-\nbased models. Note that we cannot say a priori that\nContentWindow2 is better than Document because\nit favors coordinates. However, while they are both\nable to sort out true and random relata, the latter\nshows a weaker ability to discriminate among differ-\nent types of semantic relations (co-occurring within\na document is indeed a much looser cue to similarity\nthan speciﬁcally co-occurring within a narrow win-\ndow). Traditional DSM tests, based on a single qual-\n', 200, 0)
(313.2008056640625, 354.3582458496094, 540.0008544921875, 381.052734375, 'ity measure, would not have given us this broad view\nof how models are behaving.\n', 201, 0)
(313.2008056640625, 395.7212829589844, 388.26751708984375, 411.2749938964844, '6\nConclusion\n', 202, 0)
(313.2008056640625, 421.92724609375, 540.006103515625, 706.0567016601562, 'We introduced BLESS, the ﬁrst data set speciﬁcally\ndesigned for the intrinsic evaluation of DSMs. The\ndata set contains tuples instantiating different, ex-\nplicitly typed semantic relations, plus a number of\ncontrolled random tuples. Thus, BLESS can be used\nto evaluate both the ability of DSMs to discriminate\ntruly related word pairs, and to perform in-depth\nanalyses of the types of semantic relata that different\nmodels tend to favor among the nearest neighbors of\na target concept. Even a simple comparison of the\nperformance of a few DSMs on BLESS - like the\none we have shown here - is able to highlight inter-\nesting differences in the semantic spaces produced\nby the various models. The success of BLESS will\nobviously depend on whether it will become a refer-\nence model for the evaluation of DSMs, something\nthat can not be foreseen a priori. Whatever its des-\ntiny, we believe that the BLESS approach can boost\nand innovate evaluation in distributional semantics,\nas a key condition to get at a deeper understanding\nof its potentialities as a viable model for meaning.\n', 203, 0)
(293.2098388671875, 724.5390014648438, 298.6643981933594, 737.6844482421875, '9\n', 204, 0)

page suivante
(72.00080108642578, 54.4732666015625, 127.54463958740234, 70.0269775390625, 'References\n', 0, 0)
(72.00080108642578, 75.3148193359375, 298.7993469238281, 87.31975555419922, 'Herv Abdi and Lynne Williams. 2010. Newman-Keuls\n', 1, 0)
(82.90979766845703, 87.26983642578125, 298.8004455566406, 123.18578338623047, 'and Tukey test. In N.J. Salkind, D.M. Dougherty, and\nB. Frey, editors, Encyclopedia of Research Design.\nSage, Thousand Oaks, CA.\n', 2, 0)
(72.00080108642578, 123.76080322265625, 298.79937744140625, 135.76573181152344, 'Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana\n', 3, 0)
(82.90979766845703, 135.71685791015625, 298.80224609375, 171.6317596435547, 'Kravalova, Marius Pasc¸a, and Aitor Soroa. 2009. A\nstudy on similarity and relatedness using distributional\nand WordNet-based approaches.\nIn Proceedings of\n', 4, 0)
(82.90979766845703, 171.58184814453125, 247.0532989501953, 183.58677673339844, 'HLT-NAACL, pages 19–27, Boulder, CO.\n', 5, 0)
(72.00080108642578, 184.162841796875, 298.7972412109375, 196.1677703857422, 'Abdulrahman Almuhareb. 2006. Attributes in Lexical\n', 6, 0)
(82.90979766845703, 196.1177978515625, 260.5722351074219, 208.1227264404297, 'Acquisition. Phd thesis, University of Essex.\n', 7, 0)
(72.00080108642578, 208.6988525390625, 298.79937744140625, 220.7037811279297, 'Marco Baroni and Alessandro Lenci.\n2010.\nDis-\n', 8, 0)
(82.90979766845703, 220.65380859375, 298.7992858886719, 232.6587371826172, 'tributional\nMemory:\nA\ngeneral\nframework\nfor\n', 9, 0)
(82.90979766845703, 232.60882568359375, 298.8004455566406, 256.56878662109375, 'corpus-based semantics. Computational Linguistics,\n36(4):673–721.\n', 10, 0)
(72.00080108642578, 257.14483642578125, 298.79937744140625, 269.1497802734375, 'Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-\n', 11, 0)
(82.90979766845703, 269.0998229980469, 298.8006896972656, 316.97076416015625, 'tors. 2008. Bridging the Gap between Semantic The-\nory and Computational Simulations: Proceedings of\nthe ESSLLI Workshop on Distributional Lexical Se-\nmantic. FOLLI, Hamburg.\n', 12, 0)
(72.00080108642578, 317.54583740234375, 298.79931640625, 329.55078125, 'Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-\n', 13, 0)
(82.90979766845703, 329.5008239746094, 298.7992858886719, 365.4167785644531, 'simo Poesio. 2010. Strudel: A distributional semantic\nmodel based on properties and types. Cognitive Sci-\nence, 34(2):222–254.\n', 14, 0)
(72.00080108642578, 365.9918212890625, 298.79937744140625, 377.99676513671875, 'Alexander Budanitsky and Graeme Hirst. 2006. Evalu-\n', 15, 0)
(82.90979766845703, 377.94781494140625, 298.79937744140625, 401.90777587890625, 'ating wordnet-based measures of lexical semantic re-\nlatedness. Computational Linguistics, 32:13–47.\n', 16, 0)
(72.00080108642578, 402.4828186035156, 298.7991943359375, 414.4877624511719, 'D. A. Cruse. 1986. Lexical Semantics. Cambridge Uni-\n', 17, 0)
(82.90979766845703, 414.4388427734375, 185.14596557617188, 426.44378662109375, 'versity Press, Cambridge.\n', 18, 0)
(72.00080108642578, 427.0188293457031, 298.79791259765625, 439.0237731933594, 'Stefan Evert.\n2005.\nThe Statistics of Word Cooccur-\n', 19, 0)
(82.90979766845703, 438.9748229980469, 248.02020263671875, 450.9797668457031, 'rences. Dissertation, Stuttgart University.\n', 20, 0)
(72.00080108642578, 451.5548400878906, 298.802734375, 463.5597839355469, 'Christiane Fellbaum, editor. 1998. WordNet: An Elec-\n', 21, 0)
(82.90979766845703, 463.50982666015625, 298.7997131347656, 475.5147705078125, 'tronic Lexical Database. MIT Press, Cambridge, MA.\n', 22, 0)
(72.00080108642578, 476.0908203125, 298.7993469238281, 488.09576416015625, 'Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,\n', 23, 0)
(82.90979766845703, 488.04583740234375, 298.8004455566406, 535.9168090820312, 'Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan\nRuppin. 2002. Placing search in context: The concept\nrevisited. ACM Transactions on Information Systems,\n20(1):116–131.\n', 24, 0)
(72.00080108642578, 536.4918212890625, 298.79931640625, 548.4967651367188, 'Thomas Landauer and Susan Dumais.\n1997.\nA solu-\n', 25, 0)
(82.90979766845703, 548.4478149414062, 298.79931640625, 584.36279296875, 'tion to Plato’s problem: The latent semantic analysis\ntheory of acquisition, induction, and representation of\nknowledge. Psychological Review, 104(2):211–240.\n', 26, 0)
(72.00080108642578, 584.9388427734375, 298.7992248535156, 596.9437866210938, 'Hugo Liu and Push Singh. 2004. ConceptNet: A prac-\n', 27, 0)
(82.90979766845703, 596.893798828125, 298.7972412109375, 620.853759765625, 'tical commonsense reasoning toolkit. BT Technology\nJournal, pages 211–226.\n', 28, 0)
(72.00080108642578, 621.4298095703125, 298.7993469238281, 633.4347534179688, 'Kevin Lund and Curt Burgess.\n1996.\nProducing\n', 29, 0)
(82.90979766845703, 633.3848266601562, 298.8044738769531, 657.3447875976562, 'high-dimensional semantic spaces from lexical co-\noccurrence. Behavior Research Methods, 28:203–208.\n', 30, 0)
(72.00080108642578, 657.9208374023438, 298.7993469238281, 669.92578125, 'Ken McRae, George Cree, Mark Seidenberg, and Chris\n', 31, 0)
(82.90979766845703, 669.8758544921875, 298.8006896972656, 705.790771484375, 'McNorgan. 2005. Semantic feature production norms\nfor a large set of living and nonliving things. Behavior\nResearch Methods, 37(4):547–559.\n', 32, 0)
(313.2008056640625, 56.74481201171875, 539.999267578125, 68.74974822998047, 'George Miller and Walter Charles. 1991. Contextual cor-\n', 33, 0)
(324.10980224609375, 68.6998291015625, 540.0014038085938, 92.65978240966797, 'relates of semantic similarity. Language and Cogni-\ntive Processes, 6(1):1–28.\n', 34, 0)
(313.2008056640625, 93.35784912109375, 539.9983520507812, 105.36278533935547, 'Gregory Murphy. 2002. The Big Book of Concepts. MIT\n', 35, 0)
(324.10980224609375, 105.31280517578125, 417.9176940917969, 117.31774139404297, 'Press, Cambridge, MA.\n', 36, 0)
(313.2008056640625, 118.01483154296875, 540.000732421875, 130.01976013183594, 'Timothy Rogers and James McClelland. 2004. Seman-\n', 37, 0)
(324.10980224609375, 129.9708251953125, 539.9993896484375, 153.93077087402344, 'tic Cognition: A Parallel Distributed Processing Ap-\nproach. MIT Press, Cambridge, MA.\n', 38, 0)
(313.2008056640625, 154.6278076171875, 539.9993286132812, 166.6327362060547, 'Herbert Rubenstein and John Goodenough. 1965. Con-\n', 39, 0)
(324.10980224609375, 166.58282470703125, 540.0056762695312, 190.5427703857422, 'textual correlates of synonymy. Communications of\nthe ACM, 8(10):627–633.\n', 40, 0)
(313.2008056640625, 191.2408447265625, 540.0010375976562, 203.2457733154297, 'Karen Sparck Jones and Julia R. Galliers. 1996. Evaluat-\n', 41, 0)
(324.10980224609375, 203.19580078125, 539.9993286132812, 227.15574645996094, 'ing Natural Language Processing Systems: An Analy-\nsis and Review. Springer Verlag, Berlin.\n', 42, 0)
(313.2008056640625, 227.85382080078125, 539.9994506835938, 239.85874938964844, 'Peter Turney and Patrick Pantel. 2010. From frequency\n', 43, 0)
(324.10980224609375, 239.808837890625, 540.00048828125, 263.768798828125, 'to meaning: Vector space models of semantics. Jour-\nnal of Artiﬁcial Intelligence Research, 37:141–188.\n', 44, 0)
(313.2008056640625, 264.4658203125, 539.9993286132812, 276.47076416015625, 'Peter Turney.\n2006.\nSimilarity of semantic relations.\n', 45, 0)
(324.10980224609375, 276.42181396484375, 496.81036376953125, 288.4267578125, 'Computational Linguistics, 32(3):379–416.\n', 46, 0)
(313.2008056640625, 289.12384033203125, 539.999267578125, 301.1287841796875, 'Morton E. Winston, Roger Chafﬁn, and Douglas Her-\n', 47, 0)
(324.10980224609375, 301.0788269042969, 539.999267578125, 325.03875732421875, 'rmann. 1987. A taxonomy of part-whole relations.\nCognitive Science, 11:417–444.\n', 48, 0)
(290.4825439453125, 724.5390014648438, 301.39166259765625, 737.6844482421875, '10\n', 49, 0)

page suivante
