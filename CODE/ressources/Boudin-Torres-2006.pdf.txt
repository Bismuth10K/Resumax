(160.4530029296875, 801.6880493164062, 440.4891357421875, 823.8195190429688, 'Coling 2008: Companion volume – Posters and Demonstrations, pages 23–26\nManchester, August 2008\n', 0, 0)
(151.64700317382812, 77.11167907714844, 445.902099609375, 111.7160873413086, 'A Scalable MMR Approach to Sentence Scoring\nfor Multi-Document Update Summarization\n', 1, 0)
(103.84500122070312, 128.00344848632812, 294.0196533203125, 147.46478271484375, 'Florian Boudin ♮ and Marc El-B`eze ♮\n', 2, 0)
(108.19798278808594, 142.06045532226562, 290.1680908203125, 188.70046997070312, '♮ Laboratoire Informatique d’Avignon\n339 chemin des Meinajaries, BP1228,\n84911 Avignon Cedex 9, France.\n', 3, 0)
(118.48497772216797, 192.01731872558594, 279.8801574707031, 201.41409301757812, 'florian.boudin@univ-avignon.fr\n', 4, 0)
(126.55497741699219, 205.96434020996094, 271.8106384277344, 215.36111450195312, 'marc.elbeze@univ-avignon.fr\n', 5, 0)
(317.7959899902344, 131.91107177734375, 478.428466796875, 147.46478271484375, 'Juan-Manuel Torres-Moreno ♮,♭\n', 6, 0)
(307.5409851074219, 142.94644165039062, 489.18829345703125, 189.58645629882812, '♭ ´Ecole Polytechnique de Montr´eal\nCP 6079 Succ. Centre Ville H3C 3A7\nMontr´eal (Qu´ebec), Canada.\n', 7, 0)
(306.906005859375, 192.90330505371094, 489.82098388671875, 202.30007934570312, 'juan-manuel.torres@univ-avignon.fr\n', 8, 0)
(159.60101318359375, 255.73309326171875, 204.0863037109375, 271.28680419921875, 'Abstract\n', 9, 0)
(89.00898742675781, 277.55206298828125, 274.681884765625, 412.6405944824219, 'We present SMMR, a scalable sentence\nscoring method for query-oriented up-\ndate summarization. Sentences are scored\nthanks to a criterion combining query rele-\nvance and dissimilarity with already read\ndocuments (history).\nAs the amount of\ndata in history increases, non-redundancy\nis prioritized over query-relevance.\nWe\nshow that SMMR achieves promising re-\nsults on the DUC 2007 update corpus.\n', 10, 0)
(72.00098419189453, 418.91815185546875, 154.81466674804688, 434.47186279296875, '1\nIntroduction\n', 11, 0)
(72.00098419189453, 440.67413330078125, 291.6885070800781, 697.70458984375, 'Extensive experiments on query-oriented multi-\ndocument summarization have been carried out\nover the past few years.\nMost of the strategies\nto produce summaries are based on an extrac-\ntion method, which identiﬁes salient textual seg-\nments, most often sentences, in documents. Sen-\ntences containing the most salient concepts are se-\nlected, ordered and assembled according to their\nrelevance to produce summaries (also called ex-\ntracts) (Mani and Maybury, 1999).\nRecently emerged from the Document Under-\nstanding Conference (DUC) 20071, update sum-\nmarization attempts to enhance summarization\nwhen more information about knowledge acquired\nby the user is available. It asks the following ques-\ntion: has the user already read documents on the\ntopic? In the case of a positive answer, producing\nan extract focusing on only new facts is of inter-\nest. In this way, an important issue is introduced:\n', 12, 0)
(72.00101470947266, 702.9140625, 291.68731689453125, 774.6765747070312, 'c⃝ 2008.\nLicensed under the Creative Commons\nAttribution-Noncommercial-Share Alike 3.0 Unported\nli-\ncense\n(http://creativecommons.org/licenses/by-nc-sa/3.0/).\nSome rights reserved.\n1Document Understanding Conferences are conducted\nsince 2000 by the National Institute of Standards and Tech-\nnology (NIST), http://www-nlpir.nist.gov\n', 13, 0)
(305.8590393066406, 257.1300048828125, 525.5465087890625, 775.2225341796875, 'redundancy with previously read documents (his-\ntory) has to be removed from the extract.\nA natural way to go about update summarization\nwould be extracting temporal tags (dates, elapsed\ntimes, temporal expressions...) (Mani and Wilson,\n2000) or to automatically construct the timeline\nfrom documents (Swan and Allan, 2000). These\ntemporal marks could be used to focus extracts on\nthe most recently written facts. However, most re-\ncently written facts are not necessarily new facts.\nMachine Reading (MR) was used by (Hickl et\nal., 2007) to construct knowledge representations\nfrom clusters of documents. Sentences contain-\ning “new” information (i.e. that could not be in-\nferred by any previously considered document)\nare selected to generate summary. However, this\nhighly efﬁcient approach (best system in DUC\n2007 update) requires large linguistic resources.\n(Witte et al., 2007) propose a rule-based system\nbased on fuzzy coreference cluster graphs. Again,\nthis approach requires to manually write the sen-\ntence ranking scheme. Several strategies remain-\ning on post-processing redundancy removal tech-\nniques have been suggested. Extracts constructed\nfrom history were used by (Boudin and Torres-\nMoreno, 2007) to minimize history’s redundancy.\n(Lin et al., 2007) have proposed a modiﬁed Max-\nimal Marginal Relevance (MMR) (Carbonell and\nGoldstein, 1998) re-ranker during sentence selec-\ntion, constructing the summary by incrementally\nre-ranking sentences.\nIn this paper, we propose a scalable sentence\nscoring method for update summarization derived\nfrom MMR. Motivated by the need for relevant\nnovelty, candidate sentences are selected accord-\ning to a combined criterion of query relevance and\ndissimilarity with previously read sentences. The\nrest of the paper is organized as follows. Section 2\n', 14, 0)
(293.3169860839844, 787.5889892578125, 304.2261047363281, 800.7344360351562, '23\n', 15, 0)

page suivante
(72.0009994506836, 72.87799835205078, 291.6884460449219, 113.12246704101562, 'introduces our proposed sentence scoring method\nand Section 3 presents experiments and evaluates\nour approach.\n', 0, 0)
(72.0009994506836, 133.16802978515625, 129.7804718017578, 148.72174072265625, '2\nMethod\n', 1, 0)
(72.0009994506836, 163.2210235595703, 291.6907958984375, 325.4075622558594, 'The underlying idea of our method is that as the\nnumber of sentences in the history increases, the\nlikelihood to have redundant information within\ncandidate sentences also increases.\nWe propose\na scalable sentence scoring method derived from\nMMR that, as the size of the history increases,\ngives more importance to non-redundancy that to\nquery relevance. We deﬁne H to represent the pre-\nviously read documents (history), Q to represent\nthe query and s the candidate sentence. The fol-\nlowing subsections formally deﬁne the similarity\nmeasures and the scalable MMR scoring method.\n', 2, 0)
(72.0009994506836, 345.0773620605469, 254.10665893554688, 372.8191223144531, '2.1\nA query-oriented multi-document\nsummarizer\n', 3, 0)
(72.00096893310547, 382.6990966796875, 291.6910095214844, 775.2225952148438, 'We have ﬁrst started by implementing a simple\nsummarizer for which the task is to produce query-\nfocused summaries from clusters of documents.\nEach document is pre-processed: documents are\nsegmented into sentences, sentences are ﬁltered\n(words which do not carry meaning are removed\nsuch as functional words or common words) and\nnormalized using a lemmas database (i.e. inﬂected\nforms “go”, “goes”, “went”, “gone”... are replaced\nby “go”). An N-dimensional term-space Γ, where\nN is the number of different terms found in the\ncluster, is constructed. Sentences are represented\nin Γ by vectors in which each component is the\nterm frequency within the sentence. Sentence scor-\ning can be seen as a passage retrieval task in Infor-\nmation Retrieval (IR). Each sentence s is scored by\ncomputing a combination of two similarity mea-\nsures between the sentence and the query. The ﬁrst\nmeasure is the well known cosine angle (Salton et\nal., 1975) between the sentence and the query vec-\ntorial representations in Γ (denoted respectively ⃗s\nand ⃗Q). The second similarity measure is based\non the Jaro-Winkler distance (Winkler, 1999). The\noriginal Jaro-Winkler measure, denoted JW, uses\nthe number of matching characters and transposi-\ntions to compute a similarity score between two\nterms, giving more favourable ratings to terms that\nmatch from the beginning. We have extended this\nmeasure to calculate the similarity between the\n', 4, 0)
(305.8589782714844, 72.87818145751953, 427.7486877441406, 86.02365112304688, 'sentence s and the query Q:\n', 5, 0)
(332.1059875488281, 89.06224060058594, 432.46173095703125, 129.77500915527344, 'JWe(s, Q) =\n1\n|Q| ·\nX\n', 6, 0)
(416.4009704589844, 97.27716827392578, 525.5439453125, 128.7235107421875, 'q∈Q\nmax\nm∈S′ JW(q, m)\n(1)\n', 7, 0)
(305.85894775390625, 126.21814727783203, 525.5492553710938, 197.51962280273438, 'where S′ is the term set of s in which the terms\nm that already have maximized JW(q, m) are re-\nmoved. The use of JWe smooths normalization and\nmisspelling errors. Each sentence s is scored using\nthe linear combination:\n', 8, 0)
(317.8019714355469, 204.185302734375, 457.8905944824219, 225.5968780517578, 'Sim1(s, Q) = α · cosine(⃗s, ⃗Q)\n', 9, 0)
(402.73394775390625, 221.09124755859375, 525.5439453125, 242.13490295410156, '+ (1 − α) · JWe(s, Q)\n(2)\n', 10, 0)
(305.8589172363281, 241.76226806640625, 525.5463256835938, 309.5946960449219, 'where α = 0.7, optimally tuned on the past DUCs\ndata (2005 and 2006). The system produces a list\nof ranked sentences from which the summary is\nconstructed by arranging the high scored sentences\nuntil the desired size is reached.\n', 11, 0)
(305.8589172363281, 317.8124694824219, 455.9571838378906, 332.0052185058594, '2.2\nA scalable MMR approach\n', 12, 0)
(305.8589172363281, 335.68121337890625, 525.5462646484375, 430.1217346191406, 'MMR re-ranking algorithm has been successfully\nused in query-oriented summarization (Ye et al.,\n2005). It strives to reduce redundancy while main-\ntaining query relevance in selected sentences. The\nsummary is constructed incrementally from a list\nof ranked sentences, at each iteration the sentence\nwhich maximizes MMR is chosen:\n', 13, 0)
(318.9469299316406, 437.1563415527344, 473.86651611328125, 464.7237243652344, 'MMR = arg max\ns∈S\n[ λ · Sim1(s, Q)\n', 14, 0)
(363.64886474609375, 461.6053466796875, 525.5438232421875, 487.0517272949219, '− (1 − λ) · max\nsj∈E Sim2(s, sj) ]\n(3)\n', 15, 0)
(305.85882568359375, 488.4012451171875, 525.5474853515625, 664.13671875, 'where S is the set of candidates sentences and E\nis the set of selected sentences. λ represents an\ninterpolation coefﬁcient between sentence’s rele-\nvance and non-redundancy. Sim2(s, sj) is a nor-\nmalized Longest Common Substring (LCS) mea-\nsure between sentences s and sj. Detecting sen-\ntence rehearsals, LCS is well adapted for redun-\ndancy removal.\nWe propose an interpretation of MMR to tackle\nthe update summarization issue. Since Sim1 and\nSim2 are ranged in [0, 1], they can be seen as prob-\nabilities even though they are not. Just as rewriting\n(3) as (NR stands for Novelty Relevance):\n', 16, 0)
(316.6788635253906, 671.1703491210938, 459.53350830078125, 698.73876953125, 'NR = arg max\ns∈S\n[ λ · Sim1(s, Q)\n', 17, 0)
(335.6798400878906, 695.619384765625, 525.5438232421875, 721.0667724609375, '+ (1 − λ) · (1 − max\nsh∈H Sim2(s, sh)) ]\n(4)\n', 18, 0)
(305.85882568359375, 721.4292602539062, 525.5462036132812, 775.2227172851562, 'We can understand that (4) equates to an OR com-\nbination. But as we are looking for a more intu-\nitive AND and since the similarities are indepen-\ndent, we have to use the product combination. The\n', 19, 0)
(293.3169860839844, 787.5889892578125, 304.2261047363281, 800.7344360351562, '24\n', 20, 0)

page suivante
(72.0009994506836, 72.87799835205078, 291.6884765625, 140.22048950195312, 'scoring method deﬁned in (2) is modiﬁed into a\ndouble maximization criterion in which the best\nranked sentence will be the most relevant to the\nquery AND the most different to the sentences in\nH.\n', 0, 0)
(78.19200134277344, 151.548095703125, 187.7936553955078, 165.74856567382812, 'SMMR(s) = Sim1(s, Q)\n', 1, 0)
(135.03102111816406, 165.0061492919922, 291.6860046386719, 205.7189178466797, '·\n\x12\n1 − max\nsh∈H Sim2(s, sh)\n\x13f(H)\n(5)\n', 2, 0)
(72.00098419189453, 209.0810089111328, 291.6884765625, 351.5768127441406, 'Decreasing λ in (3) with the length of the sum-\nmary was suggested by (Murray et al., 2005) and\nsuccessfully used in the DUC 2005 by (Hachey\net al., 2005), thereby emphasizing the relevance\nat the outset but increasingly prioritizing redun-\ndancy removal as the process continues.\nSim-\nilarly, we propose to follow this assumption in\nSMMR using a function denoted f that as the\namount of data in history increases, prioritize non-\nredundancy (f(H) → 0).\n', 3, 0)
(72.00096893310547, 353.9190673828125, 155.01788330078125, 369.4727783203125, '3\nExperiments\n', 4, 0)
(72.00096893310547, 376.6060485839844, 291.68841552734375, 443.94854736328125, 'The method described in the previous section has\nbeen implemented and evaluated by using the\nDUC 2007 update corpus2. The following subsec-\ntions present details of the different experiments\nwe have conducted.\n', 5, 0)
(72.0009765625, 453.32232666015625, 234.62294006347656, 467.51507568359375, '3.1\nThe DUC 2007 update corpus\n', 6, 0)
(72.0009765625, 471.4930419921875, 291.68853759765625, 674.3275146484375, 'We used for our experiments the DUC 2007 up-\ndate competition data set. The corpus is composed\nof 10 topics, with 25 documents per topic. The up-\ndate task goal was to produce short (∼100 words)\nmulti-document update summaries of newswire ar-\nticles under the assumption that the user has al-\nready read a set of earlier articles. The purpose\nof each update summary will be to inform the\nreader of new information about a particular topic.\nGiven a DUC topic and its 3 document clusters: A\n(10 documents), B (8 documents) and C (7 doc-\numents), the task is to create from the documents\nthree brief, ﬂuent summaries that contribute to sat-\nisfying the information need expressed in the topic\nstatement.\n', 7, 0)
(80.6559829711914, 684.3060913085938, 263.1870422363281, 697.4515380859375, '1. A summary of documents in cluster A.\n', 8, 0)
(80.6559829711914, 707.4300537109375, 291.6882019042969, 747.6735229492188, '2. An update summary of documents in B, un-\nder the assumption that the reader has already\nread documents in A.\n', 9, 0)
(72.0009994506836, 752.8616943359375, 291.6861877441406, 774.6765747070312, '2More information about the DUC 2007 corpus is avail-\nable at http://duc.nist.gov/.\n', 10, 0)
(314.5140075683594, 72.87805938720703, 525.546142578125, 113.12252807617188, '3. An update summary of documents in C, un-\nder the assumption that the reader has already\nread documents in A and B.\n', 11, 0)
(305.8590087890625, 121.99304962158203, 525.54638671875, 189.33554077148438, 'Within a topic, the document clusters must be pro-\ncessed in chronological order. Our system gener-\nates a summary for each cluster by arranging the\nhigh ranked sentences until the limit of 100 words\nis reached.\n', 12, 0)
(305.8590087890625, 198.08433532714844, 381.21905517578125, 212.27706909179688, '3.2\nEvaluation\n', 13, 0)
(305.8590087890625, 215.9530792236328, 525.54638671875, 310.3945617675781, 'Most existing automated evaluation methods work\nby comparing the generated summaries to one or\nmore reference summaries (ideally, produced by\nhumans). To evaluate the quality of our generated\nsummaries, we choose to use the ROUGE3 (Lin,\n2004) evaluation toolkit, that has been found to be\nhighly correlated with human judgments. ROUGE-\n', 14, 0)
(305.8589782714844, 310.798095703125, 525.5463256835938, 364.5906066894531, 'N is a n-gram recall measure calculated between\na candidate summary and a set of reference sum-\nmaries. In our experiments ROUGE-1, ROUGE-2\nand ROUGE-SU4 will be computed.\n', 15, 0)
(305.8590087890625, 373.3393859863281, 364.3426818847656, 387.5321350097656, '3.3\nResults\n', 16, 0)
(305.8590087890625, 391.2091064453125, 525.5463256835938, 499.1986389160156, 'Table 1 reports the results obtained on the DUC\n2007 update data set for different sentence scor-\ning methods. cosine + JWe stands for the scor-\ning method deﬁned in (2) and NR improves it\nwith sentence re-ranking deﬁned in equation (4).\nSMMR is the combined adaptation we have pro-\nposed in (5). The function f(H) used in SMMR is\nthe simple rational function 1\n', 17, 0)
(305.8590393066406, 486.05303955078125, 525.5447998046875, 526.2974853515625, 'H , where H increases\nwith the number of previous clusters (f(H) = 1\nfor cluster A, 1\n', 18, 0)
(368.6679992675781, 512.9594116210938, 459.70111083984375, 530.0594482421875, '2 for cluster B and 1\n', 19, 0)
(305.8589782714844, 513.1520385742188, 525.548828125, 757.1965942382812, '3 for cluster C).\nThis function allows to simply test the assumption\nthat non-redundancy have to be favoured as the\nsize of history grows. Baseline results are obtained\non summaries generated by taking the leading sen-\ntences of the most recent documents of the cluster,\nup to 100 words (ofﬁcial baseline of DUC). The\ntable also lists the three top performing systems at\nDUC 2007 and the lowest scored human reference.\nAs we can see from these results, SMMR out-\nperforms the other sentence scoring methods. By\nways of comparison our system would have been\nranked second at the DUC 2007 update competi-\ntion. Moreover, no post-processing was applied to\nthe selected sentences leaving an important margin\nof progress. Another interesting result is the high\nperformance of the non-update speciﬁc method\n(cosine + JWe) that could be due to the small size\n', 20, 0)
(318.5119934082031, 762.82470703125, 510.1763610839844, 774.6765747070312, '3ROUGE is available at http://haydn.isi.edu/ROUGE/.\n', 21, 0)
(293.3169860839844, 787.5889892578125, 304.2261047363281, 800.7344360351562, '25\n', 22, 0)

page suivante
(72.0009994506836, 72.87799835205078, 291.68841552734375, 86.02346801757812, 'of the corpus (little redundancy between clusters).\n', 0, 0)
(149.77700805664062, 111.21205139160156, 295.6011962890625, 122.01657104492188, 'ROUGE-1\nROUGE-2\nROUGE-SU4\n', 1, 0)
(77.97899627685547, 125.75605010986328, 290.4466247558594, 166.01852416992188, 'Baseline\n0.26232\n0.04543\n0.08247\n3rd system\n0.35715\n0.09622\n0.13245\n2nd system\n0.36965\n0.09851\n0.13509\n', 2, 0)
(77.97899627685547, 166.42201232910156, 290.4466247558594, 193.11648559570312, 'cosine + JWe\n0.35905\n0.10161\n0.13701\nNR\n0.36207\n0.10042\n0.13781\n', 3, 0)
(80.97899627685547, 193.5262908935547, 291.8106384277344, 207.71902465820312, 'SMMR\n0.36323\n0.10223\n0.13886\n', 4, 0)
(77.97899627685547, 203.9080352783203, 290.4466247558594, 234.56149291992188, '1st system\n0.37032\n0.11189\n0.14306\nWorst human\n0.40497\n0.10511\n0.14779\n', 5, 0)
(72.0009994506836, 248.8600616455078, 291.6890563964844, 275.5555114746094, 'Table 1: ROUGE average recall scores computed\non the DUC 2007 update corpus.\n', 6, 0)
(72.0009994506836, 300.60809326171875, 235.9546356201172, 316.16180419921875, '4\nDiscussion and Future Work\n', 7, 0)
(72.0009994506836, 323.6050720214844, 291.689697265625, 539.9876708984375, 'In this paper we have described SMMR, a scal-\nable sentence scoring method based on MMR that\nachieves very promising results. An important as-\npect of our sentence scoring method is that it does\nnot requires re-ranking nor linguistic knowledge,\nwhich makes it a simple and fast approach to the\nissue of update summarization. It was pointed out\nat the DUC 2007 workshop that Question Answer-\ning and query-oriented summarization have been\nconverging on a common task. The value added\nby summarization lies in the linguistic quality. Ap-\nproaches mixing IR techniques are well suited for\nquery-oriented summarization but they require in-\ntensive work for making the summary ﬂuent and\ncoherent. Among the others, this is a point that we\nthink is worthy of further investigation.\n', 8, 0)
(72.0009994506836, 550.1702270507812, 165.5265350341797, 565.7239379882812, 'Acknowledgments\n', 9, 0)
(72.0009994506836, 573.1671752929688, 291.6913146972656, 599.8616333007812, 'This work was supported by the Agence Nationale\nde la Recherche, France, project RPM2.\n', 10, 0)
(72.0009994506836, 622.6821899414062, 127.54483795166016, 638.2359008789062, 'References\n', 11, 0)
(72.0009994506836, 643.2877807617188, 291.6895446777344, 699.1287231445312, 'Boudin, F. and J.M. Torres-Moreno.\n2007.\nA Co-\nsine Maximization-Minimization approach for User-\nOriented Multi-Document Update Summarization.\nIn Recent Advances in Natural Language Processing\n(RANLP), pages 81–87.\n', 12, 0)
(72.0009994506836, 708.15673828125, 291.6896667480469, 774.9567260742188, 'Carbonell, J. and J. Goldstein. 1998. The use of MMR,\ndiversity-based reranking for reordering documents\nand producing summaries. In 21st annual interna-\ntional ACM SIGIR conference on Research and de-\nvelopment in information retrieval, pages 335–336.\nACM Press New York, NY, USA.\n', 13, 0)
(305.8590087890625, 73.7528076171875, 525.5442504882812, 129.5936737060547, 'Hachey, B., G. Murray, and D. Reitter.\n2005.\nThe\nEmbra System at DUC 2005: Query-oriented Multi-\ndocument Summarization with a Very Large Latent\nSemantic Space. In Document Understanding Con-\nference (DUC).\n', 14, 0)
(305.8590087890625, 137.51373291015625, 525.5455322265625, 182.39561462402344, 'Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC’s\nGISTexter at DUC 2007: Machine Reading for Up-\ndate Summarization.\nIn Document Understanding\nConference (DUC).\n', 15, 0)
(305.8590087890625, 190.315673828125, 525.5444946289062, 235.1975555419922, 'Lin, Z., T.S. Chua, M.Y. Kan, W.S. Lee, L. Qiu, and\nS. Ye.\n2007.\nNUS at DUC 2007: Using Evolu-\ntionary Models of Text. In Document Understanding\nConference (DUC).\n', 16, 0)
(305.8590087890625, 243.11761474609375, 525.544189453125, 277.04052734375, 'Lin, C.Y.\n2004.\nRouge: A Package for Automatic\nEvaluation of Summaries. In Workshop on Text Sum-\nmarization Branches Out, pages 25–26.\n', 17, 0)
(305.8590087890625, 284.9605712890625, 525.5478515625, 307.92449951171875, 'Mani, I. and M.T. Maybury. 1999. Advances in Auto-\nmatic Text Summarization. MIT Press.\n', 18, 0)
(305.8589782714844, 315.84454345703125, 525.5462646484375, 371.6854248046875, 'Mani, I. and G. Wilson. 2000. Robust temporal pro-\ncessing of news. In 38th Annual Meeting on Asso-\nciation for Computational Linguistics, pages 69–76.\nAssociation for Computational Linguistics Morris-\ntown, NJ, USA.\n', 19, 0)
(305.8589782714844, 379.60546875, 525.544189453125, 424.48736572265625, 'Murray, G., S. Renals, and J. Carletta. 2005. Extractive\nSummarization of Meeting Recordings. In Ninth Eu-\nropean Conference on Speech Communication and\nTechnology. ISCA.\n', 20, 0)
(305.8589782714844, 432.40740966796875, 525.5443115234375, 466.330322265625, 'Salton, G., A. Wong, and C. S. Yang. 1975. A vector\nspace model for automatic indexing. Communica-\ntions of the ACM, 18(11):613–620.\n', 21, 0)
(305.85894775390625, 474.2503662109375, 525.544189453125, 519.1322631835938, 'Swan, R. and J. Allan. 2000. Automatic generation\nof overview timelines. In 23rd annual international\nACM SIGIR conference on Research and develop-\nment in information retrieval, pages 49–56.\n', 22, 0)
(305.8589172363281, 527.0523071289062, 525.544189453125, 560.9752197265625, 'Winkler, W. E. 1999. The state of record linkage and\ncurrent research problems. In Survey Methods Sec-\ntion, pages 73–79.\n', 23, 0)
(305.8589172363281, 568.895263671875, 525.544189453125, 602.8181762695312, 'Witte, R., R. Krestel, and S. Bergler. 2007. Generat-\ning Update Summaries for DUC 2007. In Document\nUnderstanding Conference (DUC).\n', 24, 0)
(305.8589172363281, 610.73828125, 525.5479125976562, 655.6201782226562, 'Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUS\nat DUC 2005: Understanding documents via con-\ncept links. In Document Understanding Conference\n(DUC).\n', 25, 0)
(293.3169860839844, 787.5889892578125, 304.2261047363281, 800.7344360351562, '26\n', 26, 0)

page suivante
