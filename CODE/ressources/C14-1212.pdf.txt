(89.77999877929688, 804.5470581054688, 503.2516174316406, 823.801513671875, 'Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,\npages 2249–2259, Dublin, Ireland, August 23-29 2014.\n', 0, 0)
(129.4029998779297, 67.91966247558594, 468.14556884765625, 86.58406829833984, 'Learning to Distinguish Hypernyms and Co-Hyponyms\n', 1, 0)
(108.68598175048828, 116.77801513671875, 488.8616638183594, 186.87940979003906, 'Julie Weeds, Daoud Clarke, Jeremy Refﬁn, David Weir and Bill Keller\nDepartment of Informatics,\nUniversity of Sussex,\nBrighton, UK\njuliewe,D.Clarke,J.P.Reffin,davidw,billk@sussex.ac.uk\n', 2, 0)
(276.5299987792969, 249.13201904296875, 321.0152893066406, 264.68572998046875, 'Abstract\n', 3, 0)
(89.00899505615234, 275.5570068359375, 508.5401611328125, 356.4485168457031, 'This work is concerned with distinguishing different semantic relations which exist between\ndistributionally similar words. We compare a novel approach based on training a linear Support\nVector Machine on pairs of feature vectors with state-of-the-art methods based on distributional\nsimilarity. We show that the new supervised approach does better even when there is minimal\ninformation about the target words in the training data, giving a 15% reduction in error rate over\nunsupervised approaches.\n', 4, 0)
(72.0009994506836, 367.33306884765625, 154.81468200683594, 382.88677978515625, '1\nIntroduction\n', 5, 0)
(72.0009994506836, 390.47802734375, 525.546875, 715.9295043945312, 'Over recent years there has been much interest in the ﬁeld of distributional semantics, drawing on the\ndistributional hypothesis: words that occur in similar contexts tend to have similar meanings (Harris,\n1954). There is a large body of work on the use of different similarity measures (Lee, 1999; Weeds and\nWeir, 2003; Curran, 2004) and many researchers have built thesauri (i.e. lists of “nearest neighbours”)\nautomatically and applied them in a variety of applications, generally with a good deal of success.\nIn early research there was much interest in how these automatically generated thesauri compare with\nhuman-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000).\nMore recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Dis-\ntributional thesauri have been used in a wide variety of areas including sentiment classiﬁcation (Bollegala\net al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), pre-\ndicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh,\n2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), tax-\nonomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013).\nA primary focus of distributional semantics has been on identifying words which are similar to each\nother. However, semantic similarity encompasses a variety of different lexico-semantic and topical re-\nlations. Even if we just consider nouns, an automatically generated thesaurus will tend to return a mix\nof synonyms, antonyms, hyponyms, hypernyms, co-hyponyms, meronyms and other topically related\nwords. A central problem here is that whilst most measures of distributional similarity are symmetric,\nsome of the important semantic relations are not. The hyponymy relation (and converse hypernymy)\nwhich forms the ISA backbone of taxonomies and ontologies such as WordNet (Fellbaum, 1989), and\ndetermines lexical entailment (Geffet and Dagan, 2005), is asymmetric. On the other hand, the co-\nhyponymy relation which relates two words unrelated by hyponymy but sharing a (close) hypernym, is\nsymmetric, as are synonymy and antonymy. Table 1 shows the distributionally nearest neighbours of the\nwords cat, animal and dog. In the list for cat we can see 2 hypernyms and 13 co-hyponyms1.\n', 6, 0)
(72.0009994506836, 724.09375, 525.5486450195312, 765.4845581054688, '1We read cat in the sense domestic cat rather than big cat, hence tiger is a co-hyponym rather than hyponym\nof cat.\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings\nfooter are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/\n', 7, 0)
(287.8630065917969, 786.9010009765625, 309.6812438964844, 800.0464477539062, '2249\n', 8, 0)

page suivante
(77.97899627685547, 62.61005401611328, 516.479248046875, 89.30551147460938, 'cat\ndog 0.32, animal 0.29, rabbit 0.27, bird 0.26, bear 0.26, monkey 0.26, mouse 0.25, pig 0.25,\nsnake 0.24, horse 0.24, rat 0.24, elephant 0.23, tiger 0.23, deer 0.23, creature 0.23\n', 0, 0)
(77.97899627685547, 90.10700225830078, 516.479248046875, 116.80148315429688, 'animal\nbird 0.36, ﬁsh 0.34, creature 0.33, dog 0.31, horse 0.30, insect 0.30, species 0.29, cat 0.29,\nhuman 0.28, mammal, 0.28, cattle 0.27, snake 0.27, pig 0.26, rabbit 0.26, elephant 0.25\n', 1, 0)
(77.97899627685547, 117.60401153564453, 516.4791259765625, 144.29849243164062, 'dog\ncat 0.32, animal 0.31, horse 0.29, bird 0.26, rabbit 0.26, pig 0.25, bear 0.26, man 0.25, ﬁsh\n0.24, boy 0.24, creature 0.24, monkey 0.24, snake 0.24, mouse 0.24, rat 0.23\n', 2, 0)
(72.0009994506836, 154.01502990722656, 525.5419311523438, 180.71047973632812, 'Table 1: Top 15 neighbours of cat, animal and dog generated using Lin’s similarity measure (Lin,\n1998) considering all words and dependency features occurring 100 or more times in Wikipedia.\n', 3, 0)
(72.00102996826172, 201.3460235595703, 525.5466918945312, 255.94149780273438, 'Distributional similarity is being deployed (e.g., Dinu and Thater (2012)) in situations where it can\nbe useful to be able to distinguish between these different relationships. Consider the following two\nsentences.\nThe cat ran across the road.\n(1)\n', 4, 0)
(229.04205322265625, 264.55999755859375, 525.5440673828125, 277.7054748535156, 'The animal ran across the road.\n(2)\n', 5, 0)
(72.00106048583984, 285.52099609375, 525.546875, 638.1995239257812, 'Sentence 1 textually entails sentence 2, but sentence 2 does not textually entail sentence 1. The ability\nto determine whether entailment holds between the sentences, and in which direction, depends on the\nability to identify hyponymy. Given a similarity score of 0.29 between cat and animal, how do we\nknow which is the hyponym and which is the hypernym?\nIn applying distributional semantics to the problem of textual entailment, there is a need to generalise\nlexical entailment to phrases and sentences. Thus, the ability to distinguish different semantic relations\nis crucial if approaches to the composition of distributional representations of meaning that are currently\nreceiving considerable interest (Widdows, 2008; Mitchell and Lapata, 2008; Baroni and Zamparelli,\n2010; Grefenstette et al., 2011; Socher et al., 2012; Weeds et al., 2014) are to be applied to the textual\nentailment problem.\nWe formulate the challenge as follows: Consider a set of pairs of similar words ⟨A, B⟩ where one of\nthree relationships hold between A and B: A lexically entails B, B lexically entails A or A and B are\nrelated by co-hyponymy. Given such a set, how can we determine which relationship holds? In Section\n2, we discuss existing attempts to address this problem through the use of various directional measures\nof distributional similarity.\nThis paper considers the effectiveness of various supervised approaches, and makes the following\ncontributions. First, we show that a SVM can distinguish the entailment and co-hyponymy relations,\nachieving a signiﬁcant reduction in error rate in comparison to existing state-of-the-art methods based\non the notion of distributional generality. Second, by comparing two different data sets, one built from\nBLESS (Baroni and Lenci, 2011) and the other from WordNet (Fellbaum, 1989), we derive important\ninsights into the requirements of a valid evaluation of supervised approaches, and provide a data set\nfor further research in this area. Third, we show that when learning how to determine an ontological\nrelationship between a pair of similar words by means of the word’s distributional vectors, quite different\nvector operations are useful when identifying different ontological relationships. In particular, using the\ndifference between the vectors for pairs of words is appropriate for the entailment task, whereas adding\nthe vectors works well for the co-hyponym task.\n', 6, 0)
(72.00109100341797, 648.34912109375, 161.09124755859375, 663.90283203125, '2\nRelated Work\n', 7, 0)
(72.00109100341797, 671.3220825195312, 525.5468139648438, 766.030517578125, 'Lee (1999) noted that the substitutability of one word for another was asymmetric and proposed the\nalpha-skew divergence measure, an asymmetric version of the Kullback-Leibler divergence measure. She\nfound that this measure improved results in language modelling, when a word’s distribution is smoothed\nusing the distributions of its nearest neighbours.\nWeeds et al. (2004) proposed a notion of distributional generality, observing that more general words\ntend to occur in a larger variety of contexts than more speciﬁc words. For example, we would expect to be\nable to replace any occurrence of cat with animal and so all of the contexts of cat must be plausible\n', 8, 0)
(287.8630065917969, 786.9010009765625, 309.6812438964844, 800.0464477539062, '2250\n', 9, 0)

page suivante
(72.00096893310547, 63.68604278564453, 525.5465087890625, 117.47952270507812, 'contexts for animal. However, not all of the contexts of animal would be plausible for cat, e.g.,\n“the monstrous animal barked at the intruder”. Weeds et al. (2004) attempt to capture this asymmetry\nby framing word similarity in terms of co-occurrence retrieval (Weeds and Weir, 2003), where precision\nand recall are deﬁned as:\n', 0, 0)
(133.61798095703125, 124.18817901611328, 286.6176452636719, 146.72262573242188, 'Pww(u, v) = Σf∈F(u)∩F(v)I(u, f)\n', 1, 0)
(206.28900146484375, 124.18817901611328, 462.732666015625, 160.407470703125, 'Σf∈F(u)I(u, f)\nand Rww(u, v) = Σf∈F(u)∩F(v)I(v, f)\n', 2, 0)
(383.2170104980469, 140.005126953125, 450.89764404296875, 160.407470703125, 'Σf∈F(v)I(v, f)\n', 3, 0)
(72.00096893310547, 164.4171142578125, 525.5475463867188, 272.9385681152344, 'where I(n, f) is the pointwise mutual information (PMI) between noun n and feature f and F(n) is the\nset of all features f for which I(n, f) > 0.\nBy comparing the precision and recall of one word’s retrieval of another word’s contexts, they were\nable to successfully identify the direction of an entailment relation in 71% of pairs drawn from WordNet.\nHowever, this was not signiﬁcantly better than a baseline which proposed that the most frequent word\nwas the most general.\nClarke (2009) formalised the idea of distributional generality using a partially ordered vector space.\nHe also argued for using a variation of co-occurrence retrieval where precision and recall are deﬁned as:\n', 4, 0)
(75.22998809814453, 281.8831787109375, 286.3363342285156, 304.5586242675781, 'Pcl(u, v) = Σf∈F(u)∩F(v)min(I(u, f), I(v, f))\n', 5, 0)
(173.80801391601562, 281.88311767578125, 521.122314453125, 318.10345458984375, 'Σf∈F(u)I(u, f)\nand Rcl(u, v) = Σf∈F(u)∩F(v)min(I(u, f), I(v, f))\n', 6, 0)
(409.1230163574219, 297.70111083984375, 476.8036804199219, 318.10345458984375, 'Σf∈F(v)I(v, f)\n', 7, 0)
(72.00102996826172, 321.88201904296875, 525.5467529296875, 375.6755065917969, 'Lenci and Benotto (2012) took the notion further and hypothesised that more general terms should\nhave high recall and low precision, which would thus make it possible to distinguish them from other\nrelated terms such as synonyms and co-hyponyms. They proposed a variant of the Clarke (2009) measure\nto identify hypernyms:\n', 8, 0)
(199.1080322265625, 379.58990478515625, 281.4836120605469, 416.77032470703125, 'invCL(u, v) =\n2q\n', 9, 0)
(281.4830017089844, 388.9911193847656, 398.4393005371094, 410.0347900390625, 'Pcl(u, v) ∗ (1 − Rcl(u, v))\n', 10, 0)
(72.00096893310547, 414.0520324707031, 525.546875, 766.030517578125, 'Evaluation on the BLESS data set (Baroni and Lenci, 2011), showed that this measure is better at distin-\nguishing hypernyms from other relations than the measures of Weeds et al. (2004) and Clarke (2009).\nGeffet and Dagan (2005) proposed an approach based on feature inclusion, which extends the rationale\nof Weeds et al. (2004) to lexical entailment. Using data from the web they demonstrated a strong cor-\nrelation between complete inclusion of prominent features and lexical entailment. However, they were\nunable to assess this using an off-line corpus due to data sparseness.\nSzpektor and Dagan (2008) found that the Pww measure tends to promote relationships between infre-\nquent words with narrow vectors (i.e. those with relatively few distinct context features). They proposed\nusing the geometric average of Pww and the symmetric similarity measure of Lin (1998) in order to\npenalise low frequency words.\nKotlerman et al. (2010) apply the IR evaluation method of Average Precision to the problem of identi-\nfying lexical inference and use the balancing approach of Szpektor and Dagan (2008) to demote similar-\nities for narrow feature vectors; their measure is called balAPinc. They show that all of the asymmetric\nsimilarity measures previously proposed perform much better than symmetric similarity measures on\na directionality detection experiment, and that their method and that of Clarke (2009) outperform the\nothers with statistical signiﬁcance. They also show that their measure is superior when used for term\nexpansion in an event detection task.\nBaroni et al. (2012) investigate the relation between phrasal and lexical entailment, and demonstrate\nthat support vector machines can generalise entailment relations between quantiﬁer phrases to entailment\ninvolving unseen quantiﬁers. They compare the performance of their system with the balAPinc measure.\nThe Stanford WordNet project (Snow et al., 2004) expands the WordNet taxonomy by analysing large\ncorpora to ﬁnd patterns that are indicative of hyponymy. For example, the pattern “NPX and other NPY ”\nis an indication that NPX is a NPY , i.e. that NPX is a hyponym of NPY . They use machine learning\nto identify other such patterns from known hyponym-hypernym pairs, and then use these patterns to ﬁnd\nnew relations in the corpus. The transitivity relation of the taxonomy is enforced by searching only over\nvalid taxonomies and evaluating the likelihood of each taxonomy given the available evidence (Snow\n', 11, 0)
(287.8630065917969, 786.9010009765625, 309.6812438964844, 800.0464477539062, '2251\n', 12, 0)

page suivante
(72.00096893310547, 63.68604278564453, 525.5493774414062, 266.9425964355469, 'et al., 2006). The approach is similar to ours in providing a supervised method of learning semantic\nrelations, but relies on having features for occurrences of pairs of terms rather than just vectors for terms\nthemselves. Our approach is therefore more generally applicable to systems which compose distribu-\ntional representations of meaning.\nMost recently, Rei and Briscoe (2013) note that hyponyms are well suited for lexical substitution.\nIn their experiments with smoothing edge scores for parser lexicalisation, they ﬁnd that a directional\nsimilarity measure, WeightedCosine2, performs best. Also of note, Mikolov et al. (2013) propose a vector\noffset method to capture syntactic and semantic regularities between word representations learnt by a\nrecurrent neural network language model. Yih et al. (2012) present a method for distinguishing synonyms\nand antonyms by inducing polarity in a document-term matrix before applying Latent Semantic Analysis.\nSantus et al. (2014) propose identifying hypernyms using a new measure based on entropy, SLQS, which\nis based on the hypothesis that the most typical linguistic contexts of a hypernym are less informative\nthan the most typical linguistic contexts of its hyponyms. Evaluated on pairs extracted from the BLESS\ndataset (Baroni and Lenci, 2011), this measure outperforms Pww at both discriminating hypernym test\npairs from other types of relation and at determining the direction of the entailment relation.\n', 0, 0)
(72.00096893310547, 277.62615966796875, 157.014404296875, 293.17987060546875, '3\nMethodology\n', 1, 0)
(72.00096893310547, 299.73876953125, 467.968017578125, 314.1266174316406, 'The code used to perform our experiments has been open sourced, and is available online.3\n', 2, 0)
(72.00096893310547, 324.4353942871094, 204.6229248046875, 338.6281433105469, '3.1\nVector Representations\n', 3, 0)
(72.00096893310547, 343.1421203613281, 525.5482788085938, 723.3806762695312, 'Distributional information was collected for all of the nouns from Wikipedia provided they had oc-\ncurred 100 or more times. We used a Wikimedia dump of Wikipedia from June 2011 and extracted\ntext using wp2txt4. This was part-of-speech tagged, lemmatised and dependency parsed using the Malt\nParser (Nivre, 2004). All major grammatical dependency relations involving open class parts of speech\n(nsubj, dobj, iobj, conj, amod, nnmod) and also occurring 100 or more times were ex-\ntracted as features of the POS-tagged and lemmatised nouns. The value of each feature is the positive\npoint wise mutual information (PPMI) (Church and Hanks, 1989) between the noun and the feature. The\ntotal number of noun vectors which can be harvested from Wikipedia with these parameters is 124, 345.\nOur goal is to build classiﬁers that establish whether or not a given semantic relation, rel, holds be-\ntween two similar words A and B. Support vector machines (SVMs), which are effective across a variety\nof classiﬁcation scenarios, learn a boundary between two classes from a set of positive and negative ex-\nample vectors. The two classes correspond to the relation rel holding or not holding. Here, however, we\ndo not start with a single vector, but with two distributional vectors vA and vB for the words A and B,\nrespectively. These vectors must be combined in some way to produce the SVM’s input, and a number\nof ways were considered, deﬁned in Table 2. Of these operations, the vector difference (used by svm-\nDIFF and knnDIFF) and direct sum (used by svmCAT) are asymmetric, whereas the sum and pointwise\nmultiplication (used by svmADD and svmMULT) are symmetric.\nWe now motivate the use of each of these operations. First, we note that pointwise multiplication\n(svmMULT) is intersective. Similar vectors will have a large intersection and it might be possible to\nlearn the features that nouns occurring in different semantic relations should share. However, it does\nnot retain any information about non-shared features and it is symmetric so it is difﬁcult to see how it\nwould be possible to use it to distinguish hypernyms from hyponyms. Pointwise addition (svmADD)\neffectively performs the union of the features, giving emphasis to the shared features. Whilst it does\nretain information about the non-shared features, it is also symmetric, making it difﬁcult again to see\nhow it would be useful in determining the direction of an entailment relation\nVector difference (as used in svmDIFF and knnDIFF), on the other hand, is asymmetric. Further,\nwe might expect a small difference vector (containing many zeroes) to be indicative of similar nouns.\nFurther, considering the majority sign of features in this difference vector might indicate the direction of\n', 4, 0)
(84.65399932861328, 731.8837280273438, 357.13214111328125, 765.090087890625, '2The details of this measure are unpublished.\n3https://github.com/SussexCompSem/learninghypernyms\n4https://github.com/yohasebe/wp2txt\n', 5, 0)
(287.8630065917969, 786.9010009765625, 309.6812438964844, 800.0464477539062, '2252\n', 6, 0)

page suivante
(72.0009994506836, 63.68604278564453, 525.5468139648438, 351.0885925292969, 'entailment. Using an SVM, we might expect to be able to effectively learn which of these features should\nbe ignored and which should be combined, to decide the correct direction of entailment in the majority\nnumber of cases in our training data. However, note that if one uses vector difference it is impossible to\ndistinguish between the case where a feature occurred with both nouns (to the same extent) and the case\nwhere a feature occurs with neither noun. Accordingly, a small difference vector may indicate that both\nnouns do not occur in many distinct contexts. A possible solution to this problem is to use the direct\nsum of the vectors (i.e., the concatenation of the two vectors) which retains all of the information from\nthe original vectors. Finally, we consider the use of the single vector corresponding to the second word\n(svmSING) as a baseline. High performance by this operation would indicate that we can learn features\nof words which tend to be hypernyms (or co-hyponyms) without any regard to the other word in the\nputative relationship.\nWe also note that the behaviour of these methods may differ depending on the weighting used for vec-\ntors. For example, PMI is the log of a ratio of probabilities and therefore one might expect vector addition\nwhere vectors are weighted using PMI to correspond to multiplication where vectors are weighted using\nfrequency or probability. However, the use of positive PMI (where negative PMI scores are regarded\nequal to zero), which is consistent with other work in this area, means that this correspondence is lost.\nBecause of the nature of our datasets, we were concerned that systems could learn information about\nthe taxonomy from the relations in the training data, without making use of information in the vectors\nthemselves. To investigate this, we constructed random vectors to be used in place of the vectors derived\nfrom Wikipedia. The dimensionality of the random vectors was chosen to be 1000 since this substantially\nexceeds the average number (398) of non-zero features in the Wikipedia vectors.\n', 0, 0)
(72.00102996826172, 365.6033935546875, 144.4265594482422, 379.796142578125, '3.2\nClassiﬁers\n', 1, 0)
(72.0009994506836, 386.72113037109375, 525.5468139648438, 509.8966369628906, 'We constructed linear SVMs for each of the vector operations outlined in Section 3.1. We used linear\nSVMs for speed and simplicity, since the point is to compare the different vector representations of\nthe pairings. For comparison, we also constructed a number of supervised, unsupervised, and weakly\nsupervised classiﬁers. These are listed in Table 2. For the linear SVMs and kNN classiﬁer, we used the\nscikit-learn implementations with default settings. For k nearest neighbours, we performed a parameter\nsearch, using nested cross-validation, varying k between 1 and 50.\nFor weakly supervised approaches, we evaluated the measure on the training set, then found the best\nthreshold p on the training set that best divides the two classes using that measure. When classifying, we\ndetermine that the relation holds if the value of the measure exceeds p.\n', 2, 0)
(77.97899627685547, 523.9830322265625, 381.3601989746094, 546.2237548828125, 'svmDIFF\nA linear SVM trained on the vector difference vB − vA\n', 3, 0)
(77.97899627685547, 537.5320434570312, 412.8681945800781, 559.7727661132812, 'svmMULT\nA linear SVM trained on the pointwise product vector vB ∗ vA\n', 4, 0)
(77.97899627685547, 550.590087890625, 355.60418701171875, 565.5773315429688, 'svmADD\nA linear SVM trained on the vector sum vB + vA\n', 5, 0)
(77.97899627685547, 564.6300659179688, 398.0081787109375, 586.8717651367188, 'svmCAT\nA linear SVM trained on the vector concatenation vB ⊕ vA\n', 6, 0)
(77.97899627685547, 578.1790161132812, 308.83203125, 591.9405517578125, 'svmSING\nA linear SVM trained on the vector vB\n', 7, 0)
(77.97899627685547, 591.2371215820312, 491.4590759277344, 613.9697875976562, 'knnDIFF\nk nearest neighbours (knn) trained on the vector difference vB − vA.1 < k < 50\n', 8, 0)
(77.97899627685547, 605.1851196289062, 527.9058227539062, 626.228759765625, 'widthdiff\nwidth(B) > width(A) → rel(A, B) where width(A) is number of non-zero features in A\n', 9, 0)
(77.97899627685547, 618.734130859375, 260.70867919921875, 639.7777709960938, 'singlewidth\nwidth(B) > p → rel(A, B)\n', 10, 0)
(77.97899627685547, 632.2841186523438, 512.0264282226562, 653.3277587890625, 'cosineP\nsimcos(A, B) > p → rel(A, B) where simcos(A, B) is cosine similarity using PPMI\n', 11, 0)
(77.97899627685547, 645.8331298828125, 329.0759582519531, 666.8767700195312, 'linP\nsimlin(A, B) > p → rel(A, B) (Lin, 1998)\n', 12, 0)
(77.97899627685547, 659.382080078125, 405.7945861816406, 680.4257202148438, 'CRdiff\nPww(A, B) > Rww(A, B) → rel(A, B) (Weeds et al., 2004)\n', 13, 0)
(77.97899627685547, 672.93115234375, 369.438720703125, 693.9747924804688, 'clarkediff\nPcl(A, B) > Rcl(A, B) → rel(A, B) (Clarke, 2009)\n', 14, 0)
(77.97899627685547, 686.4801025390625, 393.6175842285156, 707.5237426757812, 'invCLP\ninvCL(A, B) > p → rel(A, B) (Lenci and Benotto, 2012)\n', 15, 0)
(77.97899627685547, 700.0291137695312, 397.5660400390625, 721.07275390625, 'balAPincP\nbalAPinc(A, B) > p → rel(A, B) (Kotlerman et al., 2010)\n', 16, 0)
(77.97899627685547, 714.0700073242188, 462.64202880859375, 727.2154541015625, 'most freq\nThe most frequent label in the training data is assigned to every test point.\n', 17, 0)
(227.83200073242188, 736.9320068359375, 369.7156677246094, 750.0774536132812, 'Table 2: Implemented classiﬁers\n', 18, 0)
(287.8630065917969, 786.9010009765625, 309.6812438964844, 800.0464477539062, '2253\n', 19, 0)

page suivante
(72.0009994506836, 63.29331588745117, 140.47743225097656, 77.48605346679688, '3.3\nData Sets\n', 0, 0)
(72.0009994506836, 84.56006622314453, 525.5487060546875, 472.0877380371094, 'One of key the challenges of this work has been to construct a data set which accurately and validly tests\nour hypotheses. All four of our datasets detailed below are available online 5.\nIn order to test our hypotheses, a data set needs to be balanced in many respects in order to prevent the\nsupervised classiﬁers making use of artefacts of the data. This would not only make it unfair to compare\nthe supervised approaches with the unsupervised approaches, but also make it unlikely that our results\nwould be generalisable to other data. Here, we outline the requirements for the data sets, the importance\nof which is demonstrated by our initial results for a data set which does not satisfy all of them.\nThere should be an equal number of positive and negative examples of a semantic relation. Thus,\nrandom guessing or labelling with the most frequently seen label in the training data will yield 50%\naccuracy and precision. An advantage of incorporating this requirement means that evaluation can be in\nterms of simple accuracy (or error rate).\nIt should not be possible to do well simply by considering the distributional similarity of the terms.\nHence, the negative examples need to be pairs of equally similar words, but where the relationship under\nconsideration does not hold.\nIt should not be possible to do well by pre-supposing an entailment relation and guessing the direction.\nFor example, it has been shown (Weeds et al., 2004) that given a pair of entailing words selected from\nWordNet, over 70% of the time the more frequent word is also the entailed word.\nIt should not be possible to do well using ontological information learnt about one or both of the\nwords from the training data that is not generalisable to their distributional representations. For example,\nit should not be possible for the classiﬁer simply to learn directly from the training pairs ⟨cat ISA\nmammal⟩ and ⟨mammal ISA animal⟩ that ⟨cat ISA animal⟩. Furthermore, we must ensure that\na classiﬁer cannot learn that a particular word is near the top of the ontological hierarchy, and, as a\nresult, do well by guessing that a particular pairing probably has an entailment relation. For example,\ngiven many pairs such as ⟨cat ISA animal⟩, ⟨dog ISA animal⟩, a system which guessed ⟨rabbit\nISA animal⟩ but not ⟨animal ISA rabbit⟩ would do better than random guessing. Whilst both\nof these types of information could be useful in a hybrid system, they do not require any distributional\ninformation and therefore we would not be learning anything about the distributional features of animal\nwhich make it likely to be a hypernym.\n', 1, 0)
(72.00106048583984, 485.3885192871094, 138.68838500976562, 499.5812683105469, '3.3.1\nBLESS\n', 2, 0)
(72.00106048583984, 505.1822509765625, 525.546875, 545.4257202148438, 'We have constructed two data sets from BLESS (Baroni and Lenci, 2011) which is a collection of ex-\namples of hypernyms, co-hyponyms, meronyms and random unrelated words for each of 200 concrete,\nlargely monosemous nouns. We will refer to these 200 nouns as the BLESS concepts.\n', 3, 0)
(72.00104522705078, 551.7493896484375, 525.546875, 626.989990234375, 'hyponymBLESS is a set of 1976 labelled pairs of nouns. For each BLESS concept, 80% of the hypernyms\nwere randomly selected to provide positive examples of entailment. The remaining hypernyms for the\ngiven concept were reversed and taken with the same number of co-hyponyms, meronyms and random\nwords to form negative examples of entailment. A ﬁlter was applied to ensure that duplicate pairs were\nnot included (e.g., if ⟨cat, animal⟩ is a positive pair then ⟨animal, cat⟩ cannot be a negative pair).\n', 4, 0)
(72.00102233886719, 626.3963012695312, 525.54736328125, 680.189697265625, 'cohyponymBLESS is a set of 5835 labelled pairs of nouns. For each BLESS concept, the co-hyponyms\nwere taken as positive examples of this relation. The same total number of (and split evenly between)\nhypernyms, meronyms and random words was taken to form the negative examples. The order of 50%\nof the pairs was reversed and again duplicate pairs were disallowed.\n', 5, 0)
(72.00102996826172, 687.0042724609375, 525.546630859375, 740.7966918945312, 'In both cases the pairs are labelled as positive or negative for the speciﬁed semantic relation and in\nboth cases there are equal (±1) numbers of positive and negative examples. For 99% of the generated\nBLESS pairs, both nouns had associated vectors harvested from Wikipedia. If a noun does not have an\nassociated vector, the classiﬁers use a zero vector.\n', 6, 0)
(84.65399932861328, 753.6326904296875, 357.13214111328125, 765.090087890625, '5https://github.com/SussexCompSem/learninghypernyms\n', 7, 0)
(287.8630065917969, 786.9010009765625, 309.6812438964844, 800.0464477539062, '2254\n', 8, 0)

page suivante
(72.00096893310547, 63.29331588745117, 525.5493774414062, 411.5456848144531, '3.3.2\nWordNet\nWe constructed two data sets using WordNet. Whilst these data sets are similar in size to the BLESS\ndata sets they more adequately satisfy the requirements laid out above6. We constructed a list of all non-\nrare, largely monosemous, single word terms in WordNet. To be considered non-rare, a word needed to\nhave occurred in SemCor at least once (i.e. frequency information is provided about it in the WordNet\npackage) and to have occurred in Wikipedia at least 100 times. To be considered largely monosemous,\nthe predominant sense of the word needed to account for over 50% of the occurrences in the SemCor\nfrequency information provided with WordNet. This led to a list of 7613 nouns.\nhyponymWN is a set of 2564 labelled pairs of nouns constructed in the following way. Pairs ⟨A, B⟩ were\nfound in the list of nouns where B is an ancestor of A (i.e., A lexically entails B). Each found pair is\nadded either as a positive or a negative in the ratio 2:1 provided that the reverse pairing has not already\nbeen added and provided that each word has not previously been used in that position. Co-hyponym\npairs (i.e., words which share a direct hypernym) were also found within the list of nouns. Each found\npair is added to the data set (as a negative) provided the reverse pairing has not already been added, and\nprovided that neither word has already been seen in that position in a pairing (either in the entailment\npairs or the co-hyponym pairs). The same number of co-hyponym pairs as hypernym-hyponym negatives\nis selected. This provides a balanced data set where half of the pairs are positive examples of entailment\nand the other half are semantically similar but not entailing.\ncohyponymWN is a set of 3771 labelled pairs of nouns. It was constructed in the same way as hyponymWN\nexcept the same number of co-hyponym pairs were selected as the total number of entailment pairs (in\neither direction). These co-hyponym pairs were labelled as positive and the entailment pairs were labelled\nas negative. Thus, this provides a balanced data set where half of the pairs are positive examples of co-\nhyponyms and the other half, the negative examples, are entailment pairs (with direction unspeciﬁed)\nIn both these sets, the average path distance between entailment pairs is 1.64, whereas path distance\nbetween co-hyponym pairs is 2.\n', 0, 0)
(72.00096893310547, 419.7734680175781, 189.57928466796875, 433.9662170410156, '3.4\nExperimental Setup\n', 1, 0)
(72.00096893310547, 437.6422119140625, 525.5467529296875, 626.927734375, 'Most of our experiments were carried out using an implementation of ﬁve-fold cross-validation using\neach combination of data set, vector set and classiﬁer. In this setup, the pairs are randomly partitioned\ninto ﬁve subsets, one subset is held out for testing whilst the classiﬁers are trained on the remaining four,\nand this process is repeated using each subset as the test set.\nIn initial experiments with the BLESS datasets, the SVM classiﬁers were able to achieve classiﬁcation\naccuracy of over 95% for hyponymBLESS and over 90% for cohyponymBLESS. However, the results us-\ning random vectors were not signiﬁcantly different from using the distributional vectors harvested from\nWikipedia. This indicated that the classiﬁers were learning ontological information implicit in the train-\ning data. In order to address this, when using the BLESS datasets, we removed any pair from the training\ndata if either word was present in the test data. In order to preserve a reasonable amount of training data,\nwe implemented this approach with ten-fold cross-validation. In all subsequent experiments, across all\ndatasets and classiﬁers, we found performance by the random vectors was no higher than 52%. This\nindicates that the performance seen in Table 3 is due to learning from distributional features rather than\nany ontological information implicit in the training set.\n', 2, 0)
(72.00096893310547, 635.5313110351562, 127.12639617919922, 651.0850219726562, '4\nResults\n', 3, 0)
(72.00096893310547, 657.2212524414062, 525.547119140625, 738.1126708984375, 'In Table 3, we compare average accuracy for a number of different classiﬁers on each of two tasks,\ndistinguishing hyponyms and distinguishing co-hyponyms, on each of the two datasets.\nLooking at the results for the hyponymBLESS data set, we can see that the SVM methods do generally\noutperform the unsupervised methods. However, the best performing model is svmSING, suggesting\nthat, for this data set, it is best to try to learn the distributional features of more general terms, rather than\ncomparing the vector representations of the two terms under consideration.\n', 4, 0)
(72.0009994506836, 743.6697387695312, 525.547607421875, 765.4845581054688, '6Note that imposing these requirements on the BLESS data sets would lead to very small data sets, since information is only\nprovided for 200 nouns.\n', 5, 0)
(287.8630065917969, 786.9010009765625, 309.6812438964844, 800.0464477539062, '2255\n', 6, 0)

page suivante
(97.59300231933594, 64.40510559082031, 428.8209228515625, 75.20962524414062, 'dataset\nsvmDIFF\nsvmMULT\nsvmADD\nsvmCAT\nsvmSING\nknnDIFF\n', 0, 0)
(83.1449966430664, 76.16532897949219, 421.9259338378906, 90.94767761230469, 'hyponymBLESS\n0.74\n0.56\n0.66\n0.68\n0.75\n0.54\n', 1, 0)
(78.9129867553711, 90.10706329345703, 421.9230041503906, 104.49668884277344, 'cohyponymBLESS\n0.62\n0.39\n0.41\n0.40\n0.40\n0.58\n', 2, 0)
(87.79998016357422, 103.26335144042969, 421.9308776855469, 118.04570007324219, 'hyponymWN\n0.75\n0.45\n0.37\n0.74\n0.69\n0.50\n', 3, 0)
(83.56795501708984, 116.81333923339844, 421.9308776855469, 131.59471130371094, 'cohyponymWN\n0.37\n0.60\n0.68\n0.64\n0.58\n0.50\n', 4, 0)
(97.59300231933594, 132.9480743408203, 518.6370849609375, 143.75259399414062, 'dataset\nmost freq\ncosineP\nlinP\nwidthdiff\nsinglewidth\nCRdiff\ninvCLP\nbalAPincP\n', 5, 0)
(83.1449966430664, 145.1010284423828, 508.9994812011719, 159.4906463623047, 'hyponymBLESS\n0.54\n0.53\n0.54\n0.56\n0.58\n0.52\n0.54\n0.54\n', 6, 0)
(78.91299438476562, 158.25730895996094, 501.26904296875, 173.03965759277344, 'cohyponymBLESS\n0.61\n0.79\n0.78\n-\n-\n-\n-\n-\n', 7, 0)
(87.79999542236328, 172.1990509033203, 508.9994812011719, 186.5886688232422, 'hyponymWN\n0.50\n0.53\n0.52\n0.70\n0.65\n0.70\n0.66\n0.53\n', 8, 0)
(83.5679931640625, 185.74806213378906, 501.27569580078125, 200.13768005371094, 'cohyponymWN\n0.50\n0.50\n0.55\n-\n-\n-\n-\n-\n', 9, 0)
(72.00096893310547, 208.21205139160156, 525.5466918945312, 248.45553588867188, 'Table 3: Accuracy Figures for the data sets generated from BLESS and WordNet (standard errors <\n0.02). For cohyponyms, results for measures designed to detect hyponymy have been omitted. We also\nomit results of clarkediff as these were consistently the same or less than CRdiff.\n', 10, 0)
(72.00089263916016, 271.925048828125, 525.5499267578125, 766.0306396484375, 'On the corresponding co-hyponym task, using the cohyponymBLESS data set, we see the best performing\nclassiﬁer is the cosine measure. The cosine measure is able to perform relatively well here because a\nsubstantial proportion of the negative examples (25%) are random unrelated words which will have low\ncosine scores. It is also consistent with earlier work (e.g., (Lenci and Benotto, 2012)) which suggests\nthat measures such as the cosine measure “prefer” words in symmetric semantic relationships such as co-\nhyponymy. The poor performance of the SVM methods here can perhaps be explained by the paucity of\nthe training data in this experimental set up with this data set. If, for example, our test concept is robin,\nour approach requires that we will not have any training pairs containing robin, or any training pairs\ncontaining any of the words to which robin is related in the test set. In a dataset as small as BLESS,\nthis requirement effectively removes all knowledge of the distributional features of words in the target\ndomain. Hence, the need for a larger dataset as we have extracted from WordNet.\nLooking at the results for the hyponymWN data set, the directional SVM methods (svmDIFF and svm-\nCAT) substantially outperform the symmetric SVM methods, and their performance is signiﬁcantly better\n(at the 0.01% level) than the unsupervised methods. Also of note is the substantial difference between\nsvmDIFF and knnDIFF. Both of these methods are trained on the differences of vectors. However, the\nlinear SVM outperforms kNN by 19–25%. This may suggest that the shape of the vector space inhabited\nby the positive entailment pairs is particularly conducive for learning a linear SVM. Positive and negative\npairs are close together (as evidenced by the poor performance of kNN), but generally linearly separable.\nLooking at the results for the cohyponymWN data set, it is clear that the unsupervised methods cannot\ndistinguish the co-hyponym pairs from the entailing pairs. The supervised SVM methods do substantially\nbetter, with the best performance achieved by svmADD and svmCAT. Both of these methods essentially\nretain information about all of the features of both words. svmMULT does much better than svmDIFF,\nwhich suggests that the shared features are more indicative than the non-shared features for this task.\nThe reasonably high performance of svmSING on both data sets suggests that words which have co-\nhyponyms in the data set tend to inhabit a somewhat different part of the feature space to words which\nare included as entailed words in the data set. We hypothesise that there are speciﬁc features which more\ngeneral words tend to share (regardless of their topic) which makes it possible to identify more general\nwords from more speciﬁc words. This is completely consistent with very recent results using SLQS, a\nnew entropy-based measure (Santus et al., 2014). Here, the authors hypothesise that the most typical\ncontexts of a hypernym are less informative than the most typical linguistic contexts of its hyponyms,\nwith some promising results. It would be plausible to hypothesise that svmSING is learning which nouns\ntypically have less informative contexts and are therefore likely to by hypernyms.\nGiven prior work, the performance of the balAPincP measure is lower than expected on the\nhyponymWN dataset. Our task is slightly different to that of (Kotlerman et al., 2010), since we are deter-\nmining the existence (or not) of hyponymy, rather than the direction of entailment for pairs where it is\nknown that a relationship exists. It could be that the measure is particularly suited to the latter task.\n', 11, 0)
(287.8630065917969, 786.9010009765625, 309.6812438964844, 800.0464477539062, '2256\n', 12, 0)

page suivante
(72.0009994506836, 62.2890625, 249.45205688476562, 77.8427734375, '5\nConclusions and Further Work\n', 0, 0)
(72.0009994506836, 84.39905548095703, 525.5467529296875, 436.2746887207031, 'We have shown that it is possible to predict to a large extent whether or not there is a speciﬁc semantic\nrelation between two words given their distributional vectors, using a supervised approach based on\nlinear SVMs. The increase in accuracy over unsupervised methods is signiﬁcant at the 0.01% level and\ncorresponds to a substantial absolute reduction in error rate (over 15%).\nWe have also shown that the choice of vector operation is signiﬁcant. Whilst concatenating the vectors,\nand therefore retaining all of the information from both vectors including direction, generally performs\nwell, we have also shown that different vector operations are useful in establishing different relationships.\nIn particular, the vector difference operation, which loses information about the original vectors, achieved\nperformance indistinguishable from concatenation on the entailment task, where the classiﬁer is required\nto distinguish hyponyms from other semantically related words including hypernyms. On the other\nhand, the addition operation, which also loses information, outperformed concatenation by 4% (which\nis statistically signiﬁcant at the 0.01% level) on the coordinate task, where the classiﬁer is required to\ndistinguish co-hyponyms from hyponyms and hypernyms. Hence the nature of the relationship one is\ntrying to establish between words determines the nature of the operation one should perform on their\nassociated vectors.\nWe have also shown that it is possible to outperform state-of-the-art unsupervised methods even when\na data set has been constructed without ontological information, and when target words have not previ-\nously been seen in that position of a relationship in the training data. Hence, we believe the supervised\nmethods are learning characteristics of the underlying feature space which are generalisable to new words\n(inhabiting the same feature space).\nIn future work, we intend to apply this approach to the problem of labelling the distributional neigh-\nbours found for a given word with speciﬁc semantic relations. We also plan to investigate the use of\nbag-of-words (windowed) vectors instead of grammatical relations for this task.\nFinally, we believe that the data sets constructed from WordNet, which we publish alongside this\npaper, can be used as a useful benchmark in evaluating future advances in this area, both for supervised\nand unsupervised methods.\n', 1, 0)
(72.0009994506836, 445.2982482910156, 170.8346405029297, 460.8519592285156, 'Acknowledgements\n', 2, 0)
(72.0009994506836, 467.4082336425781, 525.546630859375, 494.10272216796875, 'This work was funded by UK EPSRC project EP/IO37458/1 “A Uniﬁed Model of Compositional and\nDistributional Compositional Semantics: Theory and Applications”.\n', 3, 0)
(72.0009994506836, 515.8412475585938, 127.54483795166016, 531.3949584960938, 'References\n', 4, 0)
(72.0009994506836, 535.6138305664062, 525.5447387695312, 558.5777587890625, 'Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings\nof the GEMS 2011 workshop on Geometric Models of Natural Language Semantics, EMNLP 2011.\n', 5, 0)
(72.0009994506836, 566.091796875, 525.5482788085938, 600.0147705078125, 'Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-\nnoun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural\nLanguage Processing.\n', 6, 0)
(72.00100708007812, 607.5298461914062, 525.5482788085938, 641.4527587890625, 'Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word\nlevel in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Associ-\nation for Computational Linguistics, pages 23–32. Association for Computational Linguistics.\n', 7, 0)
(72.00101470947266, 648.966796875, 525.5481567382812, 682.8897705078125, 'Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2010. Global learning of focused entailment graphs. In\nProceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220–1229,\nUppsala, Sweden, July. Association for Computational Linguistics.\n', 8, 0)
(72.0009994506836, 690.40380859375, 525.5482177734375, 724.3267822265625, 'Shane Bergsma, Aditya Bhargava, Hua He, and Grzegorz Kondrak. 2010. Predicting the semantic composi-\ntionality of preﬁx verbs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language\nProcessing, pages 293–303, Cambridge, MA, October. Association for Computational Linguistics.\n', 9, 0)
(72.00099182128906, 731.841796875, 525.5482177734375, 765.7647705078125, 'Danushka Bollegala, David Weir, and John Carroll. 2011. Using multiple sources to construct a sentiment sen-\nsitive thesaurus for cross-domain sentiment classiﬁcation. In Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human Language Technologies (ACL-HLT 2011).\n', 10, 0)
(287.8630065917969, 786.9010009765625, 309.6812438964844, 800.0464477539062, '2257\n', 11, 0)

page suivante
(72.0009994506836, 64.56060791015625, 525.5482788085938, 98.48351287841797, 'Kenneth Ward Church and Patrick Hanks. 1989. Word association norms, mutual information, and lexicography.\nIn Proceedings of the 27th Annual Meeting on Association for Computational Linguistics, ACL ’89, pages\n76–83, Stroudsburg, PA, USA. Association for Computational Linguistics.\n', 0, 0)
(72.0009994506836, 107.75555419921875, 525.5423583984375, 130.71946716308594, 'Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the\nWorkshop of Geometric Models for Natural Language Semantics.\n', 1, 0)
(72.0009994506836, 139.99151611328125, 487.5896301269531, 151.99644470214844, 'James Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.\n', 2, 0)
(72.00096893310547, 161.26953125, 525.5481567382812, 184.2334442138672, 'Georgiana Dinu and Stefan Thater.\n2012.\nSaarland: Vector-based models of semantic textual similarity.\nIn\nProceedings of the First Joint Conference on Lexical and Computational Semantics.\n', 3, 0)
(72.00096893310547, 193.5054931640625, 525.544921875, 216.4694061279297, 'Christaine Fellbaum, editor. 1989. WordNet: An Electronic Lexical Database. The MIT Press, Cambridge,\nMassachusetts.\n', 4, 0)
(72.00096893310547, 225.741455078125, 525.5479736328125, 259.66436767578125, 'Trevor Fountain and Mirella Lapata. 2012. Taxonomy induction using hierarchical random graphs. In Proceed-\nings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 466–476, Montr´eal, Canada, June.\n', 5, 0)
(72.00096893310547, 268.9364013671875, 525.5446166992188, 291.90032958984375, 'Maayan Geffet and Ido Dagan. 2005. Lexical entailment and the distributional inclusion hypothesis. In Proceed-\nings of the 43rd meeting of the Association for Computational Liuguistics (ACL), pages 107–114.\n', 6, 0)
(72.00096893310547, 301.17236328125, 525.5482177734375, 335.09527587890625, 'Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen Pulman. 2011. Concrete\nsentence spaces for compositional distributional models of meaning. Proceedings of the 9th International Con-\nference on Computational Semantics (IWCS 2011), pages 125–134.\n', 7, 0)
(72.00096893310547, 344.3673400878906, 327.002197265625, 356.3722839355469, 'Zelig Harris. 1954. Distributional structure. Word, 10:146–162.\n', 8, 0)
(72.00098419189453, 365.64434814453125, 525.5481567382812, 399.5672607421875, 'Mitesh Khapra, Anup Kulkarni, Saurabh Sohoney, and Pushpak Bhattacharyya. 2010. All words domain adapted\nWSD: Finding a middle ground between supervision and unsupervision. In Proceedings of the 48th Annual\nMeeting of the Association for Computational Linguistics, pages 1532–1541, Uppsala, Sweden, July.\n', 9, 0)
(72.00101470947266, 408.8393249511719, 525.5477294921875, 431.80328369140625, 'Adam Kilgarriff and Colin Yallop.\n2000.\nWhat’s in a thesaurus?\nIn Proceedings of the 2nd International\nConference on Language Resources and Evaluation (LREC2000).\n', 10, 0)
(72.00102996826172, 441.0753479003906, 525.548095703125, 474.998291015625, 'Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional simi-\nlarity for lexical inference. Special Issue of Natural Language Engineering on Distributional Lexical Semantics,\n4(16):359–389.\n', 11, 0)
(72.00102996826172, 484.2703552246094, 525.5415649414062, 507.23431396484375, 'Lillian Lee.\n1999.\nMeasures of distributional similarity.\nIn Proceedings of the 37th Annual Meeting of the\nAssociation for Computational Linguistics, pages 25–32, College Park, Maryland, USA, June.\n', 12, 0)
(72.00104522705078, 516.50634765625, 525.5447387695312, 539.4703369140625, 'Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceed-\nings of the First Joint Conference on Lexical and Computational Semantics (*Sem).\n', 13, 0)
(72.00106048583984, 548.743408203125, 525.5447387695312, 571.7063598632812, 'Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th International\nConference on Computational Linguistics (COLING 1998).\n', 14, 0)
(72.0010757446289, 580.9794311523438, 525.5432739257812, 614.9013671875, 'Tara McIntosh. 2010. Unsupervised discovery of negative categories in lexicon bootstrapping. In Proceedings of\nthe 2010 Conference on Empirical Methods in Natural Language Processing, pages 356–365, Cambridge, MA,\nOctober. Association for Computational Linguistics.\n', 15, 0)
(72.00106048583984, 624.1744384765625, 525.5482177734375, 658.0963745117188, 'Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-\nresentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 746–751, Atlanta, Georgia, June.\n', 16, 0)
(72.00104522705078, 667.3694458007812, 525.5482788085938, 701.2913818359375, 'Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using distributional similarity for\nlexical expansion in knowledge-based word sense disambiguation. In Proceedings of COLING 2012, pages\n1781–1796, Mumbai, India, December.\n', 17, 0)
(72.00102996826172, 710.564453125, 525.5466918945312, 733.5283813476562, 'Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08:\nHLT, pages 236–244, Columbus, Ohio, June. Association for Computational Linguistics.\n', 18, 0)
(72.00102233886719, 742.8004150390625, 525.5397338867188, 765.764404296875, 'Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the ACL workshop on\nIncremental Parsing, pages 50–57.\n', 19, 0)
(287.8630065917969, 786.9010009765625, 309.6812438964844, 800.0464477539062, '2258\n', 20, 0)

page suivante
(72.0009994506836, 64.56060791015625, 525.5481567382812, 98.48351287841797, 'Marek Rei and Ted Briscoe. 2013. Parser lexicalisation through self-learning. In Proceedings of the 2013 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 391–400, Atlanta, Georgia, June. Association for Computational Linguistics.\n', 0, 0)
(72.00099182128906, 106.403564453125, 525.54833984375, 140.3264617919922, 'Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine Schulte Im Walde. 2014. Chasing hypernyms in vector\nspaces with entropy. In Proceedings of the 14th Conference of the European Chapter of the Association for\nComputational Linguistics, pages 38–42, Gothenburg, Sweden, April.\n', 1, 0)
(72.0009994506836, 148.24652099609375, 525.5482177734375, 171.21043395996094, 'Rion Snow, Daniel Jurafsky, and Andrew Y Ng.\n2004.\nLearning syntactic patterns for automatic hypernym\ndiscovery. Advances in Neural Information Processing Systems 17.\n', 2, 0)
(72.0009994506836, 179.13153076171875, 525.5482788085938, 213.05345153808594, 'Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006. Semantic taxonomy induction from heterogenous evidence.\nIn Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting\nof the Association for Computational Linguistics, pages 801–808. Association for Computational Linguistics.\n', 3, 0)
(72.00101470947266, 220.97454833984375, 525.5489501953125, 254.89744567871094, 'Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality\nthrough recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natural Language Learning, pages 1201–1211.\n', 4, 0)
(72.00102996826172, 262.76751708984375, 525.5488891601562, 296.74041748046875, 'Gy¨orgy Szarvas, Chris Biemann, and Iryna Gurevych. 2013. Supervised all-words lexical substitution using\ndelexicalized features. In Proceedings of the 2013 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, pages 1131–1141, Atlanta, Georgia, June.\n', 5, 0)
(72.00102996826172, 304.66046142578125, 525.5453491210938, 338.5833740234375, 'Idan Szpektor and Ido Dagan.\n2008.\nLearning entailment rules for unary templates.\nIn Proceedings of the\n22nd International Conference on Computational Linguistics (Coling 2008), pages 849–856, Manchester, UK,\nAugust.\n', 6, 0)
(72.00102996826172, 346.50341796875, 525.5481567382812, 380.42633056640625, 'Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and Mark\nSteedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,\npages 81–88.\n', 7, 0)
(72.00102996826172, 388.34637451171875, 525.5482177734375, 411.310302734375, 'Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity.\nIn Proceedings of Coling 2004, pages 1015–1021, Geneva, Switzerland, Aug 23–Aug 27. COLING.\n', 8, 0)
(72.00102996826172, 419.2303466796875, 525.5481567382812, 453.15325927734375, 'Julie Weeds, David Weir, and Jeremy Refﬁn. 2014. Distributional composition using higher-order dependency\nvectors. In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality,\nEACL 2014, Gothenburg, Sweden, April.\n', 9, 0)
(72.00102996826172, 461.07330322265625, 525.5416259765625, 484.0372314453125, 'Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second\nSymposium on Quantum Interaction, Oxford, UK, pages 1–8.\n', 10, 0)
(72.00104522705078, 491.9582824707031, 525.548095703125, 525.8802490234375, 'Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Polarity inducing latent semantic analysis. In Proceedings of\nthe 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural\nLanguage Learning, pages 1212–1222, Jeju Island, Korea, July. Association for Computational Linguistics.\n', 11, 0)
(72.00102996826172, 533.80126953125, 525.5447387695312, 567.7232666015625, 'Chen Zhang and Joyce Chai. 2010. Towards conversation entailment: An empirical investigation. In Proceedings\nof the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756–766, Cambridge,\nMA, October. Association for Computational Linguistics.\n', 12, 0)
(287.8630065917969, 786.9010009765625, 309.6812438964844, 800.0464477539062, '2259\n', 13, 0)

page suivante
