(104.8095474243164, 722.5316772460938, 285.74755859375, 737.7928466796875, 'c⃝ 2002 Association for Computational Linguistics\n', 0, 0)
(104.80999755859375, 83.33748626708984, 353.3193359375, 103.49192810058594, 'Periods, Capitalized Words, etc.\n', 1, 0)
(104.80999755859375, 136.20921325683594, 196.49118041992188, 150.6750030517578, 'Andrei Mikheev∗\n', 2, 0)
(104.80999755859375, 151.0576934814453, 214.3836212158203, 163.11244201660156, 'University of Edinburgh\n', 3, 0)
(104.71037292480469, 192.22518920898438, 487.3718566894531, 307.7314147949219, 'In this article we present an approach for tackling three important aspects of text normaliza-\ntion: sentence boundary disambiguation, disambiguation of capitalized words in positions where\ncapitalization is expected, and identiﬁcation of abbreviations. As opposed to the two dominant\ntechniques of computing statistics or writing specialized grammars, our document-centered ap-\nproach works by considering suggestive local contexts and repetitions of individual words within\na document. This approach proved to be robust to domain shifts and new lexica and produced per-\nformance on the level with the highest reported results. When incorporated into a part-of-speech\ntagger, it helped reduce the error rate signiﬁcantly on capitalized words and sentence boundaries.\nWe also investigated the portability to other languages and obtained encouraging results.\n', 4, 0)
(104.80999755859375, 320.6957702636719, 173.1564178466797, 332.55126953125, '1. Introduction\n', 5, 0)
(104.8099365234375, 344.5761413574219, 487.3739318847656, 655.5087280273438, 'Disambiguation of sentence boundaries and normalization of capitalized words, as\nwell as identiﬁcation of abbreviations, however small in comparison to other tasks\nof text processing, are of primary importance in the developing of practical text-\nprocessing applications. These tasks are usually performed before actual “intelligent”\ntext processing starts, and errors made at this stage are very likely to cause more errors\nat later stages and are therefore very dangerous.\nDisambiguation of capitalized words in mixed-case texts has received little atten-\ntion in the natural language processing and information retrieval communities, but in\nfact it plays an important role in many tasks. In mixed-case texts capitalized words\nusually denote proper names (names of organizations, locations, people, artifacts, etc.),\nbut there are special positions in the text where capitalization is expected. Such manda-\ntory positions include the ﬁrst word in a sentence, words in titles with all signiﬁcant\nwords capitalized or table entries, a capitalized word after a colon or open quote, and\nthe ﬁrst word in a list entry, among others. Capitalized words in these and some other\npositions present a case of ambiguity: they can stand for proper names, as in White\nlater said . . . , or they can be just capitalized common words, as in White elephants are\n. . . . The disambiguation of capitalized words in ambiguous positions leads to the\nidentiﬁcation of proper names (or their derivatives), and in this article we will use\nthese two terms and the term case normalization interchangeably.\nChurch (1995, p. 294) studied, among other simple text normalization techniques,\nthe effect of case normalization for different words and showed that “sometimes case\nvariants refer to the same thing (hurricane and Hurricane), sometimes they refer to\ndifferent things (continental and Continental) and sometimes they don’t refer to much\nof anything (e.g., anytime and Anytime).” Obviously these differences arise because\nsome capitalized words stand for proper names (such as Continental, the name of an\nairline) and some do not.\n', 6, 0)
(109.0770034790039, 680.1685180664062, 473.8694763183594, 698.7787475585938, '∗ Institute for Communicating and Collaborative Systems, Division of Informatics, 2 Buccleuch Place,\nEdinburgh EH8 9LW, UK. E-mail: mikheev@cogsci.ed.ac.uk\n', 7, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '290\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(104.81002044677734, 87.5367660522461, 487.37445068359375, 661.482421875, 'Proper names are the main concern of the named-entity recognition subtask (Chin-\nchor 1998) of information extraction. The main objective of this subtask is the identi-\nﬁcation of proper names and also their classiﬁcation into semantic categories (person,\norganization, location, etc.).1 There the disambiguation of the ﬁrst word in a sentence\n(and in other ambiguous positions) is one of the central problems: about 20% of named\nentities occur in ambiguous positions. For instance, the word Black in the sentence-\ninitial position can stand for a person’s surname but can also refer to the color. Even\nin multiword capitalized phrases, the ﬁrst word can belong to the rest of the phrase\nor can be just an external modiﬁer. In the sentence Daily, Mason and Partners lost their\ncourt case, it is clear that Daily, Mason and Partners is the name of a company. In the\nsentence Unfortunately, Mason and Partners lost their court case, the name of the company\ndoes not include the word Unfortunately, but the word Daily is just as common a word\nas Unfortunately.\nIdentiﬁcation of proper names is also important in machine translation, because\nusually proper names are transliterated (i.e., phonetically translated) rather than prop-\nerly (semantically) translated. In conﬁdential texts, such as medical records, proper\nnames must be identiﬁed and removed before making such texts available to people\nunauthorized to have access to personally identiﬁable information. And in general,\nmost tasks that involve text analysis will beneﬁt from the robust disambiguation of\ncapitalized words into proper names and common words.\nAnother important task of text normalization is sentence boundary disambigua-\ntion (SBD) or sentence splitting. Segmenting text into sentences is an important aspect\nin developing many applications: syntactic parsing, information extraction, machine\ntranslation, question answering, text alignment, document summarization, etc. Sen-\ntence splitting in most cases is a simple matter: a period, an exclamation mark, or a\nquestion mark usually signals a sentence boundary. In certain cases, however, a period\ndenotes a decimal point or is a part of an abbreviation, and thus it does not necessarily\nsignal a sentence boundary. Furthermore, an abbreviation itself can be the last token\nin a sentence in which case its period acts at the same time as part of this abbreviation\nand as the end-of-sentence indicator (fullstop). A detailed introduction to the SBD\nproblem can be found in Palmer and Hearst (1997).\nThe disambiguation of capitalized words and sentence boundaries presents a\nchicken-and-egg problem. If we know that a capitalized word that follows a period is\na common word, we can safely assign such period as sentence terminal. On the other\nhand, if we know that a period is not sentence terminal, then we can conclude that\nthe following capitalized word is a proper name.\nAnother frequent source of ambiguity in end-of-sentence marking is introduced by\nabbreviations: if we know that the word that precedes a period is not an abbreviation,\nthen almost certainly this period denotes a sentence boundary. If, however, this word\nis an abbreviation, then it is not that easy to make a clear decision. This problem is\nexacerbated by the fact that abbreviations do not form a closed set; that is, one can-\nnot list all possible abbreviations. Moreover, abbreviations can coincide with regular\nwords; for example, “in” can denote an abbreviation for “inches,” “no” can denote an\nabbreviation for “number,” and “bus” can denote an abbreviation for “business.”\nIn this article we present a method that tackles sentence boundaries, capitalized\nwords, and abbreviations in a uniform way through a document-centered approach.\nAs opposed to the two dominant techniques of computing statistics about the words\nthat surround potential sentence boundaries or writing specialized grammars, our ap-\n', 2, 0)
(109.32599639892578, 689.134765625, 393.04022216796875, 698.7786254882812, '1 In this article we are concerned only with the identiﬁcation of proper names.\n', 3, 0)

page suivante
(472.431396484375, 722.0972290039062, 487.3752746582031, 734.1519775390625, '291\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.3785095214844, 67.42921447753906, 'Mikheev\nPeriods, Capitalized Words, etc.\n', 1, 0)
(104.80999755859375, 87.5367660522461, 487.3708801269531, 123.50178527832031, 'proach disambiguates capitalized words and abbreviations by considering suggestive\nlocal contexts and repetitions of individual words within a document. It then applies\nthis information to identify sentence boundaries using a small set of rules.\n', 2, 0)
(104.80999755859375, 136.215087890625, 436.05950927734375, 148.07058715820312, '2. Performance Measure, Corpora for Evaluation, and Intended Markup\n', 3, 0)
(104.80999755859375, 160.0954132080078, 487.3707580566406, 184.1053009033203, 'A standard practice for measuring the performance of a system for the class of tasks\nwith which we are concerned in this article is to calculate its error rate:\n', 4, 0)
(222.68247985839844, 194.26651000976562, 362.7834777832031, 212.9016571044922, 'error rate =\nincorrectly assigned\n', 5, 0)
(275.989013671875, 207.84066772460938, 368.30572509765625, 219.7360076904297, 'all assigned by system\n', 6, 0)
(104.81088256835938, 232.3097686767578, 487.3747253417969, 315.9259033203125, 'This single measure gives enough information, provided that the system does not\nleave unassigned word tokens that it is intended to handle. Obviously, we want the\nsystem to handle all cases as accurately as possible. Sometimes, however, it is beneﬁcial\nto assign only cases in which the system is conﬁdent enough, leaving the rest to be\nhandled by other methods. In this case apart from the error rate (which corresponds\nto precision or accuracy as 1−error rate) we also measure the system’s coverage or\nrecall\n', 7, 0)
(234.5259246826172, 314.9147033691406, 356.461181640625, 333.5497131347656, 'coverage = correctly assigned\n', 8, 0)
(284.34271240234375, 328.4892272949219, 355.6153564453125, 340.3845520019531, 'all to be assigned\n', 9, 0)
(104.81036376953125, 352.1214904785156, 487.37469482421875, 663.0254516601562, '2.1 Corpora for Evaluation\nThere are two corpora normally used for evaluation in a number of text-processing\ntasks: the Brown corpus (Francis and Kucera 1982) and the Wall Street Journal (WSJ)\ncorpus, both part of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993).\nThe Brown corpus represents general English. It contains over one million word tokens\nand is composed from 15 subcorpora that belong to different genres and domains,\nranging from news wire texts and scientiﬁc papers to ﬁction and transcribed speech.\nThe Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and\nungrammatical sentences with complex internal structure. Altogether there are about\n500 documents in the Brown corpus, with an average length of 2,300 word tokens.\nThe WSJ corpus represents journalistic news wire style. Its size is also over a\nmillion word tokens, and the documents it contains are rich in abbreviations and\nproper names, but they are much shorter than those in the Brown corpus. Altogether\nthere are about 2,500 documents in the WSJ corpus, with an average length of about\n500 word tokens.\nDocuments in the Penn Treebank are segmented into paragraphs and sentences.\nSentences are further segmented into word tokens annotated with part-of-speech (POS)\ninformation. POS information can be used to distinguish between proper names and\ncommon words. We considered proper nouns (NNP), plural proper nouns (NNPS), and\nproper adjectives2 (JJP) to signal proper names, and all other categories were consid-\nered to signal common words or punctuation. Since proper adjectives are not included\nin the Penn Treebank tag set, we had to identify and retag them ourselves with the\nhelp of a gazetteer.\nAbbreviations in the Penn Treebank are tokenized together with their trailing pe-\nriods, whereas fullstops and other sentence boundary punctuation are tokenized as\nseparate tokens. This gives all necessary information for the evaluation in all our three\n', 10, 0)
(109.32599639892578, 689.134765625, 355.5622863769531, 698.7786254882812, '2 These are adjectives derived from proper nouns (e.g. “American”).\n', 11, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '292\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(104.80999755859375, 87.5367660522461, 487.3717346191406, 111.54664611816406, 'tasks: the sentence boundary disambiguation task, the capitalized word disambigua-\ntion task, and the abbreviation identiﬁcation task.\n', 2, 0)
(104.80999755859375, 123.4320068359375, 487.37396240234375, 458.24517822265625, '2.2 Tokenization Convention and Corpora Markup\nFor easier handling of potential sentence boundary punctuation, we developed a new\ntokenization convention for periods. In the traditional Penn Treebank schema, abbrevi-\nations are tokenized together with their trailing periods, and thus stand-alone periods\nunambiguously signal the end of a sentence. We decided to treat periods and all other\npotential sentence termination punctuation as “ﬁrst-class citizens” and adopted a con-\nvention always to tokenize a period (and other punctuation) as a separate token when\nit is followed by a white space, line break, or punctuation. In the original Penn Tree-\nbank format, periods are unambiguous, whereas in our new convention they can take\non one of the three tags: fullstop (.), part of abbreviation (A) or both (*).\nTo generate the new format from the Penn Treebank, we had to split ﬁnal periods\nfrom abbreviations, mark them as separate tokens and assign them with A or * tags\naccording to whether or not the abbreviation was the last token in a sentence. We\napplied a similar tokenization convention to the case in which several (usually three)\nperiods signal ellipsis in a sentence. Again, sometimes such constructions occur within\na sentence and sometimes at a sentence break. We decided to treat such constructions\nsimilarly to abbreviations, tokenize all periods but the last together in a single token,\nand tokenize the last period separately and tag it with A or * according to whether\nor not the ellipsis was the last token in a sentence. We treated periods in numbers\n(e.g., 14.534) or inside acronyms (e.g., Y.M.C.A.) as part of tokens rather than separate\nperiods.\nIn all our experiments we treated embedded sentence boundaries in the same way\nas normal sentence boundaries. An embedded sentence boundary occurs when there\nis a sentence inside a sentence. This can be a quoted direct-speech subsentence inside a\nsentence, a subsentence embedded in brackets, etc. We considered closing punctuation\nof such sentences equal to closing punctuation of normal sentences.\nWe also specially marked word tokens in positions where they were ambiguously\ncapitalized if such word tokens occurred in one of the following contexts:\n', 3, 0)
(122.74278259277344, 471.20751953125, 264.8153991699219, 490.0069274902344, '•\nthe ﬁrst token in a sentence\n', 4, 0)
(122.74278259277344, 488.9190368652344, 460.32049560546875, 524.8839721679688, '•\nfollowing a separately tokenized period, question mark, exclamation\nmark, semicolon, colon, opening quote, closing quote, opening bracket,\nor closed bracket\n', 5, 0)
(122.74278259277344, 530.540771484375, 364.81597900390625, 549.3402099609375, '•\noccurring in a sentence with all words capitalized\n', 6, 0)
(104.81007385253906, 555.7789916992188, 487.37396240234375, 639.5645141601562, 'All described transformations were performed automatically by applying a simple\nPerl script. We found quite a few infelicities in the original tokenization and tagging,\nhowever, which we had to correct by hand. We also converted both our corpora from\ntheir original Penn Treebank format into an XML format where each word token is\nrepresented as an XML element (W) with the attribute C holding its POS information\nand attribute A set to Y for ambiguously capitalized words. An example of such a\nmarkup is displayed in Figure 1.\n', 7, 0)
(104.81009674072266, 651.4498901367188, 364.0947570800781, 663.3053588867188, '3. Our Approach to Sentence Boundary Disambiguation\n', 8, 0)
(104.81009674072266, 675.3302001953125, 487.3728942871094, 699.340087890625, 'If we had at our disposal entirely correct information on whether or not each word\npreceding a period was an abbreviation and whether or not each capitalized word\n', 9, 0)

page suivante
(472.431396484375, 722.0972290039062, 487.3752746582031, 734.1519775390625, '293\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.3785095214844, 67.42921447753906, 'Mikheev\nPeriods, Capitalized Words, etc.\n', 1, 0)
(104.80999755859375, 88.25618743896484, 460.4692077636719, 98.21878814697266, '...<W C=RB>soon</W><W C=’.’>.</W> <W A=Y C=NNP>Mr</W><W C=A>.</W>...\n', 2, 0)
(104.80999755859375, 112.1664047241211, 355.8636779785156, 134.08413696289062, '...<W C=VBN>said</W> <W C=NNP>Mr</W><W C=A>.</W>\n<W A=Y C=NNP>Brown</W>...\n', 3, 0)
(104.80738067626953, 148.03175354003906, 475.66302490234375, 231.75672912597656, '...<W C=’,’>,</W> <W C=NNP>Tex</W><W C=’*’>.</W>\n<W A=Y C=JJP>American</W>...\nFigure 1\nExample of the new tokenization and markup generated from the Penn Treebank format.\nTokens are represented as XML elements W, where the attribute C holds POS information.\nProper names are tagged as NNP, NNPS and JJP. Periods are tagged as . (fullstop), A (part of\nabbreviation), * (a fullstop and part of abbreviation at the same time). Ambiguously\ncapitalized words are marked with A = Y.\n', 4, 0)
(104.80999755859375, 257.9458923339844, 487.37384033203125, 281.9557800292969, 'that follows a period was a proper name, we could apply a very simple set of rules\nto disambiguate sentence boundaries:\n', 5, 0)
(122.74267578125, 296.14849853515625, 429.88165283203125, 314.9479064941406, '•\nIf a period follows a nonabbreviation, it is sentence terminal (.).\n', 6, 0)
(122.74267578125, 314.1678466796875, 469.4261169433594, 350.1328125, '•\nIf a period follows an abbreviation and is the last token in a text passage\n(paragraph, document, etc.), it is sentence terminal and part of the\nabbreviation (*).\n', 7, 0)
(122.74267578125, 356.096435546875, 461.85845947265625, 380.1062927246094, '•\nIf a period follows an abbreviation and is not followed by a capitalized\nword, it is part of the abbreviation and is not sentence terminal (A).\n', 8, 0)
(122.74267578125, 386.0708923339844, 469.4390869140625, 422.0358581542969, '•\nIf a period follows an abbreviation and is followed by a capitalized word\nthat is not a proper name, it is sentence terminal and part of the\nabbreviation (*).\n', 9, 0)
(104.81000518798828, 436.14190673828125, 487.373779296875, 496.01708984375, 'It is a trivial matter to extend these rules to allow for brackets and quotation marks\nbetween the period and the following word. To handle other sentence termination\npunctuation such as question and exclamation marks and semicolons, this rule set\nalso needs to include corresponding rules. The entire rule set for sentence boundary\ndisambiguation that was used in our experiments is listed in Appendix A.\n', 10, 0)
(104.80996704101562, 507.9891357421875, 487.3739013671875, 699.3408203125, '3.1 Ideal Case: Upper Bound for Our SBD Approach\nThe estimates from the Brown corpus and the WSJ corpus (section 3) show that the\napplication of the SBD rule set described above together with the information on\nabbreviations and proper names marked up in the corpora produces very accurate\nresults (error rate less than 0.0001%), but it leaves unassigned the outcome of the case\nin which an abbreviation is followed by a proper name. This is a truly ambiguous case,\nand to deal with this situation in general, one should encode detailed information\nabout the words participating in such contexts. For instance, honoriﬁc abbreviations\nsuch as Mr. or Dr. when followed by a proper name almost certainly do not end a\nsentence, whereas the abbreviations of U.S. states such as Mo., Cal., and Ore., when\nfollowed directly by a proper name, most likely end a sentence. Obviously encoding\nthis kind of information into the system requires detailed analysis of the domain lexica,\nis not robust to unseen abbreviations, and is labor intensive.\nTo make our method robust to unseen words, we opted for a crude but simple\nsolution. If such ambiguous cases are always resolved as “not sentence boundary” (A),\nthis produces, by our measure, an error rate of less than 3%. Estimates from the Brown\n', 11, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '294\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(104.80999755859375, 98.44984436035156, 474.6523742675781, 139.16004943847656, 'Table 1\nEstimates of the upper and lower bound error rates on the SBD task for our method. Three\nestimated categories are sentence boundaries, ambiguously capitalized words, and\nabbreviations.\n', 2, 0)
(238.6251983642578, 146.24362182617188, 436.12542724609375, 157.0929718017578, 'Brown Corpus\nWSJ Corpus\n', 3, 0)
(198.87420654296875, 162.18386840820312, 482.5155029296875, 173.03321838378906, 'SBD\nAmb. Cap. Abbreviations SBD\nAmb. Cap. Abbreviations\n', 4, 0)
(104.8096694946289, 179.51882934570312, 450.99981689453125, 339.8067932128906, 'Number of\n59,539 58,957\n4,657\n53,043 54,537\n16,317\nresolved instances\nA Upper Bound:\n0.01%\n0.0%\n0.0%\n0.13%\n0.0%\n0.0%\nAll correct proper\nnames\nAll correct abbrs.\nB\nLower Bound:\n2.00%\n7.4%\n10.8%\n4.10%\n15.0%\n9.6%\nLookup proper\nnames\nGuessed abbrs.\nC\nLookup proper\n1.20%\n7.4%\n0.0%\n2.34%\n15.0%\n0.0%\nnames\nAll correct abbrs.\nD All correct proper\n0.45%\n0.0%\n10.8%\n1.96%\n0.0%\n9.6%\nnames\nGuessed abbrs.\n', 5, 0)
(104.80999755859375, 375.84466552734375, 487.3739013671875, 555.2709350585938, 'corpus and the WSJ corpus showed that such ambiguous cases constitute only 5–7%\nof all potential sentence boundaries. This translates into a relatively small impact of\nthe crude strategy on the overall error rate on sentence boundaries. This impact was\nmeasured at 0.01% on the Brown corpus and at 0.13% on the WSJ corpus, as presented\nin row A of Table 1. Although this overly simplistic strategy extracts a small penalty\nfrom the performance, we decided to use it because it is very general and independent\nof domain-speciﬁc knowledge.\nThe SBD handling strategy described above is simple, robust, and well perform-\ning, but it relies on the assumption that we have entirely correct information about\nabbreviations and proper names, as can be seen in row A of the table. The main dif-\nﬁculty is that when dealing with real-world texts, we have to identify abbreviations\nand proper names ourselves. Thus estimates based on the application of our method\nwhen using 100% correctly disambiguated capitalized words and abbreviations can be\nconsidered as the upper bound for the SBD approach, that is, the top performance we\ncan achieve.\n', 6, 0)
(104.80999755859375, 567.7650146484375, 487.37384033203125, 699.3411254882812, '3.2 Worst Case: Lower Bound for Our SBD Approach\nWe can also estimate the lower bound for this approach applying very simple strategies\nto the identiﬁcation of proper names and abbreviations.\nThe simplest strategy for deciding whether or not a capitalized word in an ambigu-\nous position is a proper name is to apply a lexical-lookup strategy (possibly enhanced\nwith a morphological word guesser, e.g., Mikheev [1997]). Using this strategy, words\nnot listed as known common words for a language are usually marked as proper\nnames. The application of this strategy produced a 7.4% error rate on the Brown\ncorpus and a 15% error rate on the WSJ corpus. The difference in error rates can be\nexplained by the observation that the WSJ corpus contains a higher percentage of orga-\nnization names and person names, which often coincide with common English words,\n', 7, 0)

page suivante
(472.431396484375, 722.0972290039062, 487.3752746582031, 734.1519775390625, '295\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.3785095214844, 67.42921447753906, 'Mikheev\nPeriods, Capitalized Words, etc.\n', 1, 0)
(104.80998229980469, 87.5367660522461, 487.37384033203125, 398.4696960449219, 'and it contains more words in titles with all important words capitalized, which we\nalso consider as ambiguously capitalized.\nThe simplest strategy for deciding whether a word that is followed by a period\nis an abbreviation or a regular word is to apply well-known heuristics based on the\nobservation that single-word abbreviations are short and normally do not include\nvowels (Mr., Dr., kg.). Thus a word without vowels can be guessed to be an abbreviation\nunless it is written in all capital letters and can stand for an acronym or a proper name\n(e.g., BBC). A span of single letters separated by periods forms an abbreviation too\n(e.g., Y.M.C.A.). A single letter followed by a period is also a very likely abbreviation.\nThere is also an additional heuristic that classiﬁes as abbreviations short words (with\nlength less than ﬁve characters) that are followed by a period and then by a comma, a\nlower-cased word, or a number. All other words are considered to be nonabbreviations.\nThese heuristics are reasonably accurate. On the WSJ corpus they misrecognized\nas abbreviations only 0.2% of tokens. On the Brown corpus the misrecognition rate was\nsigniﬁcantly higher: 1.6%. The major source for these errors were single letters that\nstand for mathematical symbols in the scientiﬁc subcorpora of the Brown Corpus (e.g.,\npoint T or triangle F). The major shortcoming of these abbreviation-guessing heuristics,\nhowever, comes from the fact that they failed to identify about 9.5% of abbreviations.\nThis brings the overall error rate of the abbreviation-guessing heuristics to about 10%.\nCombining the information produced by the lexical-lookup approach to proper\nname identiﬁcation with the abbreviation-guessing heuristics feeding the SBD rule set\ngave us a 2.0% error rate on the Brown corpus and 4.1% on the WSJ corpus on the\nSBD task. This can be interpreted as the lower bound to our SBD approach. Here we\nsee how errors in the identiﬁcation of proper names and abbreviations propagated\nthemselves into errors on sentence boundaries. Row B of Table 1 displays a summary\nfor the lower-bound results.\n', 2, 0)
(104.80997467041016, 417.3288879394531, 487.37384033203125, 596.725341796875, '3.3 Major Findings\nWe also measured the importance of each of the two knowledge sources (abbreviations\nand proper names) separately. First, we applied the SBD rule set when all abbreviations\nwere correctly identiﬁed (using the information presented in the corpus) but applying\nthe lexical lookup strategy to proper-name identiﬁcation (row C of Table 1). Then, we\napplied the SBD rule set when all proper names were correctly identiﬁed (using the\ninformation presented in the corpus) but applying the guessing heuristics to handle\nabbreviations (row D of the table). In general, when a knowledge source returned\n100% accurate information this signiﬁcantly improved performance on the SBD task\nmeasured against the lower-bound error rate. We also see that proper names have a\nhigher impact on the SBD task than abbreviations.\nSince the upper bound of our SBD approach is high and the lower bound is far\nfrom being acceptable, our main strategy for sentence boundary disambiguation will be to\ninvest in the disambiguation of capitalized words and abbreviations that then feed our SBD\nrule set.\n', 3, 0)
(104.80996704101562, 615.5845336914062, 464.26458740234375, 627.4400024414062, '4. Document-Centered Approach to Proper Name and Abbreviation Handling\n', 4, 0)
(104.80996704101562, 639.46484375, 487.3728332519531, 699.3401489257812, 'As we discussed above, virtually any common word can potentially act as a proper\nname or part of a multiword proper name. The same applies to abbreviations: there is\nno ﬁxed list of abbreviations, and almost any short word can be used as an abbrevia-\ntion. Fortunately, there is a mitigating factor too: important words are typically used\nin a document more than once and in different contexts. Some of these contexts create\n', 5, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '296\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(104.80996704101562, 87.5367660522461, 487.37396240234375, 577.7963256835938, 'ambiguity, but some do not. Furthermore, ambiguous words and phrases are usually\nunambiguously introduced at least once in the text unless they are part of common\nknowledge presupposed to be possessed by the readers.\nThis observation can be applied to a broader class of tasks. For example, people\nare often referred to by their surnames (e.g., Black) but are usually introduced at least\nonce in the text either with their ﬁrst name (John Black) or with their title/profession\nafﬁliation (Mr. Black, President Bush), and it is only when their names are common\nknowledge that they do not need an introduction (e.g., Castro, Gorbachev). Thus our\nsuggestion is to look at the unambiguous usages of the words in question in the entire document.\nIn the case of proper name identiﬁcation, we are not concerned with the semantic\nclass of a name (e.g., whether it is a person’s name or a location), but rather we simply\nwant to distinguish whether a capitalized word in a particular occurrence acts as a\nproper name (or part of a multiword proper name). If we restrict our scope to a single\nsentence, we might ﬁnd that there is just not enough information to make a reliable\ndecision. For instance, Riders in the sentence Riders rode all over the green is equally likely\nto be a proper noun, a plural proper noun, or a plural common noun. But if in the\nsame text we ﬁnd John Riders, this sharply increases the likelihood that the proper noun\ninterpretation is the correct one, and conversely if we ﬁnd many riders, this suggests\nthe plural-noun interpretation.\nThe above reasoning can be summarized as follows: if we detect that a word is\nused capitalized in an unambiguous context, this increases the chances that this word\nacts as a proper name in ambiguous positions in the same document. And conversely\nif a word is seen only lower-cased, this increases the chances that it should be treated\nas a common word even when used capitalized in ambiguous positions in the same\ndocument. (This, of course, is only a general principle and will be further elaborated\nelsewhere in the article.)\nThe same logic applies to abbreviations. Although a short word followed by a\nperiod is a potential abbreviation, the same word occurring in the same document\nin a different context can be unambiguously classiﬁed as a regular word if it is used\nwithout a trailing period, or it can be unambiguously classiﬁed as an abbreviation if\nit is used with a trailing period and is followed by a lower-cased word or a comma.\nThis information gives us a better chance of assigning these potential abbreviations\ncorrectly in nonobvious contexts.\nWe call such style of processing a document-centered approach (DCA), since in-\nformation for the disambiguation of an individual word token is derived from the\nentire document rather than from its immediate local context. Essentially the system\ncollects suggestive instances of usage for target words from each document under\nprocessing and applies this information on the ﬂy to the processing of the document,\nin a manner similar to instance-based learning. This differentiates DCA from the tradi-\ntional corpus-based approach, in which learning is applied prior to processing, which\nis usually performed with supervision over multiple documents of the training corpus.\n', 2, 0)
(104.80999755859375, 590.677978515625, 245.6502685546875, 602.533447265625, '5. Building Support Resources\n', 3, 0)
(104.80999755859375, 614.558349609375, 487.3738708496094, 650.5233764648438, 'Our method requires only four word lists. Each list is a collection of words that belong\nto a single type, but at the same time, a word can belong to multiple lists. Since we\nhave four lists, we have four types:\n', 4, 0)
(122.74267578125, 668.3563842773438, 339.5776672363281, 687.1558227539062, '•\ncommon word (as opposed to proper name)\n', 5, 0)
(122.74267578125, 687.2853393554688, 358.9878234863281, 706.0847778320312, '•\ncommon word that is a frequent sentence starter\n', 6, 0)

page suivante
(472.431396484375, 722.0972290039062, 487.3752746582031, 734.1519775390625, '297\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.3785095214844, 67.42921447753906, 'Mikheev\nPeriods, Capitalized Words, etc.\n', 1, 0)
(122.7426986694336, 87.5367660522461, 238.79302978515625, 106.33618927001953, '•\nfrequent proper name\n', 2, 0)
(122.74270629882812, 106.96382904052734, 330.648193359375, 125.76325225830078, '•\nabbreviation (as opposed to regular word)\n', 3, 0)
(104.80999755859375, 137.34974670410156, 487.3740539550781, 699.3400268554688, 'These four lists can be acquired completely automatically from raw (unlabeled) texts.\nFor the development of these lists we used a collection of texts of about 300,000 words\nderived from the New York Times (NYT) corpus that was supplied as training data for\nthe 7th Message Understanding Conference (MUC-7) (Chinchor 1998). We used these\ntexts because the approach described in this article was initially designed to be part\nof a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed\nfor MUC-7. Although the corpus size of 300,000 words can be seen as large, the fact\nthat this corpus does not have to be annotated in any way and that a corpus of similar\nsize can be easily collected from on-line sources (including the Internet) makes this\nresource cheap to obtain.\nThe ﬁrst list on which our method relies is a list of common words. This list\nincludes common words for a given language, but no supplementary information such\nas POS or morphological information is required to be present in this list. A variety\nof such lists for many languages are already available (e.g., Burnage 1990). Words in\nsuch lists are usually supplemented with morphological and POS information (which\nis not required by our method). We do not have to rely on pre-existing resources,\nhowever. A list of common words can be easily obtained automatically from a raw\n(unannotated in any way) text collection by simply collecting and counting lower-\ncased words in it. We generated such list from the NYT collection. To account for\npotential spelling and capitalization errors, we included in the list of common words\nonly words that occurred lower-cased at least three times in the NYT texts. The list\nof common words that we developed from the NYT collection contained about 15,000\nEnglish words.\nThe second list on which our method relies is a frequent-starters list, a list of\ncommon words most frequently used in sentence-starting positions. This list can also\nbe obtained completely automatically from an unannotated corpus by applying the\nlexical-lookup strategy. As discussed in Section 3.2, this strategy performs with a\n7–15% error rate. We applied the list of common words over the NYT text collec-\ntion to tag capitalized words in sentence-starting positions as common words and as\nproper names: if a capitalized word was found in the list of common words, it was\ntagged as a common word: otherwise it was tagged as a proper name. Of course, such\ntagging was far from perfect, but it was good enough for our purposes. We included in\nthe frequent-starters list only the 200 most frequent sentence-starting common words.\nThis was more than a safe threshold to ensure that no wrongly tagged words were\nadded to this list. As one might predict, the most frequent sentence-starting common\nword was The. This list also included some adverbs, such as However, Suddenly, and\nOnce; some prepositions, such as In, To, and By; and even a few verbs: Let, Have, Do, etc.\nThe third list on which our method relies is a list of single-word proper names that\ncoincide with common words. For instance, the word Japan is much more likely to be\nused as a proper name (name of a country) rather than a verb, and therefore it needs to\nbe included in this list. We included in the proper name list 200 words that were most\nfrequently seen in the NYT text collection as single capitalized words in unambiguous\npositions and that at the same time were present in the list of common words. For\ninstance, the word The can frequently be seen capitalized in unambiguous positions,\nbut it is always followed by another capitalized word, so we do not count it as a\ncandidate. On the other hand the word China is often seen capitalized in unambiguous\npositions where it is not preceded or followed by other capitalized words. Since china\n', 4, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '298\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(104.80999755859375, 98.44984436035156, 471.28021240234375, 139.16004943847656, 'Table 2\nError rates for different combinations of the abbreviation identiﬁcation methods, including\ncombinations of guessing heuristics (GH), lexical lookup (LL), and the document-centered\napproach (DCA).\n', 2, 0)
(123.74099731445312, 146.24362182617188, 340.05096435546875, 157.0929718017578, 'Abbreviation Identiﬁcation Method\nWSJ\nBrown\n', 3, 0)
(104.80999755859375, 163.57852172851562, 336.8236389160156, 214.27821350097656, 'A\nGH\n9.6%\n10.8%\nB\nLL\n12.6%\n11.9%\nC\nGH + LL\n1.2%\n2.1%\nD\nGH + DCA\n6.6%\n8.9%\nE\nGH + DCA + LL\n0.8%\n1.2%\n', 4, 0)
(104.80996704101562, 244.6526641845703, 487.3739318847656, 507.7647705078125, 'is also listed among common words and is much less frequently used in this way, we\ninclude it in the proper name list.\nThe fourth list on which our method relies is a list of known abbreviations.\nAgain, we induced this list completely automatically from an unannotated corpus.\nWe applied the abbreviation-guessing heuristics described in Section 6 to our NYT\ntext collection and then extracted the 270 most frequent abbreviations: all abbrevi-\nations that appeared ﬁve times or more. This list included honoriﬁc abbreviations\n(Mr, Dr), corporate designators (Ltd, Co), month name abbreviations (Jan, Feb), ab-\nbreviations of names of U.S. states (Ala, Cal), measure unit abbreviations (ft, kg), etc.\nAlthough we described these abbreviations in groups, this information was not en-\ncoded in the list; the only information this list provides is that a word is a known\nabbreviation.\nAmong these four lists the ﬁrst three reﬂect general language regularities and\nusually do not require modiﬁcation for handling texts from a new domain. The ab-\nbreviation list, however, is much more domain dependent and for better performance\nneeds to be reinduced for a new domain. Since the compilation of all four lists does not\nrequire data preannotated in any way, it is very easy to specialize the above-described\nlists to a particular domain: we can simply rebuild the lists using a domain-speciﬁc\ncorpus. This process is completely automatic and does not require any human labor\napart from collecting a raw domain-speciﬁc corpus. Since all cutoff thresholds that\nwe applied here were chosen by intuition, however, different domains might require\nsome new settings.\n', 5, 0)
(104.81002807617188, 519.9440307617188, 241.23284912109375, 531.7994995117188, '6. Recognizing Abbreviations\n', 6, 0)
(104.81002807617188, 543.8243408203125, 487.3739318847656, 699.3406372070312, 'The answer to the question of whether or not a particular word token is an abbreviation\nor a regular word largely solves the sentence boundary problem. In the Brown corpus\n92% of potential sentence boundaries come after regular words. The WSJ corpus is\nricher with abbreviations, and only 83% of sentences in that corpus end with a regular\nword followed by a period. In Section 3 we described the heuristics for abbreviation\nguessing and pointed out that although these heuristics are reasonably accurate, they\nfail to identify about 9.5% of abbreviations. Since unidentiﬁed abbreviations are then\ntreated as regular words, the overall error rate of the guessing heuristics was measured\nat about 10% (row A of Table 2). Thus, to improve this error rate, we need ﬁrst of all\nto improve the coverage of the abbreviation-handling strategy.\nA standard way to do this is to use the guessing heuristics in conjunction with a\nlist of known abbreviations. We decided to use the list of 270 abbreviations described\nin Section 5. First we applied only the lexical-lookup strategy to our two corpora (i.e.,\n', 7, 0)

page suivante
(472.431396484375, 722.0972290039062, 487.3752746582031, 734.1519775390625, '299\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.3785095214844, 67.42921447753906, 'Mikheev\nPeriods, Capitalized Words, etc.\n', 1, 0)
(104.80999755859375, 87.5367660522461, 487.3741455078125, 278.9186096191406, 'only when a token was found in the list of 270 known abbreviations was it marked\nas an abbreviation). This gave us an unexpectedly high error rate of about 12%, as\ndisplayed in row B of Table 2. When we investigated the reason for the high error\nrate, we found that the majority of single letters and spans of single letters sepa-\nrated by periods (e.g. Y.M.C.A.) found in the Brown corpus and the WSJ corpus were\nnot present in our abbreviation list and therefore were not recognized as abbrevia-\ntions.\nSuch cases, however, are handled well by the abbreviation-guessing heuristics.\nWhen we applied the abbreviation list together with the abbreviation-guessing heuris-\ntics (row C of Table 2), this gave us a very strong performance on the WSJ corpus:\nan error rate of 1.2%. On the Brown corpus, the error rate was higher: 2.1%. This can\nbe explained by the fact that we collected our abbreviation list from a corpus of news\narticles that is not too dissimilar to the texts in the WSJ corpus and thus, this list con-\ntained many abbreviations found in that corpus. The Brown corpus, in contrast, ranges\nacross several different domains and sublanguages, which makes it more difﬁcult to\ncompile a list from a single corpus to cover it.\n', 2, 0)
(104.8099365234375, 291.80023193359375, 487.3738708496094, 614.6580200195312, '6.1 Unigram DCA\nThe abbreviation-guessing heuristics supplemented with a list of abbreviations are\naccurate, but they still can miss some abbreviations. For instance, if an abbreviation\nlike sec or Okla. is followed by a capitalized word and is not listed in the list of\nabbreviations, the guessing heuristics will not uncover them. We also would like to\nboost the abbreviation handling with a domain-independent method that enables the\nsystem to function even when the abbreviation list is not much of a help. Thus, in\naddition to the list of known abbreviations and the guessing heuristics, we decided to\napply the DCA as described below.\nEach word of length four characters or less that is followed by a period is treated as\na potential abbreviation. First, the system collects unigrams of potential abbreviations\nin unambiguous contexts from the document under processing. If a potential abbre-\nviation is used elsewhere in the document without a trailing period, we can conclude\nthat it in fact is not an abbreviation but rather a regular word (nonabbreviation). To\ndecide whether a potential abbreviation is really an abbreviation, we look for contexts\nin which it is followed by a period and then by a lower-cased word, a number, or a\ncomma.\nFor instance, the word Kong followed by a period and then by a capitalized word\ncannot be safely classiﬁed as a regular word (nonabbreviation), and therefore it is a\npotential abbreviation. But if in the same document we detect a context lived in Hong\nKong in 1993, this indicates that Kong in this document is normally written without\na trailing period and hence is not an abbreviation. Having established that, we can\napply this information to nonevident contexts and classify Kong as a regular word\nthroughout the document. However, if we detect a context such as Kong., said, this\nindicates that in this document, Kong is normally written with a trailing period and\nhence is an abbreviation. This gives us grounds for classifying Kong as an abbreviation\nin all its occurrences within the same document.\n', 3, 0)
(104.80992126464844, 627.5396728515625, 487.37359619140625, 699.3401489257812, '6.2 Bigram DCA\nThe DCA relies on the assumption that there is a consistency in writing within the\nsame document. Different authors can write Mr or Dr with or without a trailing period,\nbut we assume that the same author (the author of a particular document) writes\nit in the same way consistently. A situation can arise, however, in which the same\npotential abbreviation is used as a regular word and as an abbreviation within the same\n', 4, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '300\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(104.80990600585938, 87.5367660522461, 487.3739013671875, 314.783935546875, 'document. This is usually the case when an abbreviation coincides with a regular word,\nfor example Sun. (meaning Sunday) and Sun (the name of a newspaper). To tackle\nthis problem, the system can collect from a document not only unigrams of potential\nabbreviations in unambiguous contexts but also their bigrams with the preceding\nword. Of course, as in the case with unigrams, the bigrams are collected on the ﬂy\nand completely automatically.\nFor instance, if the system ﬁnds a context vitamin C is, it stores the bigram vita-\nmin C and the unigram C with the information that C is a regular word. If in the\nsame document the system also detects a context John C. later said, it stores the bi-\ngram John C and the unigram C with the information that C is an abbreviation. Here\nwe have conﬂicting information for the word C: it was detected to act as a regu-\nlar word and as an abbreviation within the same document, so there is not enough\ninformation to resolve ambiguous cases purely using the unigram. Some cases, how-\never, can still be resolved on the basis of the bigrams. The system will assign C as a\nregular word (nonabbreviation) in an ambiguous context such as vitamin C. Research\nbecause of the stored vitamin C bigram. Obviously from such a short context, it is\ndifﬁcult even for a human to make a conﬁdent decision, but the evidence gathered\nfrom the entire document can inﬂuence this decision with a high degree of conﬁ-\ndence.\n', 2, 0)
(104.80990600585938, 333.64312744140625, 487.3738708496094, 548.9048461914062, '6.3 Resulting Approach\nWhen neither unigrams nor bigrams can help to resolve an ambiguous context for a\npotential abbreviation, the system decides in favor of the more frequent category for\nthat abbreviation. If the word In was detected to act as a regular word (preposition) ﬁve\ntimes in the current document and two times as abbreviation (for the state Indiana), in\na context in which neither of the bigrams collected from the document can be applied,\nIn is assigned as a regular word (nonabbreviation). The last-resort strategy is to assign\nall nonresolved cases as nonabbreviations.\nRow D of Table 2 shows the results when we applied the abbreviation-guessing\nheuristics together with the DCA. On the WSJ corpus, the DCA reduced the error rate\nof the guessing heuristics alone (row A) by about 30%; on the Brown corpus its impact\nwas somewhat smaller, about 18%. This can be explained by the fact that abbreviations\nin the WSJ corpus have a much higher repetition rate, which is very important for\nthe DCA.\nWe also applied the DCA together with the lexical lookup and the guessing heuris-\ntics. This reduced the error rate on abbreviation identiﬁcation by about 30% in com-\nparison with the list and guessing heuristics conﬁguration, as can be seen in row E of\nTable 2.\n', 3, 0)
(104.8099365234375, 567.7640380859375, 278.1063232421875, 579.6195068359375, '7. Disambiguating Capitalized Words\n', 4, 0)
(104.8099365234375, 591.6444091796875, 487.3718566894531, 699.3401489257812, 'The second key task of our approach is the disambiguation of capitalized words that\nfollow a potential sentence boundary punctuation sign. Apart from being an important\ncomponent in the task of text normalization, information about whether or not a\ncapitalized word that follows a period is a common word is crucial for the SBD task,\nas we showed in Section 3. We tackle capitalized words in a similar fashion as we\ntackled the abbreviations: through a document-centered approach that analyzes on\nthe ﬂy the distribution of ambiguously capitalized words in the entire document. This\nis implemented as a cascade of simple strategies, which were brieﬂy described in\nMikheev (1999).\n', 5, 0)

page suivante
(472.431396484375, 722.0972290039062, 487.3752746582031, 734.1519775390625, '301\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.3785095214844, 67.42921447753906, 'Mikheev\nPeriods, Capitalized Words, etc.\n', 1, 0)
(104.80999755859375, 87.566650390625, 487.3741760253906, 697.3475952148438, '7.1 The Sequence Strategy\nThe ﬁrst DCA strategy for the disambiguation of ambiguous capitalized words is to\nexplore sequences of words extracted from contexts in which the same words are used\nunambiguously with respect to their capitalization. We call this the sequence strategy.\nThe rationale behind this strategy is that if there is a phrase of two or more capitalized\nwords starting from an unambiguous position (e.g., following a lower-cased word),\nthe system can be reasonably conﬁdent that even when the same phrase starts from an\nunreliable position (e.g., after a period), all its words still have to be grouped together\nand hence are proper nouns. Moreover, this applies not just to the exact replication of\nthe capitalized phrase, but to any partial ordering of its words of size two characters\nor more preserving their sequence.\nFor instance, if a phrase Rocket Systems Development Co. is found in a document\nstarting from an unambiguous position (e.g., after a lower-cased word, a number, or a\ncomma), the system collects it and also generates its partial-order subphrases: Rocket\nSystems, Rocket Systems Co., Rocket Co., Systems Development, etc. If then in the same\ndocument Rocket Systems is found in an ambiguous position (e.g., after a period), the\nsystem will assign the word Rocket as a proper noun because it is part of a multiword\nproper name that was seen in the unambiguous context.\nA span of capitalized words can also internally include alpha-numerals, abbrevia-\ntions with internal periods, symbols, and lower-cased words of length three characters\nor shorter. This enables the system to capture phrases like A & M and The Phantom of\nthe Opera. Partial orders from such phrases are generated in a similar way, but with\nthe restriction that every generated subphrase should start and end with a capitalized\nword.\nThe sequence strategy can also be applied to disambiguate common words. Since\nin the case of common words the system cannot determine boundaries of a phrase,\nonly bigrams of the lower-cased words with their following words are collected from\nthe document. For instance, from a context continental suppliers of Mercury, the sys-\ntem collects three bigrams: continental suppliers, suppliers of, and of Mercury. When the\nsystem encounters the phrase Continental suppliers after a period, it can now use the\ninformation that in the previously stored bigram continental suppliers, the word token\ncontinental was written lower-cased and therefore was unambiguously used as a com-\nmon word. On this basis the system can assign the ambiguous capitalized word token\nContinental as a common word.\nRow A of Table 3 displays the results obtained in the application of the sequence\nstrategy to the Brown corpus and the WSJ corpus. The sequence strategy is extremely\nuseful when we are dealing with names of organizations, since many of them are\nmultiword phrases composed of common words. For instance, the words Rocket and\nInsurance can be used both as proper names and common words within the same\ndocument. The sequence strategy maintains contexts of the usages of such words\nwithin the same document, and thus it can disambiguate such usages in the ambiguous\npositions matching surrounding words. And indeed, the error rate of this strategy\nwhen applied to proper names was below 1%, with coverage of about 9–12%.\nFor tagging common words the sequence strategy was also very accurate (error\nrate less than 0.3%), covering 17% of ambiguous capitalized common words on the\nWSJ corpus and 25% on the Brown corpus. The higher coverage on the Brown corpus\ncan be explained by the fact that the documents in that corpus are in general longer\nthan those in the WSJ corpus, which enables more word bigrams to be collected from\na document.\nDual application of the sequence strategy contributes to its robustness against po-\ntential capitalization errors in the document. The negative evidence (not proper name)\n', 2, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '302\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(104.80999755859375, 98.44984436035156, 475.7463684082031, 129.1974639892578, 'Table 3\nFirst part: Error rates of different individual strategies for capitalized-word disambiguation.\nSecond part: Error rates of the overall cascading application of the individual strategies.\n', 2, 0)
(121.74839782714844, 136.28091430664062, 481.2391357421875, 160.08131408691406, 'Strategy\nWord Class\nError Rate\nCoverage\nWSJ\nBrown\nWSJ\nBrown\n', 3, 0)
(104.80999755859375, 166.56741333007812, 486.9813537597656, 257.1174621582031, 'A\nSequence strategy\nProper Names\n0.12%\n0.97%\n12.6%\n8.82%\nSequence strategy\nCommon Words\n0.28%\n0.21%\n17.68%\n26.5%\nB\nFrequent-list lookup strategy\nProper Names\n0.49%\n0.16%\n2.62%\n6.54%\nFrequent-list lookup strategy\nCommon Words\n0.21%\n0.14%\n64.62%\n61.20%\nC\nSingle-word assignment strategy\nProper Names\n3.18%\n1.96%\n18.77%\n34.13%\nSingle-word assignment strategy\nCommon Words\n6.51%\n2.87%\n3.07%\n4.78%\nD\nCascading DCA\nProper/Common\n1.10%\n0.76%\n84.12%\n91.76%\nE\nCascading DCA\nProper/Common\n4.88%\n2.83%\n100.0%\n100.0%\nand lexical lookup\n', 4, 0)
(104.80996704101562, 303.536865234375, 487.3738098144531, 387.322265625, 'is used together with the positive evidence (proper name) and blocks assignment when\nconﬂicts are found. For instance, if the system detects a capitalized phrase The President\nin an unambiguous position, then the sequence strategy will treat the word the as part\nof the proper name The President even when this phrase follows a period. If in the\nsame document, however, the system detects alternative evidence (e.g., the President,\nwhere the is not part of the proper name), it then will block as unsafe the assignment\nof The as a proper name in ambiguous usages of The President.\n', 5, 0)
(104.8099365234375, 400.3931884765625, 487.3738098144531, 699.3408813476562, '7.2 Frequent-List Lookup Strategy\nThe frequent-list lookup strategy applies lookup of ambiguously capitalized words\nin two word lists. The ﬁrst list contains common words that are frequently found\nin sentence-starting positions, and the other list contains the most frequent proper\nnames. Both these lists can be compiled completely automatically, as explained in\nsection 5. Thus, if an ambiguous capitalized word is found in the list of frequent\nsentence-starting common words, it is assigned as a common word, and if it is found\nin the list of frequent proper names, it is assigned as a proper name. For instance,\nthe word token The when used after a period will be recognized as a common word,\nbecause The is a frequent sentence-starting common word. The Word token Japan in a\nsimilar context will be recognized as a proper name, because Japan is a member of the\nfrequent-proper-name list.\nNote, however, that this strategy is applied after the sequence strategy and thus, a\nword listed in one of the lists will not necessarily be marked according to its list class.\nThe list lookup assignment is applied only to the ambiguously capitalized words that\nhave not been handled by the sequence strategy.\nRow B of Table 3 displays the results of the application of the frequent-list lookup\nstrategy to the Brown corpus and the WSJ corpus. The frequent-list lookup strategy\nproduced an error rate of less than 0.5%. A few wrong assignments came from phrases\nlike Mr. A and Mrs. Someone and words in titles like I’ve Got a Dog, where A, Someone,\nand I were recognized as common words although they were tagged as proper nouns\nin the text. The frequent-list lookup strategy is not very effective for proper names,\nwhere it covered under 7% of candidates in the Brown corpus and under 3% in the\nWSJ corpus, but it is extremely effective for common words: it covered over 60% of\nambiguous capitalized common words.\n', 6, 0)

page suivante
(472.431396484375, 722.0972290039062, 487.3752746582031, 734.1519775390625, '303\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.3785095214844, 67.42921447753906, 'Mikheev\nPeriods, Capitalized Words, etc.\n', 1, 0)
(104.80998229980469, 87.566650390625, 487.37396240234375, 541.9309692382812, '7.3 Single-Word Assignment\nThe sequence strategy is accurate, but it covers only 9–12% of proper names in ambigu-\nous positions. The frequent-list lookup strategy is mostly effective for common words.\nTo boost the coverage on the proper name category, we introduced another DCA\nstrategy. We call this strategy single-word assignment, and it can be summarized as\nfollows: if a word in the current document is seen capitalized in an unambiguous po-\nsition and at the same time it is not used lower-cased anywhere in the document, this\nword in this particular document is very likely to stand for a proper name even when\nused capitalized in ambiguous positions. And conversely, if a word in the current\ndocument is used only lower-cased (except in ambiguous positions), it is extremely\nunlikely that this word will act as a proper name in an ambiguous position and thus,\nsuch a word can be marked as a common word.\nNote that by the time single-word assignment is implemented, the sequence strat-\negy and the frequent-list lookup strategy have been already applied and all high-\nfrequency sentence-initial words have been assigned. This ordering is important, be-\ncause even if a high-frequency common word is observed in a document only as a\nproper name (usually as part of a multiword proper name), it is still not safe to mark\nit as a proper name in ambiguous positions.\nRow C of Table 3 displays the results of the application of the single-word assign-\nment strategy to the Brown corpus and the WSJ corpus. The single-word assignment\nstrategy is useful for proper-name identiﬁcation: although it is not as accurate as the\nsequence strategy, it still produces a reasonable error rate at the same time boosting the\ncoverage considerably (19–34%). On common words this method is not as effective,\nwith an error rate as high as 6.61% on the WSJ corpus and a coverage below 5%.\nThe single-word-assignment strategy handles well the so-called unknown-word\nproblem, which arises when domain-speciﬁc lexica are missing from a general vocab-\nulary. Since our system is not equipped with a general vocabulary but rather builds a\ndocument-speciﬁc vocabulary on the ﬂy,” important domain-speciﬁc words are iden-\ntiﬁed and treated similarly to all other words.\nA generally difﬁcult case for the single-word assignment strategy arises when a\nword is used both as a proper name and as a common word in the same document, es-\npecially when one of these usages occurs only in an ambiguous position. For instance,\nin a document about steel, the only occurrence of Steel Company happened to start\na sentence. This produced an erroneous assignment of the word Steel as a common\nword. Another example: in a document about the Acting Judge, the word acting in a\nsentence Acting on behalf. . . was wrongly classiﬁed as a proper name. These difﬁculties,\nhowever, often are compensated for by the sequence strategy, which is applied prior\nto the single-word assignment strategy and tackles such cases using n-grams of words.\n', 2, 0)
(104.80999755859375, 554.8126220703125, 487.3727722167969, 614.657958984375, '7.4 Quotes, Brackets, and “After Abbr.” Heuristic\nCapitalized words in quotes and brackets do not directly contribute to our primary\ntask of sentence boundary disambiguation, but they still present a case of ambiguity\nfor the task of capitalized-word disambiguation. To tackle them we applied two simple\nheuristics:\n', 3, 0)
(122.74267578125, 632.4910278320312, 457.021728515625, 656.5009155273438, '•\nIf a single capitalized word is used in quotes or brackets it is a proper\nnoun (e.g., John (Cool) Lee).\n', 4, 0)
(122.74267578125, 663.3751220703125, 469.4411315917969, 699.340087890625, '•\nIf there is a lowercased word, a number, or a comma that is followed by\nan opening bracket and then by a capitalized word, this capitalized word\nis a proper noun (e.g., . . . happened (Moscow News reported yesterday) but. . . ).\n', 5, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '304\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(104.80999755859375, 87.5367660522461, 487.37384033203125, 219.1428985595703, 'These heuristics are reasonably accurate: they achieved under 2% error rate on our\ntwo test corpora, but they covered only about 6–7% of proper names.\nWhen we studied the distribution of capitalized words after capitalized abbrevi-\nations, we uncovered an interesting empirical fact. A capitalized word that follows\na capitalized abbreviation is almost certainly a proper name unless it is listed in the\nlist of frequent sentence-starting common words (i.e., it is not The, However, etc.). The\nerror rate of this heuristic is about 0.8% and, not surprisingly, in 99.5% of cases the\nabbreviation and the following proper name belonged to the same sentence. Naturally,\nthe coverage of this “after abbr.” heuristic depends on the proportion of capitalized\nabbreviations in the text. In our two corpora this heuristic disambiguated about 20%\nof ambiguous capitalized proper names.\n', 2, 0)
(104.80999755859375, 232.0245361328125, 487.3738708496094, 447.2863464355469, '7.5 Tagging Proper Names: The Overall Performance\nIn general, the cascading application of the above-described strategies achieved an\nerror rate of about 1%, but it left unclassiﬁed about 9% of ambiguous capitalized\nwords in the Brown corpus and 15% of such words in the WSJ corpus. Row D of\nTable 3 displays the results of the application of the cascading application of the\ncapitalized-word disambiguation strategies to the Brown corpus and the WSJ corpus.\nFor the proper-name category, the most productive strategy was single-word as-\nsignment, followed by the “after abbr.” strategy, and then the sequence strategy. For\ncommon words, the most productive was the frequent-list lookup strategy, followed\nby the sequence strategy.\nSince our system left unassigned 10–15% of ambiguous capitalized words, we have\nto decide what to do with them. To keep our system simple and domain independent,\nwe opted for the lexical-lookup strategy that we evaluated in Section 3. This strategy,\nof course, is not very accurate, but it is applied only to the unassigned words. Row E\nof Table 3 displays the results of applying the lexical-lookup strategy after the DCA\nmethods. We see that the error rate went up in comparison to the DCA-only method\nby more than three times (2.9% on the Brown corpus and 4.9% on the WSJ corpus),\nbut no unassigned ambiguous capitalized words are left in the text.\n', 3, 0)
(104.80999755859375, 460.16796875, 372.6665344238281, 472.0234680175781, '8. Putting It All Together: Assigning Sentence Boundaries\n', 4, 0)
(104.80999755859375, 484.04833984375, 487.3739929199219, 699.3401489257812, 'After abbreviations have been identiﬁed and capitalized words have been classiﬁed\ninto proper names and common words, the system can carry out the assignments\nof sentence boundaries using the SBD rule set described in Section 3 and listed in\nAppendix A. This rule set makes use of the observation that if we have at our dis-\nposal unambiguous (but not necessarily correct) information as to whether a particular\nword that precedes a period is an abbreviation and whether the word that follows\nthis period is a proper name, then in mixed-case texts we can easily assign a pe-\nriod (and other potential sentence termination punctuation) as a sentence break or\nnot.\nThe only ambiguous outcome is generated by the conﬁguration in which an ab-\nbreviation is followed by a proper name. We decided to handle this case by applying\na crude and simple strategy of always resolving it as “not sentence boundary.” On one\nhand, this makes our method simple and robust, but on the other hand, it imposes\nsome penalty on its performance.\nRow A of Table 4 summarizes the upper bound for our SBD approach: when we\nhave entirely correct information on the abbreviations and proper names, as explained\nin Section 3.1. There the erroneous assignments come only from the crude treatment\nof abbreviations that are followed by proper names.\n', 5, 0)

page suivante
(472.431396484375, 722.0972290039062, 487.3752746582031, 734.1519775390625, '305\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.3785095214844, 67.42921447753906, 'Mikheev\nPeriods, Capitalized Words, etc.\n', 1, 0)
(104.80999755859375, 98.44984436035156, 449.8128662109375, 129.1974639892578, 'Table 4\nError rates measured on the SBD, capitalized-word disambiguation, and abbreviation\nidentiﬁcation tasks achieved by different methods described in this article.\n', 2, 0)
(115.7708969116211, 138.27346801757812, 441.585693359375, 149.12281799316406, 'Method\nBrown Corpus\nWSJ Corpus\n', 3, 0)
(212.78700256347656, 157.20254516601562, 483.8277587890625, 178.0144805908203, 'SBD\nCapitalized Abbreviations SBD\nCapitalized Abbreviations\nwords\nwords\n', 4, 0)
(104.80999755859375, 184.50015258789062, 446.83245849609375, 265.0876159667969, 'A Upper bound\n0.01% 0.0%\n0.0%\n0.13%\n0.0%\n0.0%\nB Lower bound\n2.00% 7.40%\n10.8%\n4.10% 15.0%\n9.6%\nC Best quoted\n0.20% 3.15%\n—\n0.50%\n4.72%\n—\nD DCA\n0.28% 2.83%\n0.8%\n0.45%\n4.88%\n1.2%\nE DCA (no abbreviations 0.65% 2.89%\n8.9%\n1.41%\n4.92%\n6.6%\nlexicon)\nF POS tagger\n0.25% 3.15%\n1.2%\n0.39%\n4.72%\n2.1%\nG POS tagger + DCA\n0.20% 1.87%\n0.8%\n0.31%\n3.22%\n1.2%\n', 5, 0)
(104.81002044677734, 292.76727294921875, 487.3738708496094, 699.3407592773438, 'Row B of Table 4 summarizes the lower-bound results. The lower bound for our\napproach was estimated by applying the lexical-lookup strategy for capitalized-word\ndisambiguation together with the abbreviation-guessing heuristics to feed the SBD\nrule set, as described in Section 3.2. Here we see a signiﬁcant impact of the infelicities\nin the disambiguation of capitalized words and abbreviations on the performance of\nthe SBD rule set.\nRow C of Table 4 summarizes the highest results known to us (for all three tasks)\nproduced by automatic systems on the Brown corpus and the WSJ corpus. State-of-the-\nart machine learning and rule-based SBD systems achieve an error rate of 0.8–1.5%\nmeasured on the Brown corpus and the WSJ corpus. The best performance on the\nWSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst\n1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate. The best\nperformance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who\ntrained a decision tree classiﬁer on a 25-million-word corpus. In the disambiguation of\ncapitalized words, the most widespread method is POS tagging, which achieves about\na 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported\nin Mikheev (2000). We are not aware of any studies devoted to the identiﬁcation of\nabbreviations with comprehensive evaluation on either the Brown corpus or the WSJ\ncorpus.\nIn row D of Table 4, we summarized our main results: the results obtained by the\napplication of our SBD rule set, which uses the information provided by the DCA to\ncapitalized word disambiguation applied together with lexical lookup (as described\nin Section 7.5), and the abbreviation-handling strategy, which included the guessing\nheuristics, the DCA, and the list of 270 abbreviations (as described in Section 6). As\ncan be seen in the table, the performance of this system is almost indistinguishable\nfrom the best previously quoted results. On proper-name disambiguation, it achieved\na 2.83% error rate on the Brown corpus and a 4.88% error rate on the WSJ corpus.\nOn the SBD task, it achieved a 0.28% error rate on the Brown corpus and a 0.45%\nerror rate on the WSJ corpus. If we compare these results with the upper bound\nfor our SBD approach, we can see that the infelicities in proper-name and abbrevia-\ntion identiﬁcation introduced an increase of about 0.3% in the error rate on the SBD\ntask.\nTo test the adaptability of our approach to a completely new domain, we applied\nour system in a conﬁguration in which it was not equipped with the list of 270 abbre-\n', 6, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '306\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(104.80999755859375, 87.5367660522461, 487.3739013671875, 159.36720275878906, 'viations, since this list is the only domain-sensitive resource in our system. The results\nfor this conﬁguration are summarized in row E of Table 4. The error rate increase of\n5–7% on the abbreviation handling introduced about a twofold increase in the SBD\nerror rate on the Brown corpus (a 0.65% error rate) and about a threefold increase on\nthe WSJ corpus (1.41%). But these results are still comparable to those of the majority\nof currently used sentence splitters.\n', 2, 0)
(104.80999755859375, 171.9171142578125, 253.38723754882812, 183.77261352539062, '9. Detecting Limits for the DCA\n', 3, 0)
(104.80999755859375, 195.7974395751953, 487.3728942871094, 542.5953979492188, 'Since our DCA method relies on the assumption that the words it tries to disam-\nbiguate occur multiple times in a document, its performance clearly should depend\non the length of the document: very short documents possibly do not provide enough\ndisambiguation clues, whereas very long documents possibly contain too many clues\nthat cancel each other.\nAs noted in Section 2.1, the average length of the documents in the Brown corpus\nis about 2,300 words. Also, the documents in that corpus are distributed very densely\naround their mean. Thus not much can be inferred about the dependency of the perfor-\nmance of the method on document length apart from the observation that documents\n2,000–3,000 words long are handled well by our approach. In the WSJ corpus, the aver-\nage length of the document is about 500 words, and therefore we could investigate the\neffect of short documents on the performance. We divided documents into six groups\naccording to their length and plotted the error rate for the SBD and capitalized-word\ndisambiguation tasks as well as the number of documents in a group, as shown in\nFigure 2. As can be seen in the ﬁgure, short documents (50 words or less) have the\nhighest average error rate both for the SBD task (1.63) and for the capitalized-word\ndisambiguation task (5.25). For documents 50 to 100 words long, the error rate is still\na bit higher than normal, and for longer documents the error rate stabilizes around\n1.5 for the capitalized-word disambiguation task and 0.3 for the SBD task. The error\nrate on documents 2,000 words long and higher is almost identical to that registered\non the Brown corpus on documents of the same length.\nThus here we can conclude that the proposed approach tends not to be very\neffective for documents shorter than 50 words (one to three sentences), but it handles\nwell documents up to 4,000 words long. Since our corpora did not contain documents\nsigniﬁcantly longer than that, we could not estimate whether or when the performance\nof our method signiﬁcantly deteriorates on longer documents. We also evaluated the\nperformance of the method on different subcorpora of the Brown corpus: the most\ndifﬁcult subdomains proved to be scientiﬁc texts, spoken-language transcripts, and\njournalistic texts, whereas ﬁction was the easiest genre for the system.\n', 4, 0)
(104.80999755859375, 555.144287109375, 297.1459655761719, 566.999755859375, '10. Incorporating DCA into a POS Tagger\n', 5, 0)
(104.80999755859375, 579.024658203125, 487.372802734375, 626.9447631835938, 'To test our hypothesis that DCA can be used as a complement to a local-context\napproach, we combined our main conﬁguration (evaluated in row D of Table 4) with\na POS tagger. Unlike other POS taggers, this POS tagger (Mikheev 2000) was also\ntrained to disambiguate sentence boundaries.\n', 6, 0)
(104.80999755859375, 639.4946899414062, 487.372802734375, 699.3400268554688, '10.1 Training a POS Tagger\nIn our markup convention (Section 2), periods are tokenized as separate tokens re-\ngardless of whether they stand for fullstops or belong to abbreviations. Consequently\na POS tagger can naturally treat them similarly to any other ambiguous words. There\nis, however, one difference in the implementation of such a tagger. Normally, a POS\n', 7, 0)

page suivante
(472.431396484375, 722.0972290039062, 487.3752746582031, 734.1519775390625, '307\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.3785095214844, 67.42921447753906, 'Mikheev\nPeriods, Capitalized Words, etc.\n', 1, 0)
(149.80999755859375, 248.05162048339844, 152.50999450683594, 254.07801818847656, 'x\n', 2, 0)
(185.8101806640625, 288.5516052246094, 188.5101776123047, 294.5780029296875, 'x\n', 3, 0)
(221.81036376953125, 308.8016052246094, 224.51036071777344, 314.8280029296875, 'x\n', 4, 0)
(257.810546875, 333.5514221191406, 260.51055908203125, 339.57781982421875, 'x\n', 5, 0)
(293.81072998046875, 320.0514221191406, 332.51092529296875, 328.3280029296875, 'x\nx\n', 6, 0)
(365.81109619140625, 308.8016052246094, 368.5111083984375, 314.8280029296875, 'x\n', 7, 0)
(149.81109619140625, 317.8017883300781, 152.51109313964844, 323.82818603515625, 'o\n', 8, 0)
(185.811279296875, 353.8019714355469, 188.5112762451172, 359.828369140625, 'o\n', 9, 0)
(221.81146240234375, 335.8021545410156, 224.51145935058594, 341.82855224609375, 'o\n', 10, 0)
(257.8116455078125, 351.5523376464844, 330.2618408203125, 364.3287353515625, 'o\no\no\n', 11, 0)
(363.56201171875, 347.0525207519531, 366.26202392578125, 353.07891845703125, 'o\n', 12, 0)
(149.8121795654297, 221.05271911621094, 152.20977783203125, 227.07911682128906, 'c\n', 13, 0)
(185.81236267089844, 164.8025360107422, 188.2099609375, 170.8289337158203, 'c\n', 14, 0)
(221.8125457763672, 122.05235290527344, 296.21051025390625, 134.82875061035156, 'c\nc\nc\n', 15, 0)
(329.8130798339844, 162.55235290527344, 332.210693359375, 168.57875061035156, 'c\n', 16, 0)
(354.5599060058594, 275.0521545410156, 437.0449523925781, 288.6127014160156, 'c\nNumber of Documents\n', 17, 0)
(336.5599060058594, 296.5686950683594, 409.27996826171875, 306.6127014160156, 'Cap.Word error rate\n', 18, 0)
(336.5599060058594, 337.0686950683594, 389.5429382324219, 347.1127014160156, 'SBD error rate\n', 19, 0)
(149.80990600585938, 370.8186950683594, 383.8099060058594, 380.8627014160156, '50\n100\n200\n500\n1000\n2000\n3000\n', 20, 0)
(104.80990600585938, 325.8186950683594, 109.30990600585938, 335.8627014160156, '1\n', 21, 0)
(104.80990600585938, 289.8186950683594, 109.30990600585938, 299.8627014160156, '2\n', 22, 0)
(104.80990600585938, 256.0686950683594, 109.30990600585938, 266.1127014160156, '3\n', 23, 0)
(122.80990600585938, 217.81869506835938, 136.30990600585938, 227.86270141601562, '200\n', 24, 0)
(122.80990600585938, 116.56869506835938, 136.30990600585938, 126.6126937866211, '600\n', 25, 0)
(397.3099060058594, 357.3186950683594, 442.3009033203125, 367.3627014160156, 'Doc. Length\n', 26, 0)
(107.05990600585938, 235.81869506835938, 144.7969207763672, 245.86270141601562, 'Error Rate\n', 27, 0)
(109.30990600585938, 96.31869506835938, 169.29490661621094, 106.3626937866211, 'Number of Docs\n', 28, 0)
(122.80990600585938, 161.56869506835938, 136.30990600585938, 171.61270141601562, '400\n', 29, 0)
(104.80999755859375, 379.4869689941406, 460.216552734375, 410.2345275878906, 'Figure 2\nDistribution of the error rate and the number of documents across the document length\n(measured in word tokens) in the WSJ corpus.\n', 30, 0)
(104.80999755859375, 460.13958740234375, 487.3739013671875, 699.3416137695312, 'tagger operates on text spans that form a sentence. This requires resolving sentence\nboundaries before tagging. We see no good reason, however, why such text spans\nshould necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden\nMarkov Model [HMM] [Kupiec 1992], Brill’s [Brill 1995a], and MaxEnt [Ratnaparkhi\n1996]) do not attempt to parse an entire sentence and operate only in the local win-\ndow of two to three tokens. The only reason why taggers traditionally operate on\nthe sentence level is that a sentence naturally represents a text span in which POS\ninformation does not depend on the previous and following history.\nThis issue can be also addressed by breaking the text into short text spans at\npositions where the previous tagging history does not affect current decisions. For\ninstance, a bigram tagger operates within a window of two tokens, and thus a se-\nquence of word tokens can be terminated at an unambiguous word token, since this\nunambiguous word token will be the only history used in tagging of the next token.\nAt the same time since this token is unambiguous, it is not affected by the history.\nA trigram tagger operates within a window of three tokens, and thus a sequence of\nword tokens can be terminated when two unambiguous words follow each other.\nUsing Penn Treebank with our tokenization convention (Section 2), we trained a\ntrigram HMM POS tagger. Words were clustered into ambiguity classes (Kupiec 1992)\naccording to the sets of POS tags they can take on. The tagger predictions were based\non the ambiguity class of the current word, abbreviation/capitalization information,\n', 31, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '308\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(104.80999755859375, 87.5367660522461, 220.83740234375, 99.59150695800781, 'and trigrams of POS tags:\n', 2, 0)
(162.08499145507812, 117.31514739990234, 291.1519775390625, 130.5355682373047, 'P(t1 . . . tnO1 . . . On) = argmax\n', 3, 0)
(292.81201171875, 107.69528198242188, 305.5442199707031, 131.03350830078125, 'i=n\n\x01\n', 4, 0)
(293.4067077636719, 117.4048080444336, 430.10015869140625, 140.6297149658203, 'i=1\nP(Oi | ti) ∗ P(ti | ti−1ti−2ai−1)\n', 5, 0)
(104.810302734375, 146.16383361816406, 487.3742370605469, 504.917236328125, 'where ti is a disambiguated POS tag of the ith word, ai is the abbreviation ﬂag of\nthe ith word, and Oi is the observation at the ith position, which in our case is\nthe ambiguity class the word belongs to, its capitalization, and its abbreviation ﬂag\n(AmbClassi, ai, Capi). Since the abbreviation ﬂag of the previous word strongly inﬂu-\nences period disambiguation, it was included in the standard trigram model.\nWe decided to train the tagger with the minimum of preannotated resources. First,\nwe used 20,000 tagged words to “bootstrap” the training process, because purely un-\nsupervised techniques, at least for the HMM class of taggers, yield lower precision.\nWe also used our DCA system to assign capitalized words, abbreviations, and sen-\ntence breaks, retaining only cases assigned by the strategies with an accuracy not less\nthan 99.8%. This was done because purely unsupervised techniques (e.g., Baum-Welch\n[Baum and Petrie 1966] or Brill’s [Brill 1995b]) enable regularities to be induced for\nword classes which contain many entries, exploiting the fact that individual words that\nbelong to a POS class occur in different ambiguity patterns. Counting all possible POS\ncombinations in these ambiguity patterns over multiple patterns usually produces the\nright combinations as the most frequent. Periods as many other closed-class words\ncannot be successfully covered by such technique.\nAfter bootstrapping we applied the forward-backward (Baum-Welch) algorithm\n(Baum and Petrie 1966) and trained our tagger in the unsupervised mode, that is, with-\nout using the annotation available in the Brown corpus and the WSJ corpus. For\nevaluation purposes we trained (and bootstrapped) our tagger on the Brown corpus\nand applied it to the WSJ corpus and vice versa. We preferred this method to tenfold\ncross-validation because it allowed us to produce only two tagging models instead of\ntwenty and also enabled us to test the tagger in harsher conditions, that is, when it is\napplied to texts that are very distant from the ones on which it was trained.\nThe overall performance of the tagger was close to 96%, which is a bit lower\nthan the best quoted results. This can be accounted for by the fact that training and\nevaluation were performed on two very different text corpora, as explained above.\nThe performance of the tagger on our target categories (periods and proper names)\nwas very close to that of the DCA method, as can be seen in row F of Table 4.\n', 6, 0)
(104.81033325195312, 516.8026123046875, 487.3731384277344, 648.3787231445312, '10.2 POS Tagger and the DCA\nWe felt that the DCA method could be used as a complement to the POS tagger, since\nthese techniques employ different types of information: in-document distribution and\nlocal context. Thus, a hybrid system can deliver at least two advantages. First, 10–15%\nof the ambiguous capitalized words unassigned by the DCA can be assigned using\na standard POS-tagging method based on the local syntactic context rather than the\ninaccurate lexical-lookup approach. Second, the local context can correct some of the\nerrors made by the DCA.\nTo implement this hybrid approach we incorporated the DCA system into the\nPOS tagger. We modiﬁed the tagger model by incorporating the DCA predictions\nusing linear interpolation:\n', 7, 0)
(173.45065307617188, 655.8277587890625, 418.7346496582031, 674.627197265625, 'P(combined) = λ ∗ P(tagger) + (1 − λ) ∗ P(DCA Strategy)\n', 8, 0)
(104.81027221679688, 675.33251953125, 487.37408447265625, 699.3424072265625, 'where P(DCA Strategy) is the accuracy of a speciﬁc DCA strategy and P(tagger) is the\nprobability assigned by the tagger’s model. Although it was possible to estimate an\n', 9, 0)

page suivante
(472.431396484375, 722.0972290039062, 487.3752746582031, 734.1519775390625, '309\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.3785095214844, 67.42921447753906, 'Mikheev\nPeriods, Capitalized Words, etc.\n', 1, 0)
(104.80998992919922, 87.5367660522461, 487.3739013671875, 255.00831604003906, 'optimal value for λ from the tagged corpus, we decided simply to set it to be 0.5 (i.e.,\ngiving similar weight to both sources of information). Instead of using the SBD rule\nset described in Section 3, in this conﬁguration, period assignments were handled by\nthe tagger’s model.\nRow G of Table 4 displays the results of the application of the hybrid system. We\nsee an improvement on proper-name recognition in comparison to the DCA or POS-\ntagging approaches (rows D and F) by about a 30–40% cut in the error rate: an overall\nerror rate of 1.87% on the Brown corpus and of 3.22% on the WSJ corpus. In turn this\nenabled better tagging of sentence boundaries: a 0.20% error rate on the Brown corpus\nand a 0.31% error rate on the WSJ corpus, which corresponds to about a 20% cut in\nthe error rate in comparison to the DCA or the POS-tagging approaches alone.\nThus, although for applications that rely on POS tagging it probably makes more\nsense to have a single system that assigns both POS tags and sentence boundaries,\nthere is still a clear beneﬁt in using the DCA method because\n', 2, 0)
(122.74266815185547, 270.3058776855469, 466.4693603515625, 294.31573486328125, '•\nthe DCA method incorporated into the POS tagger signiﬁcantly reduced\nthe error rate on the target categories (periods and proper names).\n', 3, 0)
(122.74267578125, 300.5552978515625, 463.521484375, 324.5651550292969, '•\nthe DCA method is domain independent, whereas taggers usually need\nto be trained for each speciﬁc domain to obtain best results.\n', 4, 0)
(122.74267578125, 330.80572509765625, 469.44122314453125, 349.6051330566406, '•\nthe DCA system was used in resource preparation for training the tagger.\n', 5, 0)
(122.74267578125, 349.10003662109375, 460.1899108886719, 385.06500244140625, '•\nthe DCA system is signiﬁcantly faster than the tagger, does not require\nresource development, and for tasks that do not require full POS\ninformation, it is a preferable solution.\n', 6, 0)
(104.80999755859375, 399.99993896484375, 487.37384033203125, 435.96490478515625, 'So in general, the DCA method can be seen as an enhancer for a POS tagger and\nalso as a lightweight alternative to such a tagger when full POS information is not\nrequired.\n', 7, 0)
(104.80999755859375, 448.2129211425781, 214.29498291015625, 460.06842041015625, '11. Further Experiments\n', 8, 0)
(104.80999755859375, 472.1231689453125, 487.3738098144531, 699.3401489257812, '11.1 The Cache Extension\nOne of the features of the method advocated in this article is that the system collects\nsuggestive instances of usage for target words from each document, then applies this\ninformation during the second pass through the document (actual processing), and\nthen “forgets” what it has learned before handling another document. The main rea-\nson for not carrying over the information that has been inferred from one document\nto process another document is that in general we do not know whether this new\ndocument comes from the same corpus as the ﬁrst document, and thus the regular-\nities that have been identiﬁed in the ﬁrst document might not be useful, but rather\nharmful, when applied to that new document. When we are dealing with documents\nof reasonable length, this “forgetful” behavior does not matter much, because such\ndocuments usually contain enough disambiguation clues. As we showed in Section 8,\nhowever, when short documents of one to three sentences are being processed, quite\noften there are not enough disambiguation clues within the document itself, which\nleads to inferior performance.\nTo improve the performance on short documents, we introduced a special caching\nmodule that propagates some information identiﬁed in previously processed docu-\nments to the processing of a new one. To propagate features of individual words from\none document to processing another one is a risky strategy, since words are very\n', 9, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '310\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(104.80999755859375, 87.5367660522461, 487.3739318847656, 207.18775939941406, 'ambiguous. Word sequences, however, are much more stable and can be propagated\nacross documents. We decided to accumulate in our cache all multiword proper names\nand lower-cased word bigrams induced by the sequence strategy (Section 7.1). These\nword sequences are used by the sequence strategy exactly as are word sequences in-\nduced on the ﬂy, and then the induced on-the-ﬂy sequences are added to the cache.\nWe also add to the cache the bigrams of abbreviations and regular words induced by\nthe abbreviation-handling module, as explained in Section 6. These bigrams are used\ntogether with the bigrams induced on the ﬂy. This strategy proved to be quite useful:\nit covered another 2% of unresolved cases (before applying the lexical lookup), with\nan error rate of less than 1%.\n', 2, 0)
(104.80999755859375, 220.06939697265625, 487.3738708496094, 507.0618896484375, '11.2 Handling Russian News\nTo test how easy it is to apply the DCA to a new language, we tested it on a corpus\nof British Broadcasting Corporation (BBC) news in Russian. We collected this corpus\nfrom the Internet ⟨http://news.bbc.co.uk/hi/russian/world/default.htm⟩ over a period of 30\ndays. This gave us a corpus of 300 short documents (one or two paragraphs each).\nWe automatically created the supporting resources from 364,000 documents from the\nRussian corpus of the European Corpus Initiative, using the method described in\nsection 5.\nSince, unlike English, Russian is a highly inﬂected language, we had to deal with\nthe case normalization issue. Before using the DCA method, we applied a Russian\nmorphological processor (Mikheev and Liubushkina 1995) to convert each word in\nthe text to its main form: nominative case singular for nouns and adjectives, inﬁnitive\nfor verbs, etc. For words that could be normalized to several main forms (polysemy),\nwhen secondary forms of different words coincided, we retained all the main forms.\nSince the documents in the BBC news corpus were rather short, we applied the cache\nmodule, as described in Section 11.1. This allowed us to reuse information across the\ndocuments.\nRussian proved to be a simpler case than English for our tasks. First, on average,\nRussian words are longer than English words: thus the identiﬁcation of abbreviations\nis simpler. Second, proper names in Russian coincide less frequently with common\nwords; this makes the disambiguation of capitalized words in ambiguous positions\neasier. The overall performance reached a 0.1% error rate on sentence boundaries and\na 1.8% error rate on ambiguous capitalized words, with the coverage on both tasks\nat 100%.\n', 3, 0)
(104.80999755859375, 519.9434814453125, 199.709716796875, 531.7989501953125, '12. Related Research\n', 4, 0)
(104.80999755859375, 543.853759765625, 487.3739013671875, 699.340087890625, '12.1 Research in Nonlocal Context\nThe use of nonlocal context and dynamic adaptation have been studied in language\nmodeling for speech recognition. Kuhn and de Mori (1998) proposed a cache model\nthat works as a kind of short-term memory by which the probability of the most re-\ncent n words is increased over the probability of a general-purpose bigram or trigram\nmodel. Within certain limits, such a model can adapt itself to changes in word frequen-\ncies, depending on the topic of the text passage. The DCA system is similar in spirit\nto such dynamic adaptation: it applies word n-grams collected on the ﬂy from the\ndocument under processing and favors them more highly than the default assignment\nbased on prebuilt lists. But unlike the cache model, it uses a multipass strategy.\nClarkson and Robinson (1997) developed a way of incorporating standard n-grams\ninto the cache model, using mixtures of language models and also exponentially de-\ncaying the weight for the cache prediction depending on the recency of the word’s last\n', 5, 0)

page suivante
(472.9783020019531, 722.0972290039062, 487.3742370605469, 734.1519775390625, '311\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.3785095214844, 67.42921447753906, 'Mikheev\nPeriods, Capitalized Words, etc.\n', 1, 0)
(104.80998229980469, 87.5367660522461, 487.37384033203125, 446.2901306152344, 'occurrence. In our experiments we applied simple linear interpolation to incorporate\nthe DCA system into a POS tagger. Instead of decaying nonlocal information, we opted\nfor not propagating it from one document for processing of another. For handling very\nlong documents with our method, however, the information decay strategy seems to\nbe the right way to proceed.\nMani and MacMillan (1995) pointed out that little attention had been paid in the\nnamed-entity recognition ﬁeld to the discourse properties of proper names. They pro-\nposed that proper names be viewed as linguistic expressions whose interpretation\noften depends on the discourse context, advocating text-driven processing rather than\nreliance on pre-existing lists. The DCA outlined in this article also uses nonlocal dis-\ncourse context and does not heavily rely on pre-existing word lists. It has been applied\nnot only to the identiﬁcation of proper names, as described in this article, but also to\ntheir classiﬁcation (Mikheev, Grover, and Moens 1998).\nGale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit\nonly one sense in a document or discourse (“one sense per discourse”). Since then\nthis idea has been applied to several tasks, including word sense disambiguation\n(Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999). Gale,\nChurch, and Yarowsky’s observation is also used in our DCA, especially for the iden-\ntiﬁcation of abbreviations. In capitalized-word disambiguation, however, we use this\nassumption with caution and ﬁrst apply strategies that rely not just on single words\nbut on words together with their local contexts (n-grams). This is similar to “one sense\nper collocation” idea of Yarowsky (1993).\nThe description of the EAGLE workbench for linguistic engineering (Baldwin et al.\n1997) mentions a case normalization module that uses a heuristic in which a capitalized\nword in an ambiguous position should be rewritten without capitalization if it is found\nlower-cased in the same document. This heuristic also employs a database of bigrams\nand unigrams of lower-cased and capitalized words found in unambiguous positions.\nIt is quite similar to our method for capitalized-word disambiguation. The description\nof the EAGLE case normalization module provided by Baldwin et al. is, however, very\nbrief and provides no performance evaluation or other details.\n', 2, 0)
(104.80998229980469, 466.14556884765625, 268.3390808105469, 478.0010681152344, '12.2 Research in Text Preprocessing\n', 3, 0)
(104.80996704101562, 484.0483703613281, 487.3740234375, 699.3401489257812, '12.2.1 Sentence Boundary Disambiguation. There exist two large classes of SBD sys-\ntems: rule based and machine learning. The rule-based systems use manually built\nrules that are usually encoded in terms of regular-expression grammars supplemented\nwith lists of abbreviations, common words, proper names, etc. To put together a few\nrules is fast and easy, but to develop a rule-based system with good performance is\nquite a labor-consuming enterprise. For instance, the Alembic workbench (Aberdeen et\nal. 1995) contains a sentence-splitting module that employs over 100 regular-expression\nrules written in Flex. Another well-acknowledged shortcoming of rule-based systems\nis that such systems are usually closely tailored to a particular corpus or sublanguage\nand are not easily portable across domains.\nAutomatically trainable software is generally seen as a way of producing sys-\ntems that are quickly retrainable for a new corpus, for a new domain, or even for\nanother language. Thus, the second class of SBD systems employs machine learning\ntechniques such as decision tree classiﬁers (Riley 1989), neural networks (Palmer and\nHearst 1994), and maximum-entropy modeling (Reynar and Ratnaparkhi 1997). Ma-\nchine learning systems treat the SBD task as a classiﬁcation problem, using features\nsuch as word spelling, capitalization, sufﬁx, and word class found in the local con-\ntext of a potential sentence-terminating punctuation sign. Although training of such\n', 4, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '312\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(104.80999755859375, 87.5367660522461, 487.37384033203125, 266.9634704589844, 'systems is completely automatic, the majority of machine learning approaches to the\nSBD task require labeled examples for training. This implies an investment in the data\nannotation phase.\nThe main difference between the existing machine learning and rule-based meth-\nods for the SBD task and our approach is that we decomposed the SBD task into\nseveral subtasks. We decided to tackle the SBD task through the disambiguation of\nthe period preceding and following words and then feed this information into a very\nsimple SBD rule set. In contrast, the standard practice in building SBD software is to\ndisambiguate conﬁgurations of a period with its ambiguous local context in a single\nstep, either by encoding disambiguation clues into the rules or inferring a classiﬁer\nthat accounts for the ambiguity of the words on the left and on the right of the period.\nOur approach to SBD is closer in spirit to machine learning methods because its\nretargeting does not require rule reengineering and can be done completely automat-\nically. Unlike traditional machine learning SBD approaches, however, our approach\ndoes not require annotated data for training.\n', 2, 0)
(104.80999755859375, 279.8152160644531, 487.37384033203125, 602.7028198242188, '12.2.2 Disambiguation of Capitalized Words. Disambiguation of capitalized words\nis usually handled by POS taggers, which treat capitalized words in the same way\nas other categories, that is, by accounting for the immediate syntactic context and\nusing estimates collected from a training corpus. As Church (1988) rightly pointed\nout, however, “Proper nouns and capitalized words are particularly problematic: some\ncapitalized words are proper nouns and some are not. Estimates from the Brown\nCorpus can be misleading. For example, the capitalized word ‘Acts’ is found twice in\nthe Brown Corpus, both times as a proper noun (in a title). It would be misleading to\ninfer from this evidence that the word ‘Acts’ is always a proper noun.”\nIn the information extraction ﬁeld, the disambiguation of ambiguous capitalized\nwords has always been tightly linked to the classiﬁcation of proper names into seman-\ntic classes such as person name, location, and company name. Named-entity recogni-\ntion systems usually use sets of complex hand-crafted rules that employ a gazetteer\nand a local context (Krupa and Hausman 1998). In some systems such dependencies\nare learned from labeled examples (Bikel et al. 1997). The advantage of the named-\nentity approach is that the system not only identiﬁes proper names but also determines\ntheir semantic class. The disadvantage is in the cost of building a wide-coverage set of\ncontextual clues manually or producing annotated training data. Also, the contextual\nclues are usually highly speciﬁc to the domain and text genre, making such systems\nvery difﬁcult to port.\nBoth POS taggers and named-entity recognizers are normally built using the local-\ncontext paradigm. In contrast, we opted for a method that relies on the entire distri-\nbution of a word in a document. Although it is possible to train some classes of POS\ntaggers without supervision, this usually leads to suboptimal performance. Thus the\nmajority of taggers are trained using at least some labeled data. Named-entity recog-\nnition systems are usually hand-crafted or trained from labeled data. As was shown\nabove, our method does not require supervised training.\n', 3, 0)
(104.81001281738281, 615.5545654296875, 487.37286376953125, 699.340087890625, '12.2.3 Disambiguation of Abbreviations. Not much information has been published\non abbreviation identiﬁcation. One of the better-known approaches is described in\nGrefenstette and Tapanainen (1994), which suggested that abbreviations ﬁrst be ex-\ntracted from a corpus using abbreviation-guessing heuristics akin to those described\nin Section 6 and then reused in further processing. This is similar to our treatment of\nabbreviation handling, but our strategy is applied on the document rather than corpus\nlevel. The main reason for restricting abbreviation discovery to a single document is\n', 4, 0)

page suivante
(472.431396484375, 722.0972290039062, 487.3752746582031, 734.1519775390625, '313\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.3785095214844, 67.42921447753906, 'Mikheev\nPeriods, Capitalized Words, etc.\n', 1, 0)
(104.80999755859375, 87.5367660522461, 487.3739929199219, 314.783935546875, 'that this does not presuppose the existence of a corpus in which the current document\nis similar to other documents.\nPark and Byrd (2001) recently described a hybrid method for ﬁnding abbrevia-\ntions and their deﬁnitions. This method ﬁrst applies an “abbreviation recognizer” that\ngenerates a set of “candidate abbreviations” for a document. Then for this set of can-\ndidates the system tries to ﬁnd in the text their deﬁnitions (e.g., United Kingdom for\nUK). The abbreviation recognizer for these purposes is allowed to overgenerate signif-\nicantly. There is no harm (apart from the performance issues) in proposing too many\ncandidate abbreviations, because only those that can be linked to their deﬁnitions will\nbe retained. Therefore the abbreviation recognizer treats as a candidate any token of\ntwo to ten characters that contains at least one capital letter. Candidates then are ﬁl-\ntered through a set of known common words and proper names. At the same time\nmany good abbreviations and acronyms are ﬁltered out because not for all of them\nwill deﬁnitions exist in the current document.\nIn our task we are interested in ﬁnding all and only abbreviations that end with\na period (proper abbreviations rather than acronyms), regardless of whether they can\nbe linked to their deﬁnitions in the current document or not. Therefore, in our method\nwe cannot tolerate candidate overgeneration or excessive ﬁltering and had to develop\nmore selective methods for ﬁnding abbreviations in text.\n', 2, 0)
(104.81002807617188, 326.6693115234375, 170.9248504638672, 338.5248107910156, '13. Discussion\n', 3, 0)
(104.81002807617188, 350.5496826171875, 487.3728942871094, 410.42486572265625, 'In this article we presented an approach that tackles three important aspects of text nor-\nmalization: sentence boundary disambiguation, disambiguation of capitalized words\nwhen they are used in positions where capitalization is expected, and identiﬁcation of\nabbreviations. The major distinctive features of our approach can be summarized as\nfollows:\n', 4, 0)
(122.74270629882812, 423.38720703125, 441.068603515625, 459.3521728515625, '•\nWe tackle the sentence boundary task only after we have fully\ndisambiguated the word on the left and the word on the right of a\npotential sentence boundary punctuation sign.\n', 5, 0)
(122.74270629882812, 465.0089416503906, 446.5401611328125, 500.9739074707031, '•\nTo disambiguate capitalized words and abbreviations, we use\ninformation distributed across the entire document rather than their\nimmediate local context.\n', 6, 0)
(122.74270629882812, 506.6306457519531, 456.64715576171875, 542.5956420898438, '•\nOur approach does not require manual rule construction or data\nannotation for training. Instead, it relies on four word lists that can be\ngenerated completely automatically from a raw (unlabeled) corpus.\n', 7, 0)
(104.81002807617188, 555.7791137695312, 487.37298583984375, 699.3402709960938, 'In this approach we do not try to resolve each ambiguous word occurrence individu-\nally. Instead, the system scans the entire document for the contexts in which the words\nin question are used unambiguously, and this gives it grounds, acting by analogy, for\nresolving ambiguous contexts.\nWe deliberately shaped our approach so that it largely does not rely on precom-\npiled statistics, because the most interesting events are inherently infrequent and hence\nare difﬁcult to collect reliable statistics for. At the same time precompiled statistics\nwould be smoothed across multiple documents rather than targeted to a speciﬁc docu-\nment. By collecting suggestive instances of usage for target words from each particular\ndocument on the ﬂy, rather than relying on preacquired resources smoothed across the\nentire document collection, our approach is robust to domain shifts and new lexica\nand closely targeted to each document.\n', 8, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '314\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(104.81002044677734, 87.5367660522461, 487.3753662109375, 649.5281372070312, 'A signiﬁcant advantage of this approach is that it can be targeted to new domains\ncompletely automatically, without human intervention. The four word lists that our\nsystem uses for its operation can be generated automatically from a raw corpus and\nrequire no human annotation. Although some SBD systems can be trained on relatively\nsmall sets of labeled examples, their performance in such cases is somewhat lower than\ntheir optimal performance. For instance, Palmer and Hearst (1997) report that the SATZ\nsystem (decision tree variant) was trained on a set of about 800 labeled periods, which\ncorresponds to a corpus of about 16,000 words. This is a relatively small training set\nthat can be manually marked in a few hours’ time. But the error rate (1.5%) of the\ndecision tree classiﬁer trained on this small sample was about 50% higher than that\nwhen trained on 6,000 labeled examples (1.0%).\nThe performance of our system does not depend on the availability of labeled\ntraining examples. For its “training,” it uses a raw (unannotated in any way) corpus\nof texts. Although it needs such a corpus to be relatively large (a few hundred thousand\nwords), this is normally not a problem, since when the system is targeted to a new\ndomain, such a corpus is usually already available at no extra cost. Therefore there is no\ntrade-off between the amount of human labor and the performance of the system. This\nnot only makes retargeting of such system easier but also enables it to be operational\nin a completely autonomous way: it needs only to be pointed to texts from a new\ndomain, and then it can retarget itself automatically.\nAlthough the DCA requires two passes through a document, the simplicity of the\nunderlying algorithms makes it reasonably fast. It processes about 3,000 words per\nsecond using a Pentium II 400 MHz processor. This includes identiﬁcation of abbre-\nviations, disambiguation of capitalized words, and then disambiguation of sentence\nboundaries. This is comparable to the speed of other preprocessing systems.3 The oper-\national speed is about 10% higher than the training speed because, apart from applying\nthe system to the training corpus, training also involves collecting, thresholding, and\nsorting of the word lists—all done automatically but at extra time cost. Training on\nthe 300,000-word NYT text collection took about two minutes.\nDespite its simplicity, the performance of our approach was on the level with\nthe previously highest reported results on the same test collections. The error rate\non sentence boundaries in the Brown corpus was not signiﬁcantly worse than the\nlowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpus\nour system performed slightly better than the combination of the Alembic and SATZ\nsystems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Although\nthese error rates seem to be very small, they are quite signiﬁcant. Unlike general POS\ntagging, in which it is unfair to expect an error rate of less than 2% because even human\nannotators have a disagreement rate of about 3%, sentence boundaries are much less\nambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate\nof 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand,\none error in 200 periods means that there is one error in every two documents in the\nBrown corpus and one error in every four documents in the WSJ corpus.\nWith all its strong points, there are a number of restrictions to the proposed ap-\nproach. First, in its present form it is suitable only for processing of reasonably “well-\nbehaved” texts that consistently use capitalization (mixed case) and do not contain\nmuch noisy data. Thus, for instance, we do not expect our system to perform well\non single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on\n', 2, 0)
(109.32599639892578, 671.2020263671875, 487.3703918457031, 698.778564453125, '3 Palmer and Hearst (1997) report a speed of over 10,000 sentences a minute, which with their average\nsentence length of 20 words equals over 3,000 words per second, but on a slower machine (DEC Alpha\n3000).\n', 3, 0)

page suivante
(472.431396484375, 722.0972290039062, 487.3752746582031, 734.1519775390625, '315\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.3785095214844, 67.42921447753906, 'Mikheev\nPeriods, Capitalized Words, etc.\n', 1, 0)
(104.80999755859375, 87.5367660522461, 487.37384033203125, 422.3799133300781, 'optical character reader–generated texts. We noted in Section 8 that very short doc-\numents of one to three sentences also present a difﬁculty for our approach. This is\nwhere robust syntactic systems like SATZ (Palmer and Hearst 1997) or the POS tagger\nreported in Mikheev (2000), which do not heavily rely on word capitalization and are\nnot sensitive to document length, have an advantage.\nOur DCA uses information derived from the entire document and thus can be\nused as a complement to approaches based on the local context. When we incorpo-\nrated the DCA system into a POS tagger (Section 8), we measured a 30–35% cut in the\nerror rate on proper-name identiﬁcation in comparison to DCA or the POS-tagging\napproaches alone. This in turn enabled better tagging of sentence boundaries: a 0.20%\nerror rate on the Brown corpus and a 0.31% error rate on the WSJ corpus, which corre-\nsponds to about a 20% cut in the error rate in comparison to DCA or the POS-tagging\napproaches alone.\nWe also investigated the portability of our approach to other languages and ob-\ntained encouraging results on a corpus of news in Russian. This strongly suggests that\nthe DCA method can be applied to the majority of European languages, since they\nshare the same principles of capitalization and word abbreviation. Obvious exceptions,\nthough, are German and some Scandinavian languages in which capitalization is used\nfor things other than proper-name and sentence start signaling. This does not mean,\nhowever, that the DCA in general is not suitable for preprocessing of German texts—it\njust needs to be applied with different disambiguation clues.\nInitially the system described in this article was developed as a text normalization\nmodule for a named-entity recognition system (Mikheev, Grover, and Moens 1998) that\nparticipated in MUC-7. There the ability to identify proper names with high accuracy\nproved to be instrumental in enabling the entire system to achieve a very high level of\nperformance. Since then this text normalization module has been used in several other\nsystems, and its ability to be adapted easily to new domains enabled rapid develop-\nment of text analysis capabilities in medical, legal, and law enforcement domains.\n', 2, 0)
(104.80999755859375, 442.0062255859375, 230.70932006835938, 453.8617248535156, 'Appendix A: SBD Rule Set\n', 3, 0)
(104.80999755859375, 465.8865966796875, 487.3739013671875, 489.8964538574219, 'In this section we present the rule set used by our system to assign potential sentence\nboundary punctuation as\n', 4, 0)
(122.74267578125, 499.16168212890625, 432.96002197265625, 535.1266479492188, 'FS\nPunctuation that signals end of sentence\nAP\nPeriod that is part of abbreviation\nAFS\nPeriod that is part of abbreviation and signals end of sentence\n', 5, 0)
(104.80999755859375, 546.812744140625, 487.3739318847656, 582.7777709960938, 'This rule set operates over tokens that are disambiguated as to whether or not they\nare abbreviations and whether or not they are proper names. Tokens are categorized\ninto overlapping sets as follows:\n', 6, 0)
(122.74266052246094, 592.04296875, 455.6199951171875, 699.7410278320312, 'NONE\nNo token (end of input)\nANY\nAny token\nANY-OR-NONE\nAny token or no token at all\nABBR\nToken that was disambiguated as “abbreviation”\n(Note: . . . Ellipsis is treated as an abbreviation too)\nNot ABBR\nNonpunctuation token that was disambiguated as “not\nabbreviation”\nCLOSE PUNCT\nClosing quotes, closing brackets\nOPEN PUNCT\nOpening quotes, opening brackets\n', 7, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '316\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(122.7426986694336, 85.9427719116211, 490.2660217285156, 169.72853088378906, 'PUNCT\nPunctuation token not CLOSE PUNCT or OPEN PUNCT\nor [.!?;]\nNUM\nNumber\nLOW COMMON\nLower-cased common word\nCAP COMMON\nCapitalized word that was disambiguated as a common word\nCAP PROP\nCapitalized word that was disambiguated as a proper name\nPROPER NAME\nProper name\n', 2, 0)
(104.8101806640625, 181.40472412109375, 144.095703125, 193.26022338867188, 'Rule Set\n', 3, 0)
(104.80999755859375, 202.84841918945312, 429.479248046875, 212.4922332763672, 'word-2 word-1\nFOCAL word+1\nword+2\nAssign Example\n', 4, 0)
(104.80999755859375, 218.19082641601562, 432.70233154296875, 398.1958312988281, 'ANY\nNot ABBR\n[.?!]\nANY-OR-NONE\nANY-OR-NONE\nFS\nbook.\nANY\nCLOSE PUNCT [.?!]\nANY-OR-NONE\nANY-OR-NONE\nFS\n).\nABBR\n.\n[.?!]\nANY-OR-NONE\nANY-OR-NONE\nFS\nTex.!\nANY\nANY\n;\nCAP COMMON\nANY-OR-NONE\nFS\n; The\nABBR\n.\nNONE\nNONE\nAFS\nTex.EOF\nABBR\n.\nCAP COMMON\nANY-OR-NONE\nAFS\nTex. The\nABBR\n.\nCLOSE PUNCT\nCAP COMMON\nAFS\nkg.) This\nABBR\n.\nOPEN PUNCT\nCAP COMMON\nAFS\nkg. (This\nABBR\n.\nCLOSE PUNCT\nCAP COMMON\nAFS\nkg.) (This\nOPEN PUNCT\nABBR\n.\nPUNCT\nANY-OR-NONE\nAP\nkg.,\nABBR\n.\n[.?!]\nANY-OR-NONE\nAP\nTex.!\nABBR\n.\nLOW COMMON ANY-OR-NONE\nAP\nkg. this\nABBR\n.\nCLOSE PUNCT\nLOW COMMON AP\nkg.) this\nABBR\n.\nOPEN PUNCT\nLOW COMMON AP\nkg. (this\nABBR\n.\nCLOSE PUNCT\nLOW COMMON AP\nkg.) (this\nOPEN PUNCT\nABBR\n.\nABBR\n.\nAP\nSen. Gen.\nABBR\n.\nNUM\nANY-OR-NONE\nAP\nkg. 5\nABBR\n.\nPROPER NAME\nANY-OR-NONE\nAP\nDr. Smith\n', 5, 0)
(104.80999755859375, 414.2556457519531, 284.14068603515625, 526.6959228515625, 'Acknowledgments\nThe work reported in this article was\nsupported in part by grant GR/L21952 (Text\nTokenization Tool) from the Engineering\nand Physical Sciences Research Council,\nU.K., and also it beneﬁted from the ongoing\nefforts in building domain-independent\ntext-processing software at Infogistics Ltd. I\nam also grateful to one anonymous\nreviewer who put a lot of effort into making\nthis article as it is now.\n', 6, 0)
(104.80999755859375, 539.7842407226562, 283.6726379394531, 692.01220703125, 'References\nAberdeen, John S., John D. Burger, David S.\nDay, Lynette Hirschman, Patricia\nRobinson, and Marc Vilain. 1995. “Mitre:\nDescription of the alembic system used\nfor MUC-6.” In Proceedings of the Sixth\nMessage Understanding Conference (MUC-6),\nColumbia, Maryland, November. Morgan\nKaufmann.\nBaldwin, Breck, Christine Doran, Jeffrey\nReynar, Michael Niv, Bangalore Srinivas,\nand Mark Wasson. 1997. “EAGLE: An\nextensible architecture for general\nlinguistic engineering.” In Proceedings of\nComputer-Assisted Information Searching on\n', 7, 0)
(308.0487060546875, 414.2259826660156, 487.3031311035156, 684.1019897460938, 'Internet (RIAO ’97), Montreal, June.\nBaum, Leonard E. and Ted Petrie. 1966.\nStatistical inference for probabilistic\nfunctions of ﬁnite Markov chains. Annals\nof Mathematical Statistics 37:1559–1563.\nBikel, Daniel, Scott Miller, Richard\nSchwartz, and Ralph Weischedel. 1997.\n“Nymble: A high performance learning\nname-ﬁnder.” In Proceedings of the Fifth\nConference on Applied Natural Language\nProcessing (ANLP’97), pages 194–200.\nWashington, D.C., Morgan Kaufmann.\nBrill, Eric. 1995a. Transformation-based\nerror-driven learning and natural\nlanguage parsing: A case study in\npart-of-speech tagging. Computational\nLinguistics 21(4):543–565.\nBrill, Eric. 1995b. “Unsupervised learning of\ndisambiguation rules for part of speech\ntagging.” In David Yarovsky and Kenneth\nChurch, editors, Proceedings of the Third\nWorkshop on Very Large Corpora, pages\n1–13, Somerset, New Jersey. Association\nfor Computational Linguistics.\nBurnage, Gavin. 1990. CELEX: A Guide for\nUsers. Centre for Lexical Information,\nNijmegen, Netherlands.\n', 8, 0)

page suivante
(472.431396484375, 722.0972290039062, 487.3752746582031, 734.1519775390625, '317\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.3785095214844, 67.42921447753906, 'Mikheev\nPeriods, Capitalized Words, etc.\n', 1, 0)
(104.80998229980469, 88.45987701416016, 283.6726379394531, 697.0632934570312, 'Chinchor, Nancy. 1998. “Overview of\nMUC-7.” In Seventh Message Understanding\nConference (MUC-7): Proceedings of a\nConference Held in Fairfax, April. Morgan\nKaufmann.\nChurch, Kenneth. 1988. “A stochastic parts\nprogram and noun-phrase parser for\nunrestricted text.” In Proceedings of the\nSecond ACL Conference on Applied Natural\nLanguage Processing (ANLP’88), pages\n136–143, Austin, Texas.\nChurch, Kenneth. 1995. “One term or two?”\nIn SIGIR’95, Proceedings of the 18th Annual\nInternational ACM SIGIR Conference on\nResearch and Development in Information\nRetrieval, pages 310–318, Seattle,\nWashington, July. ACM Press.\nClarkson, Philip and Anthony J. Robinson.\n1997. “Language model adaptation using\nmixtures and an exponentially decaying\ncache.” In Proceedings IEEE International\nConference on Speech and Signal Processing,\nMunich, Germany.\nCucerzan, Silviu and David Yarowsky. 1999.\n“Language independent named entity\nrecognition combining morphological and\ncontextual evidence.” In Proceedings of\nJoint SIGDAT Conference on EMNLP and\nVLC.\nFrancis, W. Nelson and Henry Kucera. 1982.\nFrequency Analysis of English Usage: Lexicon\nand Grammar. Houghton Mifﬂin, New\nYork.\nGale, William, Kenneth Church, and David\nYarowsky. 1992. “One sense per\ndiscourse.” In Proceedings of the Fourth\nDARPA Speech and Natural Language\nWorkshop, pages 233–237.\nGrefenstette, Gregory and Pasi Tapanainen.\n1994. “What is a word, what is a\nsentence? Problems of tokenization.” In\nThe Proceedings of Third Conference on\nComputational Lexicography and Text\nResearch (COMPLEX’94), Budapest,\nHungary.\nKrupka, George R. and Kevin Hausman.\n1998. Isoquest Inc.: Description of the\nnetowl extractor system as used for\nMUC-7. In Proceedings of the Seventh\nMessage Understanding Conference (MUC-7),\nFairfax, VA. Morgan Kaufmann.\nKuhn, Roland and Renato de Mori. 1998. A\ncache-based natural language model for\nspeech recognition. IEEE Transactions on\nPattern Analysis and Machine Intelligence\n12:570–583.\nKupiec, Julian. 1992. Robust part-of-speech\ntagging using a hidden Markov model.\nComputer Speech and Language.\nMani, Inderjeet and T. Richard MacMillan.\n1995. “Identifying unknown proper\n', 2, 0)
(308.0487060546875, 88.45365142822266, 487.3794860839844, 697.0570678710938, 'names in newswire text.” In B. Boguraev\nand J. Pustejovsky, editors, Corpus\nProcessing for Lexical Acquisition. MIT Press,\nCambridge, Massachusetts, pages 41–59.\nMarcus, Mitchell, Mary Ann Marcinkiewicz,\nand Beatrice Santorini. 1993. Building a\nlarge annotated corpus of English: The\nPenn treebank. Computational Linguistics\n19(2):313–329.\nMikheev, Andrei. 1997. Automatic rule\ninduction for unknown word guessing.\nComputational Linguistics 23(3):405–423.\nMikheev, Andrei. 1999. A knowledge-free\nmethod for capitalized word\ndisambiguation. In Proceedings of the 37th\nConference of the Association for\nComputational Linguistics (ACL’99), pages\n159–168, University of Maryland, College\nPark.\nMikheev, Andrei. 2000. “Tagging sentence\nboundaries.” In Proceedings of the First\nMeeting of the North American Chapter of the\nComputational Linguistics (NAACL’2000),\npages 264–271, Seattle, Washington.\nMorgan Kaufmann.\nMikheev, Andrei, Clair Grover, and Colin\nMatheson. 1998. TTT: Text Tokenisation Tool.\nLanguage Technology Group, University\nof Edinburgh. Available at\nhttp://www.ltg.ed.ac.uk/software/ttt/\nindex.html.\nMikheev, Andrei, Clair Grover, and Marc\nMoens. 1998. Description of the ltg\nsystem used for MUC-7. In Seventh\nMessage Understanding Conference\n(MUC–7): Proceedings of a Conference Held in\nFairfax, Virginia. Morgan Kaufmann.\nMikheev, Andrei and Liubov Liubushkina.\n1995. Russian morphology: An\nengineering approach. Natural Language\nEngineering 1(3):235–260.\nPalmer, David D. and Marti A. Hearst. 1994.\n“Adaptive sentence boundary\ndisambiguation.” In Proceedings of the\nFourth ACL Conference on Applied Natural\nLanguage Processing (ANLP’94), pages\n78–83, Stuttgart, Germany, October.\nMorgan Kaufmann.\nPalmer, David D. and Marti A. Hearst. 1997.\nAdaptive multilingual sentence boundary\ndisambiguation. Computational Linguistics\n23(2):241–269.\nPark, Youngja and Roy J. Byrd. 2001.\n“Hybrid text mining for ﬁnding\nabbreviations and their deﬁnitions.” In\nProceedings of the Conference on Empirical\nMethods in Natural Language Processing\n(EMLP’01), pages 16–19, Washington,\nD.C. Morgan Kaufmann.\nRatnaparkhi, Adwait. 1996. “A maximum\nentropy model for part-of-speech\n', 3, 0)

page suivante
(104.80999755859375, 722.0972290039062, 119.75389862060547, 734.1519775390625, '318\n', 0, 0)
(104.80999755859375, 56.579872131347656, 487.37945556640625, 67.42921447753906, 'Computational Linguistics\nVolume 28, Number 3\n', 1, 0)
(104.80998992919922, 88.45987701416016, 280.88763427734375, 218.86024475097656, 'tagging.” In Proceedings of Conference on\nEmpirical Methods in Natural Language\nProcessing, pages 133–142, University of\nPennsylvania, Philadelphia.\nReynar, Jeffrey C. and Adwait Ratnaparkhi.\n1997. “A maximum entropy approach to\nidentifying sentence boundaries.” In\nProceedings of the Fifth ACL Conference on\nApplied Natural Language Processing\n(ANLP’97), pages 16–19. Morgan\nKaufmann.\nRiley, Michael D. 1989. “Some applications\nof tree-based modeling to speech and\n', 2, 0)
(308.0487060546875, 88.4592056274414, 479.6575622558594, 218.8595733642578, 'language indexing.” In Proceedings of the\nDARPA Speech and Natural Language\nWorkshop, pages 339–352. Morgan\nKaufmann.\nYarowsky, David. 1993. “One sense per\ncollocation.” In Proceedings of ARPA\nHuman Language Technology Workshop ’93,\npages 266–271, Princeton, New Jersey.\nYarowsky, David. 1995. “Unsupervised\nword sense disambiguation rivaling\nsupervised methods.” In Meeting of the\nAssociation for Computational Linguistics\n(ACL’95), pages 189–196.\n', 3, 0)

page suivante
