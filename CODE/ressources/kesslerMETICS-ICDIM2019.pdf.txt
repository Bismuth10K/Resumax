(49.25199890136719, 49.138877868652344, 562.7494506835938, 105.84580993652344, 'A word embedding approach to explore a collection\nof discussions of people in psychological distress\n', 0, 0)
(52.38999557495117, 144.3200225830078, 155.35806274414062, 210.66746520996094, '1st R´emy Kessler\nUniversit´e Bretagne Sud\nCNRS 6074A\n56017 Vannes,France\nremy.kessler@univ-ubs.fr\n', 1, 0)
(158.947998046875, 144.31996154785156, 258.3247375488281, 210.6674041748047, '2nd Nicolas B´echet\nUniversit´e Bretagne Sud\nCNRS 6074A\n56017 Vannes,France\nnicolas.bechet@irisa.fr\n', 2, 0)
(265.5059814453125, 144.3199005126953, 397.1217956542969, 210.66734313964844, '3rd Gudrun Ledegen\nUniversit´e Rennes II\nPREFics, EA 4246\n5043 Rennes, France\ngudrun.ledegen@univ-rennes2.fr\n', 3, 0)
(400.5479736328125, 144.31983947753906, 559.6109008789062, 210.6672821044922, '4rd Frederic Pugni`ere-Saavedra\nUniversit´e Bretagne Sud\nPREFics, EA 4246\n56017 Vannes, France\nfrederic.pugniere-saavedra@univ-ubs.fr\n', 4, 0)
(48.96397399902344, 253.50421142578125, 300.02362060546875, 374.9884948730469, 'Abstract—In order to better adapt to society, an association\nhas developed a web chat application that allows anyone to\nexpress and share their concerns and anguishes. Several thousand\nanonymous conversations have been gathered and form a new\ncorpus of stories about human distress and social violence. We\npresent a method of corpus analysis combining unsupervised\nlearning and word embedding in order to bring out the themes\nof this particular collection. We compare this approach with a\nstandard algorithm of the literature on a labeled corpus and\nobtain very good results. An interpretation of the obtained\nclusters collection conﬁrms the interest of the method.\nKeywords—word2vec, unsupervised learning, word embedding.\n', 5, 0)
(135.6189727783203, 395.7864990234375, 213.37176513671875, 407.79144287109375, 'I. INTRODUCTION\n', 6, 0)
(48.9639778137207, 412.0434875488281, 300.0235595703125, 699.017333984375, 'Since the nineties, social suffering has been a theme that has\nreceived much attention from public and associative action.\nAmong the consequences, there is an explosion of listening\nplaces or socio-technical devices of communication whose\nobjectives consist in moderating the various forms of suffering\nby the liberation of the speech for a therapeutic purpose [1]\n[2]. As part of the METICS project, a suicide prevention\nassociation developed an application of web chat to meet\nthis need. The web chat is an area that allows anyone to\nexpress and share with a volunteer listener their concerns and\nanguishes. The main speciﬁcity of this device is its anonymous\nnature. Protected by a pseudonym, the writers are invited\nto discuss with a volunteer the problematic aspects of their\nexistence. Several thousand anonymous conversations have\nbeen gathered and form a corpus of unpublished stories about\nhuman distress. The purpose of the METICS project is to make\nvisible the ordinary forms of suffering usually removed from\ncommon spaces and to grasp both its modes of enunciation and\ndigital support. In this study, we want to automatically identify\nthe reason for coming on the web chat for each participant.\nIndeed, even if the association provided us with the theme\nof all the conversations (work, loneliness, violence, racism,\naddictions, family, etc.), the original reason has not been\npreserved. In what follows, we ﬁrst review some of the related\n', 7, 0)
(311.9779968261719, 252.9423828125, 563.0355834960938, 336.6782531738281, 'work in Section II. Section III presents the resources used and\ngives some statistics about the collection. An overview of the\nsystem and the strategy for identify the reason for coming\non the web chat is given in Section IV. Section V presents\nthe experimental protocol, an evaluation of our system and an\ninterpretation of the ﬁnal results on the collection of human\ndistress.\n', 8, 0)
(393.03399658203125, 351.05731201171875, 481.98663330078125, 363.062255859375, 'II. RELATED WORKS\n', 9, 0)
(311.9779968261719, 371.58831787109375, 563.03564453125, 719.244140625, 'The main characteristic of the approach presented in this\npaper is to only have to provide the labels of the classes to\nbe predicted. This method does not need to have a tagged\ndata set to predict the different classes, so it is closer to an\nunsupervised (clustering) or semi-supervised learning method\nthan a supervised. The main idea of clustering is to group\nuntagged data into a number of clusters, such that similar ex-\namples are grouped together and different ones are separated.\nIn clustering, the number of classes and the distribution of\ninstances between classes are unknown and the goal is to ﬁnd\nmeaningful clusters.\nOne kind of clustering methods is the partitioning-based\none. The k-means algorithm [3] is one of the most popu-\nlar partitioning-based algorithms because it provides a good\ncompromise between the quality of the solution obtained and\nits computational complexity [4]. K-means aims to ﬁnd k\ncentroids, one for each cluster, minimizing the sum of the\ndistances of each instance of data from its respective centroid.\nWe can cite other partitioning-based algorithms such as k-\nmedoids or PAM (Partition Around Medoids), which is an\nevolution of k-means [5]. Hierarchical approaches produce\nclusters by recursively partitioning data backwards or upwards.\nFor example, in a hierarchical ascending classiﬁcation or\nCAH [6], each example from the initial dataset represents a\ncluster. Then, the clusters are merged, according to a similarity\nmeasure, until the desired tree structure is obtained. The result\nof this clustering method is called a dendrogram. Density-\nbased methods like the EM algorithm [7] assume that the data\nbelonging to each cluster is derived from a speciﬁc probability\n', 10, 0)

page suivante
(48.9640007019043, 49.70452880859375, 300.02191162109375, 300.7236328125, 'distribution [8]. The idea is to grow a cluster as the density in\nthe neighborhood of the cluster exceeds a predeﬁned threshold.\nModel-based classiﬁcation methods like self-organizing\nmap - SOM [9] are focus on ﬁnding features to represent each\ncluster. The most used methods are decision trees and neural\nnetworks. Approaches based on semi-supervised learning such\nas label propagation algorithm [10] are similar to the method\nproposed in this paper because they consist in using a learning\ndataset consisting of a few labelled data points to build a\nmodel for labelling a larger number of unlabelled data. Closer\nto the theme of our collection, [11] and [12] use supervised\napproaches to automatically detect suicidal people in social\nnetworks. They extract speciﬁc features like word distribution\nstatistics or sentiments to train different machine-learning clas-\nsiﬁers and compare performance of machine-learning models\nagainst the judgments of psychiatric trainees and mental health\nprofessionals. More recently, CLEF challenge in 2018 consists\nof performing a task on early risk detection of depression on\ntexts written in Social Media1. However, these papers and this\ntask involve tagged data sets, which is the main difference\nwith our proposed approach (we do not have tagged data set).\n', 0, 0)
(103.4800033569336, 306.85968017578125, 245.51177978515625, 318.8646240234375, 'III. RESOURCES AND STATISTICS\n', 1, 0)
(48.96399688720703, 321.8626708984375, 300.02325439453125, 548.9703979492188, 'The association provided a collection of conversations be-\ntween volunteers and callers between 2005 and 2015, which\nis called “METICS collection” henceforth.\nTo reduce noise in the collection, we removed all the\ndiscussions containing fewer than 15 exchanges between a\ncaller and a person from the association, these exchanges are\ngenerally unrepresentative (connection problem, request for\ninformation, etc.). We observe particular linguistic phenomena\nlike emoticons2, acronyms, mistakes (spelling, typography,\nglued words) and an explosive lexical creativity [13]. These\nphenomena have their origin in the mode of communication\n(direct or semi-direct), the speed of the composition of the\nmessage or in the technological constraints of input imposed\nby the material (mobile terminal, tablet, etc.). In addition, we\nused a subset of the collection of the French newspaper, Le\nMonde to validate our method on a tagged corpus. We only\nkeep articles on television, politics, art, science or economics.\nFigure 1 presents some descriptive statistics of these two\ncollections.\n', 2, 0)
(131.06698608398438, 555.1064453125, 217.9264373779297, 567.1113891601562, 'IV. METHODOLOGY\n', 3, 0)
(48.9639892578125, 570.1104736328125, 131.703369140625, 582.0057983398438, 'A. System Overview\n', 4, 0)
(48.9639892578125, 585.1134643554688, 300.0225524902344, 680.804443359375, 'Figure 2 presents an overview of the system, each step will\nbe detailed in the rest of the section. In the ﬁrst step (mod-\nule x), we apply different linguistic pre-processing to each\ndiscussion. The next module (y) creates a word embedding\nmodel with these discussions while the third module (z) uses\nthis model to create speciﬁc vectors. The last module ({)\nperforms a prediction for each discussion before separating\nthe collection into clusters based on the predicted class.\n', 5, 0)
(48.964019775390625, 689.2796020507812, 300.026123046875, 718.6845703125, '1http://early.irlab.org/\n2Symbols used in written messages to express emotions, e.g. smile or\nsadness\n', 6, 0)
(324.3620300292969, 49.23097610473633, 550.6560668945312, 60.03548812866211, 'Collection\nMETICS\nLe-Monde\n', 7, 0)
(324.36199951171875, 59.59206008911133, 550.6561279296875, 70.39657592773438, 'Total number of documents\n17 594\n205 661\n', 8, 0)
(368.4110107421875, 69.95301818847656, 452.50677490234375, 80.65890502929688, 'Without pre-processing\n', 9, 0)
(324.36199951171875, 80.31404113769531, 550.65625, 111.04354858398438, 'Total number of words\n12 276 973\n87 122 002\nTotal number of different words\n158 361\n419 579\nAverage words/document\n698\n424\n', 10, 0)
(374.1400146484375, 110.60005187988281, 446.7767333984375, 121.30593872070312, 'With pre-processing\n', 11, 0)
(324.36199951171875, 120.96205139160156, 550.65625, 151.69155883789062, 'Total number of words\n4 529 793\n41 425 938\nTotal number of different words\n120 684\n419 006\nAverage words/document\n257\n201\n', 12, 0)
(376.7749938964844, 172.14959716796875, 498.2393493652344, 181.75357055664062, 'Fig. 1. Statistics of both collections.\n', 13, 0)
(335.459716796875, 335.5269470214844, 438.19854736328125, 341.161376953125, ' \n \n', 14, 0)
(329.79034423828125, 283.08221435546875, 366.9173278808594, 296.0472412109375, 'Conversations\nor documents\n', 15, 0)
(395.78741455078125, 207.42201232910156, 418.5760498046875, 213.1061248779297, 'Word2vec\n', 16, 0)
(400.25750732421875, 213.12771606445312, 414.1228332519531, 218.81182861328125, 'model\n', 17, 0)
(451.3543395996094, 268.3312072753906, 474.03179931640625, 274.01531982421875, 'Prediction\n', 18, 0)
(471.3787841796875, 235.66357421875, 485.92352294921875, 240.53567504882812, 'Class 1\n', 19, 0)
(374.7090759277344, 268.1858215332031, 408.44757080078125, 273.86993408203125, 'pre-processing\n', 20, 0)
(452.698974609375, 205.16880798339844, 472.141845703125, 222.3006591796875, 'Specific \nvectors \ncreation\n', 21, 0)
(470.4702453613281, 248.78305053710938, 485.9962158203125, 253.6551513671875, 'Model 1\n', 22, 0)
(406.58099365234375, 276.5691833496094, 412.4134521484375, 284.61517333984375, '①\n', 23, 0)
(423.6980895996094, 220.4571533203125, 480.4456787109375, 230.28390502929688, '➁\n➂\n', 24, 0)
(474.75860595703125, 276.5320739746094, 480.6230163574219, 284.1468505859375, '④\n', 25, 0)
(491.51226806640625, 235.66357421875, 506.06134033203125, 240.53567504882812, 'Class 2\n', 26, 0)
(489.767822265625, 248.60133361816406, 505.2894287109375, 253.4734344482422, 'Model 2\n', 27, 0)
(527.3818359375, 234.64601135253906, 541.930908203125, 239.5181121826172, 'Class n\n', 28, 0)
(528.3267211914062, 248.60133361816406, 543.8526611328125, 253.4734344482422, 'Model n\n', 29, 0)
(445.6486511230469, 330.0074157714844, 528.2997436523438, 335.31561279296875, 'Cluster 1\nCluster 2\nCluster n\n', 30, 0)
(396.2820129394531, 359.7876281738281, 478.73272705078125, 369.3916015625, 'Fig. 2. System overview\n', 31, 0)
(311.97802734375, 395.6915588378906, 463.9773254394531, 407.5869140625, 'B. Normalization and pre-processing\n', 32, 0)
(311.97802734375, 410.7435607910156, 563.0355224609375, 530.3444213867188, 'We ﬁrst extract the textual content of each discussion. In\nstep x, a text normalization is performed to improve the\nquality of the process. We remove accents, special characters\nsuch as “-”,“/” or “()”. Different linguistic processes are used\nto reduce noise in the model: we remove numbers (numeric\nand/or textual), special symbols and terms contained in a stop-\nlist adapted to our problem. A lemmatization process was\nincorporated during the ﬁrst experiments but it was inefﬁcient\nconsidering the typographical variations described in Section\nIII.\n', 33, 0)
(311.97802734375, 536.7704467773438, 392.2964782714844, 548.665771484375, 'C. word2vec model\n', 34, 0)
(311.97802734375, 551.8214111328125, 563.0399169921875, 719.244384765625, 'In the next step we build a word embedding model using\nword2vec [14]. We project each word of our corpus in a vector\nspace in order to obtain a semantic representation of these.\nIn this way, words appearing in similar contexts will have a\nrelatively close vector representation. In addition to semantic\ninformation, one advantage of such modeling is the production\nof vector representations of words, depending on the context\nin which they are encountered. Some words close to a term t in\na model learned from a corpus c1 may be very different from\nthose from a model learned from a corpus c2. For example,\nwe observe in ﬁgure 3 that the ﬁrst eight words close to the\nterm “teen” vary according to the corpus used. This example\nalso shows that the use of a generic model like Le Monde in\nFrench or Wikipedia is irrelevant in our case, since the corpus\n', 35, 0)

page suivante
(81.16400146484375, 50.10357666015625, 159.4199676513672, 62.10851287841797, 'corpus\nwords\n', 0, 0)
(81.16400146484375, 62.45758056640625, 263.6785583496094, 86.41747283935547, 'METICS\nteenager, young, 15years, kid,\nschool, problem , spoiled, teen,\n', 1, 0)
(81.16400146484375, 86.76556396484375, 264.33612060546875, 110.72649383544922, 'Le-Monde\nsitcom, radio, compote, hearing\nboy, styx, scamp, rebel\n', 2, 0)
(48.9640007019043, 118.79560089111328, 300.0220947265625, 137.36557006835938, 'Fig. 3. Eight words closest to the term “teenager” according to the type of\ncorpus in learning.\n', 3, 0)
(48.9640007019043, 157.966552734375, 300.02154541015625, 193.8815155029297, 'of the association is noisy and contains a number of apocopes,\nabbreviations or acronyms. Different parameters were tested\nand the conﬁguration with the best results was kept3.\n', 4, 0)
(48.96397399902344, 200.46356201171875, 260.0814208984375, 212.35890197753906, 'D. Speciﬁc vectors creation and cluster predictions\n', 5, 0)
(48.963958740234375, 215.54156494140625, 300.0218811035156, 442.7393798828125, 'In this step, we build vectors containing terms that are\nselected using the word2vec model described in step IV-C.\nFor each theme in the collection, we build a speciﬁc linguistic\nmodel by performing a word embedding to reconstruct the\nlinguistic context of each theme. We observe, for example,\nthat the terms closest to the thematic “work” are: “unemploy-\nment”, “job”, “stress”. Similarly, for the “addiction” theme,\nwe observe the terms: “cannabis”, “alcoholism”, “drugs” and\n“heroin”. We used this context subsequently to construct a\nvector, containing the distance distc(i) between each term i\nand the theme c. Each of these models is independent, so\nthe same term can appear in several models. In this way,\nwe observed that the word “stress” is present in the vector\n“suicide” and in that of “work”, however, the associated weight\nis different. We varied the size of these vectors between 20\nand 1000 and the best results were obtained with a size of 400.\nIn the last step {, the system computes an Sc score for each\ndiscussion and for each cluster according to each linguistic\nmodel such as:\n', 6, 0)
(118.25096130371094, 455.69091796875, 151.8639678955078, 466.4002990722656, 'Sc(d) =\n', 7, 0)
(154.63597106933594, 445.478515625, 169.02197265625, 483.1871643066406, 'n\nX\n', 8, 0)
(155.3779754638672, 453.9574279785156, 300.0223388671875, 476.6603088378906, 'i=1\ntf(i) · distc(i)\n(1)\n', 9, 0)
(48.96397399902344, 480.8354187011719, 300.0258483886719, 528.7063598632812, 'with i the considered term, tf (i) frequency of i in the\ncollection, and distc(i) is the distance between the term i and\nthe thematic c. In the end, the class with the highest score is\nchosen.\n', 10, 0)
(106.29998779296875, 535.2883911132812, 242.69192504882812, 547.2933349609375, 'V. EXPERIMENTS AND RESULTS\n', 11, 0)
(48.9639892578125, 550.3663940429688, 153.00340270996094, 562.26171875, 'A. Experimental protocol\n', 12, 0)
(48.9639892578125, 565.4443969726562, 300.0215759277344, 673.09033203125, 'To evaluate the quality of the obtained clusters, we used a\nsubset of the texts of the Le-Monde newspaper, described in\nSection III, each article having a label according to the theme.\nFor these experiments, we conﬁgured the speciﬁc vectors (SV)\napproach with the optimal parameters, as deﬁned in Sections\nIV-C and IV-D. We also tested the speciﬁc vectors without\nweighting to test the particular inﬂuence of this parameter. To\nhighlight the difﬁculty of the task, we compare our system\nwith a baseline which consists in a random draw, and with\n', 13, 0)
(48.963985443115234, 681.2095336914062, 300.0260925292969, 718.6845092773438, '3The best results were obtained with the following parameter values: vector\nsize: 700, sliding window size: 5, minimum frequency: 10, vectorization\nmethod: skip grams, and a soft hierarchical max function for the model\nlearning.\n', 14, 0)
(311.97796630859375, 49.7054443359375, 563.036376953125, 181.2615203857422, 'the k-means algorithm [3], commonly used in the literature,\nas mentioned in Section II. To feed the k-means algorithm, we\ntransformed our initial collection into a bag of words matrix\n[15] where each conversation is described by the frequency\nof its words. Each of the experiments was evaluated using the\nclassic measures of Precision, Recall and F-measure, averaged\nover all classes (with\nbeta = 1 in order not to privilege\nprecision or recall [16]). Since the k-means algorithm does not\nassociate a tag with the ﬁnal clusters, we have exhaustively\ncalculated the set of solutions to keep only the one yielding\nthe highest F-score.\n', 15, 0)
(311.97796630859375, 194.2205810546875, 354.31903076171875, 206.1159210205078, 'B. Results\n', 16, 0)
(429.8210144042969, 223.83856201171875, 529.2577514648438, 235.84349060058594, 'Prec.\nRecall\nF-score\n', 17, 0)
(367.64898681640625, 236.1915283203125, 462.0445251464844, 248.1964569091797, 'Without pre-processing\n', 18, 0)
(342.2699890136719, 248.5455322265625, 523.0355834960938, 297.0132141113281, 'Baseline\n0.18\n0.16\n0.17\nk-means\n0.23\n0.20\n0.22\nWithout weighting\n0.54\n0.50\n0.52\nSpeciﬁc Vectors\n0.53\n0.54\n0.53\n', 19, 0)
(374.0150146484375, 296.7645568847656, 455.6783752441406, 308.7695007324219, 'With pre-processing\n', 20, 0)
(342.2699890136719, 309.1185607910156, 523.0358276367188, 345.6312255859375, 'k-means\n0.30\n0.21\n0.25\nWithout weighting\n0.55\n0.51\n0.53\nSpeciﬁc Vectors\n0.54\n0.54\n0.54\n', 21, 0)
(369.6300048828125, 353.1026306152344, 505.38470458984375, 362.70660400390625, 'Fig. 4. Results obtained by each system.\n', 22, 0)
(311.9779968261719, 376.93157958984375, 563.0357055664062, 496.53240966796875, 'Figure 4 presents a summary of the results obtained with\neach systems. We ﬁrst observe that baseline scores are very\nlow, but remain relatively close to the theoretical random (0.2)\ngiven by the number of classes. Linguistic pre-treatments are\nnot very efﬁcient individually, but improve overall the results\nof other experiments. The k-means algorithm obtains slightly\nbetter results in terms of F-score, but remains weak. Speciﬁc\nvectors get excellent results that outperform other systems with\nan F-score of 0.54. The execution without weighting improve\nslightly the recall.\n', 23, 0)
(311.9779968261719, 509.491455078125, 392.68499755859375, 521.3867797851562, 'C. Cluster Analysis\n', 24, 0)
(311.9779968261719, 527.1724853515625, 563.0355834960938, 719.244384765625, 'Initial objective of this work was the exploration of the\nMETICS collection, we apply the whole process with the\nspeciﬁc vectors approach to automatically categorize all the\nconversations. We use the Latent Dirichlet Allocation [17] in\norder to obtain the main topic of each cluster. Figure 5 presents\naverage weight of each thematic keywords according to each\nclusters.\nIn ﬁgure 5, fear, shrink and trust are present designations\nfor each cluster with a largely signiﬁcant rank; yet, does\nthe writer still express fear when he writes, ”I’m afraid of\nbeing sick”? Do these designations not participate in opening\nand constructing spheres of meanings around these pivotal\nwords? Conversely, with a lower rank, but also signiﬁcant, the\ndesignations of thing, difﬁcult, and problem are more vague,\nbut more reformulating to take up the elements involved in\nwriting what is wrong.\n', 25, 0)

page suivante
(218.81199645996094, 272.44464111328125, 393.18988037109375, 282.0486145019531, 'Fig. 5. Distribution of discursive routines by cluster.\n', 0, 0)
(48.9640007019043, 294.61859130859375, 300.0218811035156, 536.5133056640625, 'VI. CONCLUSION AND FUTURE WORK\nIn this article, we presented an unsupervised approach to\nexploring a collection of stories about human distress. This\napproach uses a word embedding model to build vectors\ncontaining only vocabulary from the linguistic context of the\nmodel. We evaluated the quality of the approach on a col-\nlection labeled with classical measures. The detailed analysis\nshowed very good results (average Fscore of 0.54) compared\nto the other systems tested. This method of analysis has also\nmade it possible to highlight semantic universes and thematic\ngroupings. We ﬁrst intend to study in more detail the inﬂuence\nof each of the parameters on the results obtained. We are also\nplanning to be able to assign several tags to each discussion,\nwhich would allow thematic overlaps to be taken into account.\nThe analysis reinforces the cluster approach to highlight the\ndeﬁning features of this type of speech production and to\nreveal its inner workings. This entry by the discursive routines\nis only one example which will then make it possible to\napproach other explorations with a particular focus on the\nargumentative forms and on the forms of intensity.\n', 1, 0)
(146.59800720214844, 540.809326171875, 202.39846801757812, 552.8142700195312, 'REFERENCES\n', 2, 0)
(52.949012756347656, 556.6514282226562, 300.0204772949219, 575.222412109375, '[1] D. Fassin, “Et la souffrance devint sociale,” in Critique.\n680(1), 2004,\npp. 16–29.\n', 3, 0)
(52.949005126953125, 574.54443359375, 300.01885986328125, 593.1544189453125, '[2] ——, “Souffrir par le social, gouverner par l’´ecoute,” in Politix.\n73(1),\n2006, pp. 137–157.\n', 4, 0)
(52.949005126953125, 592.5174560546875, 300.0259704589844, 629.0204467773438, '[3] MacQueen, J., “Some methods for classiﬁcation and analysis of multi-\nvariate observations,” in Proceedings of the Fifth Berkeley Symposium\non Mathematical Statistics and Probability, Vol. 1: Statistics.\nUSA:\nUniversity of California Press, 1967, pp. 281–297.\n', 5, 0)
(52.94898986816406, 628.3824462890625, 300.02362060546875, 655.91943359375, '[4] D. Arthur and S. Vassilvitskii, “K-means++: The advantages of careful\nseeding,” Proceedings of the Eighteenth Annual ACM-SIAM Symposium\non Discrete Algorithms, pp. 1027–1035, 2007.\n', 6, 0)
(52.94898223876953, 655.2814331054688, 300.0224914550781, 691.784423828125, '[5] L. Kaufman and P. Rousseeuw, Clustering by Means of Medoids.\nDelft\nUniversity\nof\nTechnology\n:\nreports\nof\nthe\nFaculty\nof\nTechnical Mathematics and Informatics, 1987. [Online]. Available:\nhttps://books.google.fr/books?id=HK-4GwAACAAJ\n', 7, 0)
(52.94895935058594, 691.1474609375, 300.0245361328125, 718.6844482421875, '[6] G. N. Lance and W. T. Williams, “A general theory of classiﬁcatory\nsorting strategies1. hierarchical systems,” The Computer Journal 4, pp.\n373–380, 1967.\n', 8, 0)
(315.96295166015625, 296.4594421386719, 563.0360717773438, 323.9964294433594, '[7] A. P. Dempster, N. M. Laird, and D. B. Rubin, “Maximum likelihood\nfrom incomplete data via the em algorithm,” in Journal of the royal\nsociety, series B, 1977, pp. 1–38.\n', 9, 0)
(315.96295166015625, 323.35845947265625, 563.0360107421875, 341.929443359375, '[8] J. D. Banﬁeld and A. E. Raftery, “Model-based gaussian and non-\ngaussian clustering,” in Biometrics, vol. 49, 1993, pp. 803–821.\n', 10, 0)
(315.9629211425781, 341.2914733886719, 563.0360717773438, 359.8614501953125, '[9] T. Kohonen, “Self-organized formation of topologically correct feature\nmaps,” Biological Cybernetics, pp. 59–69, Jan 1982.\n', 11, 0)
(311.9779357910156, 359.2244873046875, 563.0362548828125, 386.7604675292969, '[10] U. N. Raghavan, R. Albert, and S. Kumara, “Near linear time algorithm\nto detect community structures in large-scale networks.” Physical review.\nE, Statistical, nonlinear, and soft matter physics, p. 036106, 2007.\n', 12, 0)
(311.9779052734375, 386.1235046386719, 563.0361938476562, 422.6264953613281, '[11] J. P. Pestian, P. Matykiewicz, M. Linn-Gust, B. South, O. Uzuner,\nJ. Wiebe, K. B. Cohen, J. Hurdle, and C. Brew, “Sentiment analysis\nof suicide notes: A shared task,” Biomedical Informatics Insights, pp.\n3–16, 2012.\n', 13, 0)
(311.9779357910156, 421.9485168457031, 563.0372924804688, 476.4245300292969, '[12] A. Abboute, Y. Boudjeriou, G. Entringer, J. Az´e, S. Bringay, and\nP. Poncelet, “Mining twitter for suicide prevention,” in Natural Language\nProcessing and Information Systems: 19th International Conference on\nApplications of Natural Language to Information Systems, NLDB 2014,\nMontpellier, France, June 18-20, 2014. Proceedings.\nSpringer, 2014,\npp. 250–253.\n', 14, 0)
(311.9779357910156, 475.74755859375, 563.0383911132812, 503.32354736328125, '[13] R. Kessler, J.-M. Torres, and M. El-B`eze, “Classiﬁcation th´ematique de\ncourriel par des m´ethodes hybrides,” Journ´ee ATALA sur les nouvelles\nformes de communication ´ecrite, 2004.\n', 15, 0)
(311.9778747558594, 502.68658447265625, 563.0360717773438, 548.1555786132812, '[14] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Distributed\nrepresentations of words and phrases and their compositionality,” in\nProceedings\nof\nNIPS’13.\nUSA:\nCurran\nAssociates\nInc.,\n2013,\npp. 3111–3119. [Online]. Available: http://dl.acm.org/citation.cfm?id=\n2999792.2999959\n', 16, 0)
(311.9778747558594, 547.4785766601562, 563.0339965820312, 566.088623046875, '[15] C. D. Manning and H. Sch¨utze, Foundations of Statistical Natural\nLanguage Processing.\nCambridge, MA, USA: MIT Press, 1999.\n', 17, 0)
(311.97784423828125, 565.4505615234375, 563.0362548828125, 592.9876098632812, '[16] C. Goutte and E. Gaussier, “ A Probabilistic Interpretation of Precision,\nRecall and F-Score, with Implication for Evaluation,” ECIR 2005, pp.\n345–359, 2005.\n', 18, 0)
(311.9778137207031, 592.349609375, 563.0360717773438, 628.8535766601562, '[17] M. Hoffman, F. R. Bach, and D. M. Blei, “Online learning for latent\ndirichlet allocation,” in Advances in Neural Information Processing\nSystems, J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel,\nand A. Culotta, Eds.\n23, 2010, pp. 856–864.\n', 19, 0)

page suivante
