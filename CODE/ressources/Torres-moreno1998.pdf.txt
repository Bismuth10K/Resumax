(131.7760009765625, 146.44778442382812, 444.8221130371094, 155.9878692626953, 'LETTER\nCommunicated by Scott Fahlman\n', 0, 0)
(131.7760009765625, 176.12705993652344, 413.3373107910156, 202.5584716796875, 'Efﬁcient Adaptive Learning for Classiﬁcation Tasks with\nBinary Units\n', 1, 0)
(131.7751007080078, 221.29632568359375, 442.6029052734375, 266.8873291015625, 'J. Manuel Torres Moreno\nMirta B. Gordon\nD´epartement de Recherche Fondamentale sur la Mati`ere Condens´ee, CEA Grenoble,\n38054 Grenoble Cedex 9, France\n', 2, 0)
(131.7760009765625, 287.0503234863281, 443.2596740722656, 424.3450622558594, 'This article presents a new incremental learning algorithm for classi-\nﬁcation tasks, called NetLines, which is well adapted for both binary\nand real-valued input patterns. It generates small, compact feedforward\nneural networks with one hidden layer of binary units and binary output\nunits. A convergence theorem ensures that solutions with a ﬁnite num-\nber of hidden units exist for both binary and real-valued input patterns.\nAn implementation for problems with more than two classes, valid\nfor any binary classiﬁer, is proposed. The generalization error and\nthe size of the resulting networks are compared to the best published\nresultsonwell-knownclassiﬁcationbenchmarks.Earlystoppingisshown\nto decrease overﬁtting, without improving the generalization perfor-\nmance.\n', 3, 0)
(131.7760009765625, 454.4285888671875, 195.92027282714844, 465.69195556640625, '1 Introduction\n', 4, 0)
(131.7760009765625, 477.3079528808594, 442.6443786621094, 660.6213989257812, 'Feedforward neural networks have been successfully applied to the prob-\nlem of learning pattern classiﬁcation from examples. The relationship of the\nnumber of weights to the learning capacity and the network’s generalization\nability is well understood only for the simple perceptron, a single binary\nunit whose output is a sigmoidal function of the weighted sum of its inputs.\nIn this case, efﬁcient learning algorithms based on theoretical results allow\nthe determination of the optimal weights. However, simple perceptrons can\ngeneralize only those (very few) problems in which the input patterns are\nlinearly separable (LS). In many actual classiﬁcation tasks, multilayered per-\nceptrons with hidden units are needed. However, neither the architecture\n(number of units, number of layers) nor the functions that hidden units\nhave to learn are known a priori, and the theoretical understanding of these\nnetworks is not enough to provide useful hints.\nAlthough pattern classiﬁcation is an intrinsically discrete task, it may be\ncast as a problem of function approximation or regression by assigning real\nvalues to the targets. This is the approach used by backpropagation and\n', 5, 0)
(131.7760009765625, 674.1976318359375, 442.6057434082031, 689.451416015625, 'Neural Computation 10, 1007–1030 (1998)\nc⃝ 1998 Massachusetts Institute of Technology\n', 6, 0)
