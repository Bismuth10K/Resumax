(131.7760009765625, 146.44778442382812, 444.8221130371094, 155.9878692626953, 'LETTER\nCommunicated by Scott Fahlman\n', 0, 0)
(131.7760009765625, 176.12705993652344, 413.3373107910156, 202.5584716796875, 'Efﬁcient Adaptive Learning for Classiﬁcation Tasks with\nBinary Units\n', 1, 0)
(131.7751007080078, 221.29632568359375, 442.6029052734375, 266.8873291015625, 'J. Manuel Torres Moreno\nMirta B. Gordon\nD´epartement de Recherche Fondamentale sur la Mati`ere Condens´ee, CEA Grenoble,\n38054 Grenoble Cedex 9, France\n', 2, 0)
(131.7760009765625, 287.0503234863281, 443.2596740722656, 424.3450622558594, 'This article presents a new incremental learning algorithm for classi-\nﬁcation tasks, called NetLines, which is well adapted for both binary\nand real-valued input patterns. It generates small, compact feedforward\nneural networks with one hidden layer of binary units and binary output\nunits. A convergence theorem ensures that solutions with a ﬁnite num-\nber of hidden units exist for both binary and real-valued input patterns.\nAn implementation for problems with more than two classes, valid\nfor any binary classiﬁer, is proposed. The generalization error and\nthe size of the resulting networks are compared to the best published\nresultsonwell-knownclassiﬁcationbenchmarks.Earlystoppingisshown\nto decrease overﬁtting, without improving the generalization perfor-\nmance.\n', 3, 0)
(131.7760009765625, 454.4285888671875, 195.92027282714844, 465.69195556640625, '1 Introduction\n', 4, 0)
(131.7760009765625, 477.3079528808594, 442.6443786621094, 660.6213989257812, 'Feedforward neural networks have been successfully applied to the prob-\nlem of learning pattern classiﬁcation from examples. The relationship of the\nnumber of weights to the learning capacity and the network’s generalization\nability is well understood only for the simple perceptron, a single binary\nunit whose output is a sigmoidal function of the weighted sum of its inputs.\nIn this case, efﬁcient learning algorithms based on theoretical results allow\nthe determination of the optimal weights. However, simple perceptrons can\ngeneralize only those (very few) problems in which the input patterns are\nlinearly separable (LS). In many actual classiﬁcation tasks, multilayered per-\nceptrons with hidden units are needed. However, neither the architecture\n(number of units, number of layers) nor the functions that hidden units\nhave to learn are known a priori, and the theoretical understanding of these\nnetworks is not enough to provide useful hints.\nAlthough pattern classiﬁcation is an intrinsically discrete task, it may be\ncast as a problem of function approximation or regression by assigning real\nvalues to the targets. This is the approach used by backpropagation and\n', 5, 0)
(131.7760009765625, 674.1976318359375, 442.6057434082031, 689.451416015625, 'Neural Computation 10, 1007–1030 (1998)\nc⃝ 1998 Massachusetts Institute of Technology\n', 6, 0)

page suivante
(133.38999938964844, 117.62150573730469, 444.2142333984375, 128.47036743164062, '1008\nJ. Manuel Torres Moreno and Mirta B. Gordon\n', 0, 0)
(133.38998413085938, 145.053955078125, 444.2774353027344, 660.6318969726562, 'related algorithms, which minimize the squared training error of the out-\nput units. The approximating function must be highly nonlinear because it\nhas to ﬁt a constant value inside the domains of each class and present a\nlarge variation at the boundaries between classes. For example, in a binary\nclassiﬁcation task in which the two classes are coded as +1 and −1, the\napproximating function must be constant and positive in the input space\nregions or domains corresponding to class 1 and constant and negative\nfor those of class −1. The network’s weights are trained to ﬁt this function\neverywhere—in particular, inside the class domains—instead of concentrat-\ning on the relevant problem of the determination of the frontiers between\nclasses. Because the number of parameters needed for the ﬁt is not known\na priori, it is tempting to train a large number of weights that can span, at\nleast in principle, a large set of functions expected to contain the “true” one.\nThis introduces a small bias (Geman, Bienenstock, & Doursat, 1992), but\nleaves us with the difﬁcult problem of minimizing a cost function in a high-\ndimensional space, with the risk that the algorithm gets stuck in spurious\nlocal minima, whose number grows with the number of weights. In prac-\ntice, the best generalizer is determined through a trial-and-error process in\nwhich both the numbers of neurons and weights are varied.\nAn alternative approach is provided by incremental, adaptive, or growth\nalgorithms,inwhichthehiddenunitsaresuccessivelyaddedtothenetwork.\nOne advantage is fast learning, not only because the problem is reduced to\ntraining simple perceptrons but also because adaptive procedures do not\nneed the trial-and-error search for the most convenient architecture. Growth\nalgorithms allow the use of binary hidden neurons, well suited for building\nhardware-dedicated devices. Each binary unit determines a domain bound-\nary in input space. Patterns lying on either side of the boundary are given\ndifferent hidden states. Thus, all the patterns inside a domain in input space\nare mapped to the same internal representation (IR). This binary encoding is\ndifferent for each domain. The output unit performs a logic (binary) function\nof these IRs, a feature that may be useful for rule extraction. Because there\nis not a unique way of associating IRs to the input patterns, different incre-\nmental learning algorithms propose different targets to be learned by the\nappended hidden neurons. This is not the only difference. Several heuristics\nexist that generate fully connected feedforward networks with one or more\nlayers, and treelike architectures with different types of neurons (linear, ra-\ndial basis functions). Most of these algorithms are not optimal with respect\nto the number of weights or hidden units. Indeed, growth algorithms have\noften been criticized because they may generate networks that are too large,\ngenerally believed to be poor generalizers because of overﬁtting.\nThis article presents a new incremental learning algorithm for binary\nclassiﬁcation tasks that generates small feedforward networks. These net-\nworks have a single hidden layer of binary neurons fully connected to the\ninputs and a single output neuron connected to the hidden units. We call\nit NetLines, for Neural Encoder Through Linear Separations. During the\n', 1, 0)

page suivante
(131.7760009765625, 117.62150573730469, 442.60028076171875, 128.47036743164062, 'Classiﬁcation Tasks with Binary Units\n1009\n', 0, 0)
(131.7760009765625, 145.053955078125, 442.6633605957031, 546.05810546875, 'learning process, the targets that each appended hidden unit has to learn\nhelp to decrease the number of classiﬁcation errors of the output neuron.\nThe crucial test for any learning algorithm is the generalization ability of\nthe resulting network. It turns out that the networks built with NetLines are\ngenerally smaller and generalize better than the best networks found so far\non well-known benchmarks. Thus, large networks do not necessarily fol-\nlow from growth heuristics. On the other hand, although smaller networks\nmay be generated with NetLines through early stopping, we found that\nthey do not generalize better than the networks that were trained until the\nnumber of training errors vanished. Thus, overﬁtting does not necessarily\nspoil the network’s performance. This surprising result is in good agreement\nwith recent work on the bias-variance dilemma (Friedman, 1996) showing\nthat, unlike in regression problems where bias and variance compete in the\ndetermination of the optimal generalizer, in the case of classiﬁcation they\ncombine in a highly nonlinear way.\nAlthough NetLines creates networks for two-class problems, multiclass\nproblems may be solved using any strategy that combines binary classiﬁers,\nlike winner-takes-all. We propose a more involved approach, through the\nconstruction of a tree of networks, that may be coupled with any binary\nclassiﬁer.\nNetLines is an efﬁcient approach for creating small, compact classiﬁers\nfor problems with binary or continuous inputs. It is best suited for problems\nrequiring a discrete classiﬁcation decision. Although it may estimate poste-\nrior probabilities, as discussed in section 2.6, this requires more information\nthan the bare network’s output. Another weakness of NetLines is that it is\nnot simple to retrain the network when new patterns are available or class\npriors change over time.\nIn section 2, we give the basic deﬁnitions and present a simple example\nof our strategy, followed by the formal presentation of the growth heuristics\nand the perceptron learning algorithm used to train the individual units.\nIn section 3 we compare NetLines to other growth strategies. The construc-\ntion of trees of networks for multiclass problems is presented in section 4.\nA comparison of the generalization error and the network’s size, with re-\nsults obtained with other learning procedures, is presented in section 5. The\nconclusions are set out in section 6.\n', 1, 0)
(131.7760009765625, 557.549560546875, 291.8859558105469, 568.8129272460938, '2 The Incremental Learning Strategy\n', 2, 0)
(131.7603759765625, 580.4219360351562, 442.62725830078125, 660.6173706054688, '2.1 Deﬁnitions. We are given a training set of P input-output examples\n{⃗ξµ, τ µ}, where µ = 1, 2, . . . , P. The inputs ⃗ξµ = (1, ξµ\n1 , ξµ\n2 , . . . , ξµ\nN) may be\nbinary or real valued N+1 dimensional vectors. The ﬁrst component ξµ\n0 ≡ 1,\nthe same for all the patterns, allows us to treat the bias as a supplementary\nweight. The outputs are binary, τ µ = ±1. These patterns are used to learn\nthe classiﬁcation task with the growth algorithm. Assume that, at a given\nstage of the learning process, the network already has h binary neurons\n', 3, 0)

page suivante
(133.38999938964844, 117.62150573730469, 444.2142333984375, 128.47036743164062, '1010\nJ. Manuel Torres Moreno and Mirta B. Gordon\n', 0, 0)
(133.375732421875, 145.053955078125, 444.2405090332031, 208.68597412109375, 'in the hidden layer. These neurons are connected to the N + 1 input units\nthrough synaptic weights ⃗wk = (wk0, wk1 · · · wkN), 1 ≤ k ≤ h, wk0 being the\nbias.\nThen, given an input pattern ⃗ξ, the states σk of the hidden neurons (1 ≤\nk ≤ h) given by\n', 1, 0)
(157.29153442382812, 219.31396484375, 196.1637420654297, 238.74658203125, 'σk = sign\n', 2, 0)
(197.7406005859375, 204.70947265625, 217.69097900390625, 247.06507873535156, 'Ã N\nX\n', 3, 0)
(205.56100463867188, 219.39913940429688, 237.6397705078125, 245.5081787109375, 'i=0\nwkiξi\n', 4, 0)
(238.13800048828125, 204.7098388671875, 244.69725036621094, 239.92909240722656, '!\n', 5, 0)
(247.32568359375, 218.66087341308594, 444.2321472167969, 238.74658203125, '≡ sign( ⃗wk · ⃗ξ)\n(2.1)\n', 6, 0)
(133.37767028808594, 246.9859619140625, 444.23468017578125, 269.89599609375, 'deﬁne the pattern’s h-dimensional IR, ⃗σ(h) = (1, σ1, . . . , σh). The network’s\noutput ζ(h) is:\n', 7, 0)
(157.2891082763672, 287.029541015625, 204.2970428466797, 304.83319091796875, 'ζ(h) = sign\n', 8, 0)
(205.8739013671875, 272.425048828125, 225.83497619628906, 314.7790832519531, 'Ã h\nX\n', 9, 0)
(213.11000061035156, 287.1131286621094, 248.12644958496094, 313.5082092285156, 'k=0\nWkσk\n', 10, 0)
(248.66000366210938, 272.423828125, 255.21925354003906, 307.6430969238281, '!\n', 11, 0)
(257.8476867675781, 278.102783203125, 444.2373352050781, 313.32208251953125, '≡ sign\nh\n⃗W(h) · ⃗σ(h)\ni\n(2.2)\n', 12, 0)
(133.382568359375, 316.1331481933594, 444.2494201660156, 376.1833801269531, 'where ⃗W(h) = (W0, W1, . . . , Wh are the output unit weights. Hereafter,\n⃗σ µ(h) = (1, σ µ\n1 , . . . , σ µ\nh ) is the h-dimensional IR associated by the network\nof h hidden units to pattern ⃗ξµ. During the training process, h increases\nthrough the addition of hidden neurons, and we denote the ﬁnal number\nof hidden units as H.\n', 13, 0)
(133.376708984375, 385.65313720703125, 444.2768859863281, 660.61962890625, '2.2 Example. We ﬁrst describe the general strategy on a schematic ex-\nample (see Figure 1). Patterns in the gray region belong to class τ = +1, the\nothers to τ = −1. The algorithm proceeds as follows. A ﬁrst hidden unit\nis trained to separate the input patterns at best and ﬁnds one solution, say\n⃗w1, represented on Figure 1 by the line labeled 1, with the arrow pointing\ninto the positive half-space. Because training errors remain, a second hid-\nden neuron is introduced. It is trained to learn targets τ2 = +1 for patterns\nwell classiﬁed by the ﬁrst neuron and τ2 = −1 for the others (the opposite\nconvention could be adopted, both being strictly equivalent), and suppose\nthat solution ⃗w2 is found. Then an output unit is connected to the two hid-\nden neurons and is trained with the original targets. Clearly it will fail to\nseparate all the patterns correctly because the IR (−1, 1) and (+−) are not\nfaithful, as patterns of both classes are mapped onto them. The output neu-\nron is dropped, and a third hidden unit is appended and trained with targets\nτ3 = +1 for patterns that were correctly classiﬁed by the output neuron and\nτ3 = −1 for the others. Solution ⃗w3 is found, and it is easy to see that now\nthe IRs are faithful, that is, patterns belonging to different classes are given\ndifferent IRs. The algorithm converged with three hidden units that deﬁne\nthree domain boundaries determining six regions or domains in the input\nspace. It is straightforward to verify that the IRs corresponding to each do-\nmain on Figure 1 are linearly separable. Thus, the output unit will ﬁnd the\ncorrect solution to the training problem. If the faithful IRs were not linearly\nseparable, the output unit would not ﬁnd a solution without training errors,\nand the algorithm would go on appending hidden units that should learn\n', 14, 0)

page suivante
(131.7760009765625, 117.62150573730469, 442.6003112792969, 128.47036743164062, 'Classiﬁcation Tasks with Binary Units\n1011\n', 0, 0)
(274.81201171875, 163.44760131835938, 281.4345397949219, 179.70445251464844, '3\n', 1, 0)
(362.79498291015625, 222.47756958007812, 369.4175109863281, 238.7344207763672, '1\n', 2, 0)
(364.5423278808594, 259.7845153808594, 371.16485595703125, 276.0413818359375, '2\n', 3, 0)
(250.67300415039062, 216.8419189453125, 267.3080749511719, 229.4139404296875, '- + -\n', 4, 0)
(249.40280151367188, 266.2698974609375, 266.0378723144531, 278.8419189453125, '+ - -\n', 5, 0)
(289.74053955078125, 206.60992431640625, 311.24822998046875, 219.18194580078125, '- + + \n', 6, 0)
(299.7455139160156, 241.39794921875, 321.0044860839844, 253.969970703125, '+ + +\n', 7, 0)
(281.16510009765625, 276.8159484863281, 300.11212158203125, 289.3879699707031, '+ - +\n', 8, 0)
(246.38528442382812, 238.24993896484375, 265.3323059082031, 250.82196044921875, '+ + -\n', 9, 0)
(131.7760009765625, 348.7044982910156, 442.6100158691406, 403.3886413574219, 'Figure 1: Patterns inside the gray region belong to one class, those in the white\nregiontotheother.Thelines(labeled1,2, and3)representthehyperplanesfound\nwith the NetLines strategy. The arrows point into the correspondent positive\nhalf-spaces. The IRs of each domain are indicated (the ﬁrst component, σ0 = 1,\nis omitted for clarity).\n', 10, 0)
(131.7750701904297, 424.74395751953125, 442.61138916015625, 459.11138916015625, 'targets τ = 1 for well-learned patterns, and τ = −1 for the others. A proof\nthat a solution to this strategy with a ﬁnite number of hidden units exists is\nleft to the appendix.\n', 11, 0)
(131.76219177246094, 470.7041320800781, 442.65496826171875, 596.7259521484375, '2.3 The Algorithm NetLines. Like most other adaptive learning algo-\nrithms, NetLines combines a growth heuristics with a particular learning\nalgorithm for training the individual units, which are simple perceptrons.\nIn this section, we present the growth heuristics ﬁrst, followed by the de-\nscription of Minimerror, our perceptron learning algorithm.\nWe ﬁrst introduce the following useful remark: if a neuron has to learn a\ntarget τ, and the learned state turns out to be σ, then the product στ = 1 if\nthe target has been correctly learned, and στ = −1 otherwise.\nGiven a maximal accepted number of hidden units, Hmax, and a maximal\nnumber of tolerated training errors, Emax, the Netlines algorithm may be\nsummarized as follows:\n', 12, 0)
(143.72264099121094, 608.7152709960938, 191.31265258789062, 619.9786376953125, 'Algorithm.\n', 13, 0)
(143.72264099121094, 626.2804565429688, 324.97161865234375, 670.3380126953125, '• Initialize\nh = 0;\nset the targets τ µ\nh+1 = τ µ for µ = 1, . . . , P;\n', 14, 0)

page suivante
(133.38999938964844, 117.62150573730469, 444.2142333984375, 128.47036743164062, '1012\nJ. Manuel Torres Moreno and Mirta B. Gordon\n', 0, 0)
(145.34500122070312, 145.08233642578125, 185.0251007080078, 162.85760498046875, '• Repeat\n', 1, 0)
(172.9922637939453, 165.60528564453125, 400.1077880859375, 230.947998046875, '1. /* train the hidden units */\nh = h + 1; /* connect hidden unit h to the inputs */\nlearn the training set {⃗ξµ, τ µ\nh }, µ = 1, . . . , P;\nafter learning, σ µ\nh = sign( ⃗wh · ⃗ξµ), µ = 1, . . . , P;\nif h = 1 /* for the ﬁrst hidden neuron */\n', 2, 0)
(192.41192626953125, 228.3714599609375, 408.8033752441406, 250.59710693359375, 'if σ µ\n1 = τ µ\n1 ∀µ then stop. /* the training set is LS */;\n', 3, 0)
(192.41653442382812, 241.27996826171875, 381.6926574707031, 265.89007568359375, 'else set τ µ\nh+1 = σ µ\nh τ µ for µ = 1, . . . , P; go to 1;\n', 4, 0)
(185.0657958984375, 261.30999755859375, 210.5644989013672, 272.5733642578125, 'end if\n', 5, 0)
(172.98561096191406, 276.289306640625, 423.5335998535156, 317.00775146484375, '2. /* learn the mapping between the IRs and the outputs */\nconnect the output neuron to the h trained hidden units;\nlearn the training set {⃗σ µ(h), τ µ}; µ = 1, . . . , P;\n', 6, 0)
(185.0636444091797, 304.81744384765625, 408.8309020996094, 340.0370788574219, 'after learning, ζ µ(h) = sign\n³\n⃗W(h) · ⃗σ µ´\n, µ = 1, . . . , P;\n', 7, 0)
(185.0622100830078, 325.3239440917969, 419.5107116699219, 378.3186950683594, 'set τ µ\nh+1 = ζ µτ µ for µ = 1, . . . , P;\ncount the number of training errors e = P\nµ(1 − τ µ\nh+1)/2;\n', 8, 0)
(145.3314208984375, 362.12493896484375, 277.1967468261719, 379.9299621582031, '• Until (h = Hmax or e ≤ Emax);\n', 9, 0)
(133.38270568847656, 388.6534423828125, 444.2520751953125, 434.478271484375, 'The generated network has H = h hidden units. In the appendix we present\na solution to the learning strategy with a bounded number of hidden units.\nIn practice, the algorithm ends up with much smaller networks than this\nupper bound, as will be shown in section 5.\n', 10, 0)
(133.37322998046875, 449.0562744140625, 444.2710266113281, 563.6189575195312, '2.4 The Perceptron Learning Algorithm. The ﬁnal number of hidden\nneurons, which are simple perceptrons, depends on the performance of the\nlearning algorithm used to train them. The best solution should minimize\nthe number of errors. If the training set is LS, it should endow the units with\nthe lowest generalization error. Our incremental algorithm uses Minimerror\n(Gordon & Berchier, 1993) to train the hidden and output units. Minimer-\nror is based on the minimization of a cost function E that depends on the\nperceptron weights ⃗w through the stabilities of the training patterns. If the\ninput vector is ⃗ξµ and τ µ the corresponding target, then the stability γ µ of\npattern µ is a continuous and derivable function of the weights, given by:\n', 11, 0)
(157.28466796875, 582.0910034179688, 216.72715759277344, 607.1815795898438, 'γ µ = τ µ ⃗w · ⃗ξµ\n', 12, 0)
(197.08299255371094, 589.3778076171875, 444.2369079589844, 613.901611328125, '∥ ⃗w∥ ,\n(2.3)\n', 13, 0)
(133.38638305664062, 619.2086791992188, 197.86102294921875, 644.05517578125, 'where ∥ ⃗w∥ =\n√\n', 14, 0)
(133.3862762451172, 626.2500610351562, 444.2281188964844, 666.9684448242188, '⃗w · ⃗w. The stability is independent of the norm of the weights\n∥ ⃗w∥. It measures the distance of the pattern to the separating hyperplane,\nwhich is normal to ⃗w; it is positive if the pattern is well classiﬁed, negative\n', 15, 0)

page suivante
(131.7760009765625, 117.62150573730469, 442.60028076171875, 128.47036743164062, 'Classiﬁcation Tasks with Binary Units\n1013\n', 0, 0)
(131.7760009765625, 145.053955078125, 269.5201110839844, 156.506591796875, 'otherwise. The cost function E is:\n', 1, 0)
(155.6874237060547, 165.946044921875, 180.03897094726562, 190.38372802734375, 'E = 1\n', 2, 0)
(175.30499267578125, 179.2989501953125, 180.03749084472656, 190.7515869140625, '2\n', 3, 0)
(182.968994140625, 163.96188354492188, 196.36196899414062, 200.33106994628906, 'P\nX\n', 4, 0)
(182.8090057373047, 185.65606689453125, 196.51988220214844, 198.774169921875, 'µ=1\n', 5, 0)
(198.09800720214844, 160.8148193359375, 252.23416137695312, 196.03407287597656, '·\n1 − tanh γ µ\n', 6, 0)
(241.58700561523438, 179.2989501953125, 252.1026153564453, 190.7515869140625, '2T\n', 7, 0)
(253.9284210205078, 160.81378173828125, 442.6208190917969, 196.0330352783203, '¸\n.\n(2.4)\n', 8, 0)
(131.76800537109375, 203.0703125, 442.65667724609375, 443.6619873046875, 'The contribution to E of patterns with large negative stabilities is ≃ 1, that\nis, they are counted as errors, whereas the contribution of patterns with\nlarge, positive stabilities is vanishingly small. Patterns at both sides of the\nhyperplane within a window of width ≈ 4T contribute to the cost function\neven if they have positive stability.\nThe properties of the global minimum of equation 2.4 have been studied\ntheoretically with methods of statistical mechanics (Gordon & Grempel,\n1995). It was shown that in the limit T → 0, the minimum of E corresponds\nto the weights that minimize the number of training errors. If the training\nset is LS, these weights are not unique (Gyorgyi & Tishby, 1990). In that case,\nthere is an optimal learning temperature such that the weights minimizing\nE at that temperature endow the perceptron with a generalization error\nnumerically indistinguishable from the optimal (Bayesian) value.\nThe algorithm Minimerror (Gordon & Berchier, 1993; Rafﬁn & Gordon,\n1995) implements a minimization of E restricted to a subspace of normalized\nweights, through a gradient descent combined with a slow decrease of the\ntemperature T, which is equivalent to a deterministic annealing. It has been\nshown that the convergence is faster if patterns with negative stabilities are\nconsidered at a temperature T− larger than those with positive stabilities,\nT+, with a constant ratio θ = T−/T+. The weights and the temperatures are\niteratively updated through:\n', 9, 0)
(155.679443359375, 462.00042724609375, 193.951171875, 479.7189025878906, 'δ ⃗w(t) = ϵ\n', 10, 0)
(196.04861450195312, 447.3107604980469, 221.29367065429688, 489.6666259765625, '" X\n', 11, 0)
(201.9320068359375, 474.9910888671875, 227.27139282226562, 488.10919189453125, 'µ/γ µ≤0\n', 12, 0)
(250.8000030517578, 454.6258544921875, 271.1741638183594, 471.1896057128906, 'τ µ⃗ξµ\n', 13, 0)
(230.04299926757812, 454.4454345703125, 324.6710205078125, 489.6647033691406, 'cosh2(γ µ/2T−)\n+\nX\n', 14, 0)
(305.2130126953125, 474.9910888671875, 330.7337341308594, 483.42962646484375, 'µ/γ µ>0\n', 15, 0)
(354.2640075683594, 454.6258544921875, 374.6361389160156, 471.1896057128906, 'τ µ⃗ξµ\n', 16, 0)
(333.5060119628906, 468.2060852050781, 395.8924560546875, 486.9231872558594, 'cosh2(γ µ/2T+)\n', 17, 0)
(397.0878601074219, 447.308837890625, 402.96563720703125, 482.5281066894531, '#\n', 18, 0)
(424.4786376953125, 461.913330078125, 442.6136169433594, 473.365966796875, '(2.5)\n', 19, 0)
(155.67453002929688, 486.2890930175781, 442.6165771484375, 507.11260986328125, 'T−1\n+ (t + 1) = T−1\n+ (t) + δT−1; T− = θT+;\n(2.6)\n', 20, 0)
(155.677490234375, 501.05291748046875, 209.5323944091797, 536.2721557617188, '⃗w(t + 1) =\np\n', 21, 0)
(209.53900146484375, 502.1731262207031, 289.5107116699219, 526.5255737304688, 'N + 1 ⃗w(t) + δ ⃗w(t)\n', 22, 0)
(234.6649932861328, 508.7218017578125, 442.62054443359375, 533.24560546875, '∥ ⃗w(t) + δ ⃗w(t)∥ .\n(2.7)\n', 23, 0)
(131.77001953125, 534.5943603515625, 442.6379699707031, 644.055908203125, 'Notice from equation 2.5 that only the incorrectly learned patterns at dis-\ntances shorter than ≈ 2T− from the hyperplane, and those correctly learned\nlying closer than ≈ 2T+, contribute effectively to learning. The contribu-\ntion of patterns outside this region is vanishingly small. By decreasing the\ntemperature, the algorithm selects to learn patterns increasingly localized\nin the neighborhood of the hyperplane, allowing for a highly precise de-\ntermination of the parameters deﬁning the hyperplane, which are the neu-\nron’s weights. Normalization 2.7 restricts the search to the subspace with\n∥ ⃗w∥ =\n√\n', 24, 0)
(131.7732696533203, 626.2499389648438, 442.6067810058594, 666.9683837890625, 'N + 1.\nThe only adjustable parameters of the algorithm are the temperature ratio\nθ = T−/T+, the learning rate ϵ, and the annealing rate δT−1. In principle,\n', 25, 0)

page suivante
(133.38999938964844, 117.62150573730469, 444.2142333984375, 128.47036743164062, '1014\nJ. Manuel Torres Moreno and Mirta B. Gordon\n', 0, 0)
(133.38999938964844, 145.053955078125, 444.20159912109375, 167.9639892578125, 'they should be adapted to each speciﬁc problem. However, as a result of\nour normalizing the weights to\n√\n', 1, 0)
(133.38182067871094, 156.51092529296875, 444.259765625, 190.87835693359375, 'N + 1 and to data standardization (see the\nnext section), all the problems are brought to the same scale, simplifying the\nchoice of the parameters.\n', 2, 0)
(133.38180541992188, 202.46826171875, 444.25982666015625, 236.835693359375, '2.5 Data Standardization. Instead of determining the best parameters\nfor each new problem, we standardize the input patterns of the training set\nthrough a linear transformation, applied to each component:\n', 3, 0)
(157.29324340820312, 247.78045654296875, 216.85064697265625, 275.287109375, '˜ξ µ\ni = ξµ\ni − ⟨ξi⟩\n', 4, 0)
(194.1479949951172, 255.751953125, 444.23590087890625, 274.5908508300781, '1i\n; 1 ≤ i ≤ N.\n(2.8)\n', 5, 0)
(133.3853759765625, 284.30010986328125, 351.4422607421875, 303.232177734375, 'The mean ⟨ξi⟩ and the variance △2\ni , deﬁned as usual,\n', 6, 0)
(157.90957641601562, 311.4609069824219, 191.14569091796875, 335.90093994140625, '⟨ξi⟩ = 1\n', 7, 0)
(185.88699340820312, 324.9001159667969, 191.67010498046875, 336.2013244628906, 'P\n', 8, 0)
(194.6020050048828, 309.4778747558594, 207.99497985839844, 345.8470764160156, 'P\nX\n', 9, 0)
(194.4429931640625, 316.7574462890625, 444.2249755859375, 344.2901916503906, 'µ=1\nξµ\ni\n(2.9)\n', 10, 0)
(157.285888671875, 345.4319763183594, 191.14498901367188, 369.87060546875, '1i2 = 1\n', 11, 0)
(185.88699340820312, 358.87213134765625, 191.67010498046875, 370.17333984375, 'P\n', 12, 0)
(194.6020050048828, 343.4498596191406, 207.99497985839844, 379.819091796875, 'P\nX\n', 13, 0)
(194.4429931640625, 345.4319763183594, 275.2303466796875, 378.26220703125, 'µ=1\n(ξµ\ni − ⟨ξi⟩)2 = 1\n', 14, 0)
(269.97198486328125, 358.87213134765625, 275.7550964355469, 370.17333984375, 'P\n', 15, 0)
(278.6860046386719, 343.4498596191406, 292.0789794921875, 379.819091796875, 'P\nX\n', 16, 0)
(278.52801513671875, 349.39886474609375, 444.229248046875, 378.26220703125, 'µ=1\n(ξµ\ni )2 − (⟨ξi⟩)2,\n(2.10)\n', 17, 0)
(133.37872314453125, 385.28436279296875, 444.2168884277344, 408.19439697265625, 'need only a single pass of the P training patterns to be determined. After\nlearning, the inverse transformation is applied to the weights,\n', 18, 0)
(157.29107666015625, 433.9235534667969, 189.32522583007812, 469.142822265625, '˜w0 =\np\n', 19, 0)
(189.32400512695312, 420.9878845214844, 295.6079406738281, 459.3985900878906, 'N + 1\nw0 −\nNP\n', 20, 0)
(285.22900390625, 427.7981262207031, 334.3537902832031, 452.01519775390625, 'i=1\nwi⟨ξi⟩/1i\n', 21, 0)
(214.4510040283203, 441.5949401855469, 444.2259216308594, 493.2060852050781, 'rh\nw0 − PN\nj=1 wj⟨ξj⟩/1j\ni2\n+ PN\nj=1(wj/1j)2\n(2.11)\n', 22, 0)
(158.83529663085938, 473.0875549316406, 189.32522583007812, 508.30682373046875, '˜wi =\np\n', 23, 0)
(189.32400512695312, 474.2100830078125, 444.2272644042969, 532.3720703125, 'N + 1\nwi/1i\nrh\nw0 − PN\nj=1 wj⟨ξj⟩/1j\ni2\n+ PN\nj=1(wj/1j)2\n,\n(2.12)\n', 24, 0)
(133.37673950195312, 523.1384887695312, 444.2546691894531, 660.6181030273438, 'so that the normalization (see equation 2.8) is completely transparent to the\nuser: with the transformed weights (see equations 2.11 and 2.12), the neural\nclassiﬁer is applied to the data in the original user’s units, which do not\nneed to be renormalized.\nAs a consequence of the weights scaling (see equation 2.7) and the in-\nputs standardization (see equation 2.8), all the problems are automatically\nrescaled. This allows us to use always the same values of Minimerror’s pa-\nrameters: the standard values ϵ = 0.02, δT−1 = 10−3, and θ = 6. They were\nused throughout this article, the reported results being highly insensitive to\nslight variations of them. However, in some extremely difﬁcult cases, like\nlearning the parity in dimensions N > 10 and ﬁnding the separation of the\nsonar signals (see section 5), larger values of θ were needed.\n', 25, 0)

page suivante
(131.7760009765625, 117.62150573730469, 442.60028076171875, 128.47036743164062, 'Classiﬁcation Tasks with Binary Units\n1015\n', 0, 0)
(131.7724609375, 145.053955078125, 442.65997314453125, 442.94073486328125, '2.6 Interpretation. Ithasbeenshown(Gordon,Peretto,&Berchier,1993)\nthat the contribution of each pattern to the cost function of Minimerror,\n[1 − tanh(γ µ/2T)]/2, may be interpreted as the probability of misclassiﬁca-\ntion at the temperature T at which the minimum of the cost function has\nbeen determined. By analogy, the neuron’s prediction on a new input ⃗ξ may\nbe given a conﬁdence measure by replacing the (unknown) pattern stabil-\nity by its absolute value ∥γ ∥ = ∥ ⃗w · ⃗ξ∥/∥ ⃗w∥, which is its distance to the\nhyperplane. This interpretation of the sigmoidal function tanh(∥γ ∥/2T) as\nthe conﬁdence on the neuron’s output is similar to the one proposed earlier\n(Goodman, Smyth, Higgins, & Miller, 1992) within an approach based on\ninformation theory.\nThe generalization of these ideas to multilayered networks is not straight-\nforward. An estimate of the conﬁdence on the classiﬁcation by the output\nneuron should include the magnitude of the weighted sums of the hidden\nneurons, as they measure the distances of the input pattern to the domain\nboundaries. However, short distances to the separating hyperplanes are not\nalways correlated to low conﬁdence on the network’s output. For an exam-\nple, we refer again to Figure 1. Consider a pattern lying close to hyperplane\n1. A small, weighted sum on neuron 1 may cast doubt on the classiﬁcation\nif the pattern’s IR is (− + +) but not if it is (− + −), because a change of the\nsign of the weighted sum in the latter case will map the pattern to the IR\n(+ + −) which, being another IR of the same class, will be given the same\noutput by the network. It is worth noting that the same difﬁculty is met by\nthe interpretation of the outputs of multilayered perceptrons, trained with\nbackpropagation, as posterior probabilities. We do not explore this problem\nany further because it is beyond the scope of this article.\n', 1, 0)
(131.77252197265625, 454.43218994140625, 289.2322692871094, 465.695556640625, '3 Comparison with Other Strategies\n', 2, 0)
(131.7760009765625, 477.3079528808594, 442.664306640625, 660.6213989257812, 'There are few learning algorithms for neural networks composed of binary\nunits. To our knowledge, all of them are incremental. In this section, we\ngive a short overview of some of them, in order to put forward the main\ndifferences with NetLines. We discuss the growth heuristics and then the\nindividual unit training algorithms.\nThe Tiling algorithm (M´ezard & Nadal, 1989) introduces hidden layers,\none after the other. The ﬁrst neuron of each layer is trained to learn an IR that\nhelps to decrease the number of training errors; supplementary hidden units\nare then appended to the layer until the IRs of all the patterns in the train-\ning set are faithful. This procedure may generate very large networks. The\nUpstart algorithm (Frean, 1990) introduces successive couples of daughter\nhidden units between the input layer and the previously included hidden\nunits, which become their parents. The daughters are trained to correct\nthe parents’ classiﬁcation errors, one daughter for each class. The obtained\nnetwork has a treelike architecture. There are two different algorithms im-\nplementing the Tilinglike Learning in the Parity Machine (Biehl & Opper,\n', 3, 0)

page suivante
(133.38999938964844, 117.62150573730469, 444.2142333984375, 128.47036743164062, '1016\nJ. Manuel Torres Moreno and Mirta B. Gordon\n', 0, 0)
(133.38189697265625, 145.053955078125, 444.2602844238281, 660.61962890625, '1991), Offset (Martinez & Est`eve, 1992), and MonoPlane (Torres Moreno &\nGordon, 1995). In both, each appended unit is trained to correct the errors\nof the previously included unit in the same hidden layer, a procedure that\nhas been shown to generate a parity machine: the class of the input patterns\nis the parity of the learned IRs. Unlike Offset, which implements the parity\nthrough a second hidden layer that needs to be pruned, MonoPlane goes\non adding hidden units (if necessary) in the same hidden layer until the\nnumber of training errors at the output vanishes. Convergence proofs for\nbinary input patterns have been produced for all these algorithms. In the\ncase of real-valued input patterns, a solution to the parity machine with a\nbounded number of hidden units also exists (Gordon, 1996).\nThe rationale behind the construction of the parity machine is that it\nis not worth training the output unit before all the training errors of the\nhidden units have been corrected. However, Marchand, Golea, and Ruj´an\n(1990) pointed out that it is not necessary to correct all the errors of the\nsuccessively trained hidden units. It is sufﬁcient that the IRs be faithful and\nLS. If the output unit is trained immediately after each appended hidden\nunit, the network may discover that the IRs are already faithful and stop\nadding units. This may be seen in Figure 1. None of the parity machine\nimplementations would ﬁnd the solution represented on the ﬁgure, because\neach of the three perceptrons systematically unlearns part of the patterns\nlearned by the preceding one.\nTo our knowledge, Sequential Learning (Marchand et al., 1990) is the\nonly incremental learning algorithm that might ﬁnd a solution equivalent\n(although not the same) to the one of Figure 1. In this algorithm, the ﬁrst\nunit is trained to separate the training set keeping one “pure” half-space—\ncontaining patterns of only one class. Wrongly classiﬁed patterns, if any,\nmust all lie in the other half-space. Each appended neuron is trained to\nseparate wrongly classiﬁed patterns with this constraint of always keeping\none pure, error-free half-space. Thus, neurons must be appended in a precise\norder, making the algorithm difﬁcult to implement in practice. For example,\nSequential Learning applied to the problem of Figure 1 needs to impose that\nthe ﬁrst unit ﬁnds the weights ⃗w3, the only solution satisfying the purity\nrestriction.\nOther proposed incremental learning algorithms strive to solve the prob-\nlem with different architectures, and/or with real valued units. For example,\nin the algorithm Cascade Correlation (Fahlman & Lebiere, 1990), each ap-\npended unit is selected among a pool of several real-valued neurons, trained\nto learn the correlation between the targets and the training errors. The unit\nis then connected to the input units and to all the other hidden neurons\nalready included in the network.\nAnother approach to learning classiﬁcation tasks is through the construc-\ntion of decision trees (Breiman, Friedman, Olshen, & Stone, 1984), which hi-\nerarchically partition the input space through successive dichotomies. The\nneural networks implementations generate treelike architectures. Each neu-\n', 1, 0)

page suivante
(131.7760009765625, 117.62150573730469, 442.60028076171875, 128.47036743164062, 'Classiﬁcation Tasks with Binary Units\n1017\n', 0, 0)
(131.7760009765625, 145.053955078125, 442.6631774902344, 546.05810546875, 'ron of the tree introduces a dichotomy of the input space, which is treated\nseparately by the children nodes, which eventually produce new splits. Be-\nsides the weights, the resulting networks need to store the decision path.\nThe proposed heuristics (Sirat & Nadal, 1990; Farrell & Mammone, 1994;\nKnerr, Personnaz, & Dreyfus, 1990) differ in the algorithm used to train each\nnode and/or in the stopping criterion. In particular, Neural-Trees (Sirat &\nNadal, 1990) may be regarded as a generalization of Classiﬁcation and Re-\ngression Trees (CART) (Breiman et al., 1984) in which the hyperplanes are\nnot constrained to be perpendicular to the coordinate axis. The heuristics of\nthe Modiﬁed Neural Tree Network (MNTN) (Farrell & Mammone, 1994),\nsimilar to Neural-Trees, includes a criterion of early stopping based on a\nconﬁdence measure of the partition. As NetLines considers the whole input\nspace to train each hidden unit, it generates domain boundaries that may\ngreatly differ from the splits produced by trees. We are not aware of any\nsystematic study or theoretical comparison of both approaches.\nOther algorithms, like Restricted Coulomb Energy (RCE) (Reilly, Cooper,\n& Elbaum, 1982), Grow and Learn (GAL) (Alpaydin, 1990), Glocal (Depe-\nnau, 1995), and Growing Cells (Fritzke, 1994), propose to cover or mask the\ninput space with hyperspheres of adaptive size containing patterns of the\nsame class. These approaches generally end up with a very large number of\nunits. Covering Regions by the LP Method (Mukhopadhyay, Roy, Kim, &\nGovil, 1993) is a trial-and-error procedure devised to select the most efﬁcient\nmasks among hyperplanes, hyperspheres, and hyperellipsoids. The mask’s\nparameters are determined through linear programming.\nMany incremental strategies use the Pocket algorithm (Gallant, 1986)\nto train the appended units. Its main drawback is that it has no natural\nstopping condition, which is left to the user’s patience. The proposed alter-\nnative algorithms (Frean, 1992; Bottou & Vapnik, 1992) are not guaranteed\nto ﬁnd the best solution to the problem of learning. The algorithm used by\nthe MNTN (Farrell & Mammone, 1994) and the ITRULE (Goodman et al.,\n1992) minimize cost functions similar to equation 2.4, but using different\nmisclassiﬁcation measures at the place of our stability (see equation 2.3).\nThe essential difference with Minimerror is that none of these algorithms is\nable to control which patterns contribute to learning, as Minimerror does\nwith the temperature.\n', 1, 0)
(131.7760009765625, 557.549560546875, 307.6261901855469, 568.8129272460938, '4 Generalization to Multiclass Problems\n', 2, 0)
(131.7760009765625, 580.4219360351562, 442.6634826660156, 660.618896484375, 'The usual way to cope with problems having more than two classes is to\ngenerate as many networks as classes. Each network is trained to separate\npatterns of one class from all the others, and a winner-takes-all (WTA) strat-\negy based on the value of the output’s weighted sum in equation 2.2 is used\nto decide the class if more than one network recognizes the input pattern. In\nour case, because we use normalized weights, the output’s weighted sum\nis merely the distance of the IR to the separating hyperplane. All the pat-\n', 3, 0)

page suivante
(133.38999938964844, 117.62150573730469, 444.2142333984375, 128.47036743164062, '1018\nJ. Manuel Torres Moreno and Mirta B. Gordon\n', 0, 0)
(133.37408447265625, 145.053955078125, 444.27728271484375, 477.3071594238281, 'terns mapped to the same IR are given the same output’s weighted sum,\nindependent of the relative position of the pattern in input space. A strong\nweighted sum on the output neuron is not inconsistent with small weighted\nsums on the hidden neurons. Therefore, a naive WTA decision may not give\ngood results, as shown in the example in section 5.3.1.\nWe now describe an implementation for the multiclass problem that re-\nsults in a treelike architecture of networks. It is more involved than the naive\nWTA and may be applied to any binary classiﬁer. Suppose that we have a\nproblem with C classes. We must choose in which order the classes will\nbe learned, say (c1, c2, . . . , cC). This order constitutes a particular learning\nsequence. Given a particular learning sequence, a ﬁrst network is trained\nto separate class c1, which is given output target τ1 = +1, from the others\n(which are given targets τ1 = −1). The opposite convention is equivalent\nand could equally be used. After training, all the patterns of class c1 are\neliminated from the training set, and we generate a second network trained\nto separate patterns of class c2 from the remaining classes. The procedure,\nreiterated with training sets of decreasing size, generates C − 1 hierarchi-\ncally organized tree of networks (TON): the outputs are ordered sequences\n⃗ζ = (ζ1, ζ2, . . . , ζC−1). The predicted class of a pattern is ci, where i is the\nﬁrst network in the sequence having an output +1 (ζi = +1 and ζj = −1 for\nj < i), the outputs of the networks with j > i being irrelevant.\nThe performance of the TON may depend on the chosen learning se-\nquence. Therefore, it is convenient that an odd number of TONs, trained\nwith different learning sequences, compete through a vote. We veriﬁed em-\npirically, as is shown in section 5.3, that this vote improves the results ob-\ntained with each of the individual TONs participating in the vote. Notice\nthat our procedure is different from bagging (Breiman, 1994); all the net-\nworks of the TON are trained with the same training set, without the need\nof any resampling procedure.\n', 1, 0)
(133.3758087158203, 488.7986145019531, 198.56121826171875, 500.0619812011719, '5 Applications\n', 2, 0)
(133.38998413085938, 511.678955078125, 444.2593078613281, 660.6201782226562, 'Although convergence proofs of learning algorithms are satisfactory on the-\noretical grounds, they are not a guarantee of good generalization. In fact,\nthey demonstrate only that correct learning is possible; they do not address\nthe problem of generalization. This last issue still remains quite empirical\n(Vapnik, 1992; Geman et al., 1992; Friedman, 1996), and the generalization\nperformance of learning algorithms is usually tested on well-known bench-\nmarks (Prechelt, 1994).\nWe ﬁrst tested the algorithm on learning the parity function of N bits for\n2 ≤ N ≤ 11. It is well known that the smallest network with the architecture\nconsidered here needs H = N hidden neurons. The optimal architecture\nwas found in all the cases. Although this is quite an unusual performance,\nthe parity is not a representative problem: learning is exhaustive, and gen-\neralization cannot be tested. Another test, the classiﬁcation of sonar signals\n', 3, 0)

page suivante
(131.7760009765625, 117.62150573730469, 442.60028076171875, 128.47036743164062, 'Classiﬁcation Tasks with Binary Units\n1019\n', 0, 0)
(131.7644500732422, 145.053955078125, 446.68109130859375, 523.137939453125, '(Gorman & Sejnowski, 1988), revealed the quality of Minimerror, as it solved\nthe problem without hidden units. In fact, we found that not only the train-\ning set of this benchmark is linearly separable, a result already reported\n(Hoehfeld & Fahlman, 1991; Roy, Kim, & Mukhopadhyay, 1993), but that\nthe complete database—the training and the test sets together—is also lin-\nearly separable (Torres Moreno & Gordon, 1998).\nWenextpresentourresults,generalizationerrorϵg andnumberofweights,\non several benchmarks corresponding to different kinds of problems: binary\nclassiﬁcation of binary input patterns, binary classiﬁcation of real-valued\ninput patterns, and multiclass problems. These benchmarks were chosen\nbecause they have already served as a test for many other algorithms, pro-\nviding us with unbiased results for comparison. The generalization error\nϵg of NetLines was estimated as usual, through the fraction of misclassiﬁed\npatterns on a test set of data.\nThe results are reported as a function of the training sets sizes P whenever\nthese sizes are not speciﬁed by the benchmark. Besides the generalization\nerror ϵg, averaged over a (speciﬁed) number of classiﬁers trained with ran-\ndomly selected training sets, we also present the number of weights of the\ncorresponding networks which is a measure of the classiﬁer’s complexity,\nas it corresponds to the number of its parameters.\nTraining times are usually cited among the characteristics of the training\nalgorithms. Only the numbers of epochs used by backpropagation on two\nof the studied benchmarks have been published; we restrict the comparison\nto these cases. As NetLines updates only N weights per epoch, whereas\nbackpropagation updates all the network’s weights, we compare the total\nnumber of weights updates. They are of the same order of magnitude for\nboth algorithms. However, these comparisons should be taken with cau-\ntion. NetLines is a deterministic algorithm; it learns the architecture and\nthe weights through a single run, whereas with backpropagation several\narchitectures must be previously investigated, and this time is not included\nin the training time.\nThefollowingnotationisused:Disthetotalnumberofavailablepatterns,\nP the number of training patterns, and G the number of test patterns.\n', 1, 0)
(131.7741241455078, 534.60009765625, 442.6711120605469, 603.339599609375, '5.1 Binary Inputs. The case of binary input patterns has the property,\nnot shared by real-valued inputs, that every pattern may be separated from\nthe others by a single hyperplane. This solution, usually called grandmother,\nneeds as many hidden units as patterns in the training set. In fact, the conver-\ngence proofs for incremental algorithms in the case of binary input patterns\nare based on this property.\n', 2, 0)
(131.77511596679688, 614.8026733398438, 442.6529235839844, 666.9784545898438, '5.1.1 Monk’s Problem.\nThis benchmark, thoroughly studied with many\ndifferent learning algorithms (Trhun et al., 1991), contains three distinct\nproblems. Each has an underlying logical proposition that depends on six\ndiscrete variables, coded with N = 17 binary numbers. The total number of\n', 3, 0)

page suivante
(133.38999938964844, 117.62150573730469, 444.2142333984375, 128.47036743164062, '1020\nJ. Manuel Torres Moreno and Mirta B. Gordon\n', 0, 0)
(133.3768310546875, 145.053955078125, 444.26904296875, 397.1053771972656, 'possible input patterns is D = 432, and the targets correspond to the truth ta-\nble of the corresponding proposition. Both NetLines and MonoPlane found\nthe underlying logical proposition of the ﬁrst two problems; they general-\nized correctly, giving ϵg = 0. In fact, these are easy problems: all the neural\nnetwork–based algorithms, and some nonneural learning algorithms were\nreported to generalize them correctly. In the third Monk’s problem, 6 pat-\nterns among the P3 = 122 examples are given wrong targets. The general-\nization error is calculated over the complete set of D = 432 patterns, that is,\nincluding the training patterns, but in the test set all the patterns are given\nthe correct targets. Thus, any training method that learns the training set\ncorrectly will make at least 1.4% of generalization errors. Four algorithms\nspeciallyadaptedtonoisyproblemswerereportedtoreachϵg = 0.However,\nnone of them generalizes correctly the two other (noiseless) Monk’s prob-\nlems. Besides them, the best performance, ϵg = 0.0277, which corresponds\nto 12 misclassiﬁed patterns, is reached only by neural networks methods:\nbackpropagation, backpropagation with weight decay, cascade correlation,\nand NetLines. The number of hidden units generated with NetLines (58\nweights) is intermediate between backpropagation with weight decay (39)\nand cascade correlation (75) or backpropagation (77). MonoPlane reached a\nslightly worse performance (ϵg = 0.0416, or 18 misclassiﬁed patterns) with\nthe same number of weights as NetLines, showing that the parity machine\nencoding may not be optimal.\n', 1, 0)
(133.37530517578125, 408.5684509277344, 444.261962890625, 660.6181030273438, '5.1.2 Two or More Clumps.\nIn this problem (Denker et al., 1987) the net-\nwork has to discriminate if the number of clumps in a ring of N bits is strictly\nsmaller than 2 or not. One clump is a sequence of identical bits bounded by\nbits of the other kind. The patterns are generated through a Monte Carlo\nmethod in which the mean number of clumps is controlled by a parameter\nk (M´ezard & Nadal, 1989). We generated training sets of P patterns with\nk = 3, corresponding to a mean number of clumps of ≈ 1.5, for rings of\nN = 10 and N = 25 bits. The generalization error corresponding to sev-\neral learning algorithms, estimated with independently generated testing\nsets of the same sizes as the training sets, G = P, are displayed in Figure 2\nas a function of P. Points with error bars correspond to averages over 25\nindependent training sets. Points without error bars correspond to best re-\nsults. NetLines, MonoPlane, and Upstart for N = 25 have nearly the same\nperformances when trained to reach error-free learning.\nWe tested the effect of early stopping by imposing on NetLines a maximal\nnumber of two hidden units (H = 2). The residual training error ϵt is plotted\non Figure 2, as a function of P. Note that early stopping does not help to de-\ncrease ϵg. Overﬁtting, which arises when NetLines is applied until error-free\ntraining is reached, does not degrade the network’s generalization perfor-\nmance. This behavior is very different from the one of networks trained\nwith backpropagation. The latter reduces classiﬁcation learning to a regres-\nsion problem, in which the generalization error can be decomposed in two\n', 2, 0)

page suivante
(131.7760009765625, 117.62150573730469, 442.60028076171875, 128.47036743164062, 'Classiﬁcation Tasks with Binary Units\n1021\n', 0, 0)
(300.9020080566406, 289.8901062011719, 429.7143249511719, 295.6671142578125, '\x13\n\x14\x13\x13\n\x15\x13\x13\n\x16\x13\x13\n\x17\x13\x13\n\x18\x13\x13\n\x19\x13\x13\n', 1, 0)
(363.5299987792969, 264.989013671875, 393.9224548339844, 272.9110107421875, '1HW/LQHV εW\n', 2, 0)
(343.135986328125, 222.8091278076172, 385.0878601074219, 228.5861053466797, '1HW/LQHV \x0b+ \x15\x0c\n', 3, 0)
(379.45001220703125, 231.84910583496094, 404.5670471191406, 237.62608337402344, '1HW/LQHV\n', 4, 0)
(365.7671203613281, 173.1510467529297, 404.2870178222656, 178.9280242919922, '7LOLQJ\x0f*URZWK\n', 5, 0)
(404.3160095214844, 234.8621063232422, 425.6243591308594, 240.6390838623047, '8SVWDUW\n', 6, 0)
(366.0299987792969, 296.60601806640625, 370.76702880859375, 303.6960144042969, '3\n', 7, 0)
(364.18798828125, 158.49969482421875, 429.8117370605469, 176.33460998535156, '\x15 RU PRUH FOXPSV\n1 \x15\x18\n', 8, 0)
(151.0436553955078, 284.1250915527344, 288.0144958496094, 295.6671142578125, '\x13\n\x14\x13\x13\n\x15\x13\x13\n\x16\x13\x13\n\x17\x13\x13\n\x18\x13\x13\n\x13\x11\x13\n', 9, 0)
(151.0436553955078, 257.7893981933594, 159.72853088378906, 263.56640625, '\x13\x11\x14\n', 10, 0)
(151.0436553955078, 231.45372009277344, 159.72853088378906, 237.23069763183594, '\x13\x11\x15\n', 11, 0)
(151.0436553955078, 205.24888610839844, 159.72853088378906, 211.02586364746094, '\x13\x11\x16\n', 12, 0)
(151.0436553955078, 178.91319274902344, 159.72853088378906, 184.69017028808594, '\x13\x11\x17\n', 13, 0)
(151.0436553955078, 152.57749938964844, 159.72853088378906, 158.35447692871094, '\x13\x11\x18\n', 14, 0)
(134.86099243164062, 208.093994140625, 142.3594970703125, 222.49398803710938, 'εJ\n', 15, 0)
(226.4340057373047, 298.3089904785156, 231.17105102539062, 305.39898681640625, '3\n', 16, 0)
(204.72500610351562, 158.2376708984375, 290.083740234375, 176.0725860595703, '\x15 RU PRUH FOXPSV\n1 \x14\x13\n%DFNSURS\n', 17, 0)
(247.35299682617188, 177.3451385498047, 273.26019287109375, 183.1221160888672, '6WHSZLVH\n', 18, 0)
(241.03799438476562, 219.92613220214844, 272.334716796875, 225.70310974121094, '0RQR3ODQH\n', 19, 0)
(205.90899658203125, 261.5901184082031, 231.02609252929688, 267.36712646484375, '1HW/LQHV\n', 20, 0)
(131.775146484375, 332.44549560546875, 442.6195373535156, 409.0446472167969, 'Figure 2: Two or more clumps for two ring sizes, N = 10 and N = 25. Gen-\neralization error ϵg versus size of the training set P, for different algorithms.\nN = 10: backpropagation (Solla, 1989), Stepwise (Knerr et al., 1990). N = 25:\nTiling (M´ezard & Nadal, 1989), Upstart (Frean, 1990). Results with the Growth\nAlgorithm (Nadal, 1989) are indistinguishable from those of Tiling at the scale\nof the ﬁgure. Points without error bars correspond to best results. Results of\nMonoPlane and NetLines are averages over 25 tests.\n', 21, 0)
(131.7760009765625, 438.0349426269531, 442.6632385253906, 586.9762573242188, 'competing terms: bias and variance. With backpropagation, early stopping\nhelps to decrease overﬁtting because some hidden neurons do not reach\nlarge enough weights to work in the nonlinear part of the sigmoidal trans-\nfer functions. All the neurons working in the linear part may be replaced by\na single linear unit. Thus, with early stopping, the network is equivalent to\na smaller one with all the units working in the nonlinear regime. Our results\nare consistent with recent theories (Friedman, 1996) showing that, contrary\nto regression, the bias and variance components of the generalization error\nin classiﬁcation combine in a highly nonlinear way.\nThe number of weights used by the different algorithms is plotted on a\nlogarithmic scale as a function of P in Figure 3. It turns out that the strategy\nof NetLines is slightly better than that of MonoPlane with respect to both\ngeneralization performance and network size.\n', 22, 0)
(131.7760009765625, 600.8907470703125, 442.61968994140625, 623.80078125, '5.2 Real Valued Inputs. We tested NetLines on two problems that have\nreal valued inputs (we include graded-valued inputs here).\n', 23, 0)
(131.77601623535156, 637.71435546875, 442.6387939453125, 666.9754028320312, '5.2.1 Wisconsin Breast Cancer Database.\nThe input patterns of this bench-\nmark (Wolberg & Mangasarian, 1990) have N = 9 attributes characterizing\n', 24, 0)

page suivante
(133.38999938964844, 117.62150573730469, 444.2142333984375, 128.47036743164062, '1022\nJ. Manuel Torres Moreno and Mirta B. Gordon\n', 0, 0)
(302.5159912109375, 289.8901062011719, 431.32830810546875, 295.6671142578125, '\x13\n\x14\x13\x13\n\x15\x13\x13\n\x16\x13\x13\n\x17\x13\x13\n\x18\x13\x13\n\x19\x13\x13\n', 1, 0)
(361.3280029296875, 214.2931365966797, 386.4450378417969, 220.0701141357422, '1HW/LQHV\n', 2, 0)
(404.2200012207031, 220.1881561279297, 425.5283508300781, 225.9651336669922, '8SVWDUW\n', 3, 0)
(308.5690002441406, 160.857666015625, 374.1927490234375, 178.6925811767578, '\x15 RU PRUH FOXPSV\n1 \x15\x18\n', 4, 0)
(367.9070129394531, 298.3089904785156, 372.64404296875, 305.39898681640625, '3\n', 5, 0)
(154.49972534179688, 284.1250915527344, 289.6285095214844, 295.6671142578125, '\x13\n\x14\x13\x13\n\x15\x13\x13\n\x16\x13\x13\n\x17\x13\x13\n\x18\x13\x13\n\x14\x13\n', 6, 0)
(151.07901000976562, 218.35166931152344, 161.3442840576172, 224.12864685058594, '\x14\x13\x13\n', 7, 0)
(147.65829467773438, 152.57823181152344, 161.34486389160156, 158.35520935058594, '\x14\x13\x13\x13\n', 8, 0)
(228.04800415039062, 298.3089904785156, 232.78504943847656, 305.39898681640625, '3\n', 9, 0)
(137.65182495117188, 192.64877319335938, 144.74180603027344, 257.71099853515625, '1XPEHU RI ZHLJKWV\n', 10, 0)
(168.052001953125, 159.1546630859375, 233.6757354736328, 176.9895782470703, '\x15 RU PRUH FOXPSV\n1 \x14\x13\n', 11, 0)
(165.15699768066406, 185.99314880371094, 214.212890625, 191.77012634277344, '%DFNSURSDJDWLRQ\n', 12, 0)
(260.8069152832031, 174.7248992919922, 286.7140808105469, 180.5018768310547, '6WHSZLVH\n', 13, 0)
(226.20599365234375, 187.4341278076172, 257.50274658203125, 193.2111053466797, '0RQR3ODQH\n', 14, 0)
(202.39100646972656, 254.1221160888672, 227.5081024169922, 259.89910888671875, '1HW/LQHV\n', 15, 0)
(133.38999938964844, 332.44549560546875, 444.2141418457031, 365.21087646484375, 'Figure 3: Two or more clumps. Number of weights (logarithmic scale) versus\nsize of the training set P, for N = 10 and N = 25. Results of MonoPlane and\nNetLines are averages over 25 tests. The references are the same as in Figure 2.\n', 16, 0)
(133.37452697753906, 408.5659484863281, 444.2582092285156, 660.6173706054688, 'samples of breast cytology, classiﬁed as benign or malignant. We excluded\nfrom the original database 16 patterns that have the attribute ξ6 (“bare nu-\nclei”) missing. Among the remaining D = 683 patterns, the two classes are\nunevenly represented, 65.5% of the examples being benign. We studied the\ngeneralization performance of networks trained with sets of several sizes P.\nThe P patterns for each learning test were selected at random. In Figure 4a,\nthe generalization error at classifying the remaining G ≡ D − P patterns is\ndisplayed as a function of the corresponding number of weights in a loga-\nrithmic scale. For comparison, we included in the same ﬁgure results of a\nsingle perceptron trained with P = 75 patterns using Minimerror. The re-\nsults, averaged values over 50 independent tests for each P, show that both\nNetLines and MonoPlane have lower ϵg and fewer parameters than other\nalgorithms on this benchmark.\nThe total number of weights updates needed by NetLines, including the\nweights of the dropped output units, is 7 · 104; backpropagation needed\n≈ 104 (Prechelt, 1994).\nThe trained network may be used to classify the patterns with missing\nattributes. The number of misclassiﬁed patterns among the 16 cases for\nwhich attribute ξ6 is missing is plotted as a function of the possible values\nof ξ6 on Figure 4b. For large values of ξ6, there are discrepancies between the\nmedical and the network’s diagnosis on half the cases. This is an example\nof the kind of information that may be obtained in practical applications.\n', 17, 0)

page suivante
(131.7760009765625, 117.62150573730469, 442.60028076171875, 128.47036743164062, 'Classiﬁcation Tasks with Binary Units\n1023\n', 0, 0)
(147.6240234375, 282.85699462890625, 246.83091735839844, 294.39898681640625, '\x14\x13\n\x14\x13\x13\n\x13\x11\x13\x13\n', 1, 0)
(147.6240234375, 260.9759521484375, 159.73019409179688, 266.7529296875, '\x13\x11\x13\x14\n', 2, 0)
(147.6240234375, 238.9640350341797, 159.73019409179688, 244.74102783203125, '\x13\x11\x13\x15\n', 3, 0)
(147.6240234375, 217.08299255371094, 159.73019409179688, 222.8599853515625, '\x13\x11\x13\x16\n', 4, 0)
(147.6240234375, 195.2019500732422, 159.73019409179688, 200.97894287109375, '\x13\x11\x13\x17\n', 5, 0)
(147.6240234375, 173.19004821777344, 159.73019409179688, 178.967041015625, '\x13\x11\x13\x18\n', 6, 0)
(147.6240234375, 151.30955505371094, 159.73019409179688, 157.0865478515625, '\x13\x11\x13\x19\n', 7, 0)
(222.35499572753906, 252.9850311279297, 225.77767944335938, 258.76202392578125, '\x14\n', 8, 0)
(231.4333038330078, 202.1481170654297, 234.85598754882812, 207.92510986328125, '\x15\n', 9, 0)
(244.19515991210938, 251.01976013183594, 247.6178436279297, 256.7967529296875, '\x17\n', 10, 0)
(249.85275268554688, 178.69554138183594, 276.300048828125, 194.692138671875, '\x16\n\x18\n\x1a\n', 11, 0)
(287.8760681152344, 209.35496520996094, 291.29876708984375, 215.1319580078125, '\x19\n', 12, 0)
(192.35699462890625, 231.05960083007812, 241.61798095703125, 236.31158447265625, '0RQR3ODQH \x0b3 \x14\x19\x13\x0c\n', 13, 0)
(195.6463623046875, 184.54574584960938, 239.38645935058594, 189.7977294921875, '1HW/LQHV \x0b3 \x14\x19\x13\x0c\n', 14, 0)
(167.6219940185547, 159.28579711914062, 231.61825561523438, 176.171142578125, '0LQLPHUURU \x0b3 \x1a\x18\x0c\n%UHDVW FDQFHU \x0bD\x0c\n', 15, 0)
(169.46241760253906, 269.3171081542969, 234.68028259277344, 282.1685485839844, '0RQR3ODQH\n1HW/LQHV\n', 16, 0)
(132.88699340820312, 206.26519775390625, 139.99105834960938, 219.279541015625, 'εJ\n', 17, 0)
(196.17300415039062, 299.8688049316406, 261.5111389160156, 306.9587707519531, '1XPEHU RI ZHLJKWV\n', 18, 0)
(307.4809875488281, 288.62200927734375, 431.56951904296875, 294.39898681640625, '\x14\n\x15\n\x16\n\x17\n\x18\n\x19\n\x1a\n\x1b\n\x1c\n\x14\x13\n', 19, 0)
(317.875, 298.6242980957031, 419.5787048339844, 308.2860107421875, '3RVVLEOH YDOXHV RI DWWULEXWH ξ\x19\n', 20, 0)
(330.3739929199219, 174.6360321044922, 361.6706848144531, 188.01202392578125, '0RQR3ODQH\n1HW/LQHV\n', 21, 0)
(308.5329895019531, 160.07180786132812, 369.0028381347656, 167.1617889404297, '%UHDVW FDQFHU \x0bE\x0c\n', 22, 0)
(131.7760009765625, 332.44549560546875, 442.63800048828125, 420.00213623046875, 'Figure 4: Breast cancer classiﬁcation. (a) Generalization error ϵg versus num-\nber of weights (logarithmic scale), for P = 525. 1–3: Rprop with no shortcuts\n(Prechelt, 1994); 4–6: Rprop with shortcuts (Prechelt, 1994); 7: Cascade Correla-\ntion (Depenau, 1995). For comparison, results with smaller training sets, P = 75\n(single perceptron) and P = 160, are displayed. Results of MonoPlane and Net-\nLines are averages over 50 tests. (b) Classiﬁcation errors versus possible values\nof the missing attribute bare nuclei for the 16 incomplete patterns, averaged\nover 50 independently trained networks.\n', 23, 0)
(131.76278686523438, 449.9639587402344, 442.65374755859375, 575.9869384765625, '5.2.2 Diabetes Diagnosis.\nThis benchmark (Prechelt, 1994) contains D =\n768 patterns described by N = 8 real-valued attributes, corresponding to\n≈ 35% of Pima women suffering from diabetes, 65% being healthy. Training\nsets of P = 576 patterns were selected at random, and generalization was\ntested on the remaining G = 192 patterns. The comparison with published\nresults obtained with other algorithms tested under the same conditions,\npresented in Figure 5, shows that NetLines reaches the best performance\npublished so far on this benchmark, needing many fewer parameters. Train-\ning times of NetLines are of ≈ 105 updates. The numbers of updates needed\nby Rprop (Prechelt, 1994) range between 4 · 103 and 5 · 105, depending on\nthe network’s architecture.\n', 24, 0)
(131.76278686523438, 591.880615234375, 442.6123352050781, 660.6201782226562, '5.3 Multiclass Problems. We applied our learning algorithm to two dif-\nferent problems, both of three classes. We compare the results obtained with\na WTA classiﬁcation based on the results of three networks, each indepen-\ndently trained to separate one class from the two others, to the results of\nthe TON architectures described in section 4. Because the number of classes\nis low, we determined the three TONs, corresponding to the three possible\n', 25, 0)

page suivante
(133.38999938964844, 117.62150573730469, 444.2142333984375, 128.47036743164062, '1024\nJ. Manuel Torres Moreno and Mirta B. Gordon\n', 0, 0)
(194.55160522460938, 285.4820251464844, 290.57684326171875, 297.2430114746094, '\x14\x13\n\x14\x13\x13\n\x13\x11\x15\x13\n', 1, 0)
(194.55160522460938, 258.6118469238281, 206.8397216796875, 264.4908447265625, '\x13\x11\x15\x15\n', 2, 0)
(194.55160522460938, 231.74166870117188, 206.8397216796875, 237.62066650390625, '\x13\x11\x15\x17\n', 3, 0)
(194.55160522460938, 205.00564575195312, 206.8397216796875, 210.8846435546875, '\x13\x11\x15\x19\n', 4, 0)
(194.55160522460938, 178.13546752929688, 206.8397216796875, 184.01446533203125, '\x13\x11\x15\x1b\n', 5, 0)
(194.55160522460938, 151.26528930664062, 206.8397216796875, 157.144287109375, '\x13\x11\x16\x13\n', 6, 0)
(251.1739959716797, 230.80099487304688, 254.6481170654297, 236.67999267578125, '\x14\n', 7, 0)
(273.3423767089844, 251.12039184570312, 276.8164978027344, 256.9993896484375, '\x15\n', 8, 0)
(324.35662841796875, 199.65286254882812, 327.83074951171875, 205.5318603515625, '\x16\n', 9, 0)
(332.369140625, 244.83718872070312, 335.84326171875, 250.7161865234375, '\x18\n', 10, 0)
(335.8412780761719, 206.60415649414062, 339.3153991699219, 212.483154296875, '\x19\n', 11, 0)
(322.218994140625, 220.63998413085938, 325.693115234375, 226.51898193359375, '\x17\n', 12, 0)
(214.4499969482422, 244.83901977539062, 239.94454956054688, 250.718017578125, '1HW/LQHV\n', 13, 0)
(215.25100708007812, 161.1252899169922, 293.2890625, 168.3402862548828, ",QGLDQV 3LPD 'LDEHWHV\n", 14, 0)
(183.1999969482422, 213.46044921875, 190.41041564941406, 227.05548095703125, 'εJ\n', 15, 0)
(243.82899475097656, 302.8393249511719, 310.14190673828125, 310.0543212890625, '1XPEHU RI ZHLJKWV\n', 16, 0)
(133.38999938964844, 335.55450439453125, 444.2492980957031, 368.31988525390625, 'Figure 5: Diabetes diagnosis: Generalization error ϵg versus number of weights.\nResults of NetLines are averages over 50 tests. 1–3: Rprop no shortcuts, 4–6:\nRprop with shortcuts (Prechelt, 1994).\n', 17, 0)
(133.38999938964844, 394.5449523925781, 444.2110290527344, 417.4549865722656, 'learning sequences. The vote of the three TONs improves the performances,\nas expected.\n', 18, 0)
(133.38063049316406, 431.48211669921875, 444.2680358886719, 660.6190185546875, '5.3.1 Breiman’s Waveform Recognition Problem.\nThis problem was intro-\nduced as a test for the algorithm CART (Breiman et al., 1984). The input\npatterns are deﬁned by N = 21 real-valued amplitudes x(t) observed at reg-\nularly spaced intervals t = 1, 2, . . . , N. Each pattern is a noisy convex linear\ncombination of two among three elementary waves (triangular waves cen-\ntered on three different values of t). There are three possible combinations,\nand the pattern’s class identiﬁes from which combination it is issued.\nWe trained the networks with the same 11 training sets of P = 300 ex-\namples, and generalization was tested on the same independent test set\nof G = 5000, as in Gascuel (1995). Our results are displayed in Figure 6,\nwhere only results of algorithms reaching ϵg < 0.25 in Gascuel (1995) are\nincluded. Although it is known that due to the noise, the classiﬁcation error\nhas a lower bound of ≈ 14% (Breiman et al., 1984), the results of NetLines\nand MonoPlane presented here correspond to error-free training. The net-\nworks generated by NetLines have between three and six hidden neurons,\ndepending on the training sets. The results obtained with a single percep-\ntron trained with Minimerror and with the perceptron learning algorithm,\nwhich may be considered the extreme case of early stopping, are hardly im-\nproved by the more complex networks. Here again the overﬁtting produced\nby error-free learning with NetLines does not cause the generalization per-\n', 19, 0)

page suivante
(131.7760009765625, 117.62150573730469, 442.60028076171875, 128.47036743164062, 'Classiﬁcation Tasks with Binary Units\n1025\n', 0, 0)
(245.5540008544922, 292.6545104980469, 381.91387939453125, 298.53350830078125, '\x14\x13\n\x14\x13\x13\n\x14\x13\x13\x13\n\x14\x13\x13\x13\x13\n\x14\x13\x13\x13\x13\x13\n', 1, 0)
(224.45387268066406, 276.47900390625, 236.7419891357422, 282.3580322265625, '\x13\x11\x14\x17\n', 2, 0)
(224.45387268066406, 255.7584991455078, 236.7419891357422, 261.63751220703125, '\x13\x11\x14\x19\n', 3, 0)
(224.45387268066406, 235.17152404785156, 236.7419891357422, 241.05052185058594, '\x13\x11\x14\x1b\n', 4, 0)
(224.45387268066406, 214.4510040283203, 236.7419891357422, 220.3300018310547, '\x13\x11\x15\x13\n', 5, 0)
(224.45387268066406, 193.86402893066406, 236.7419891357422, 199.74302673339844, '\x13\x11\x15\x15\n', 6, 0)
(224.45387268066406, 173.1435089111328, 236.7419891357422, 179.0225067138672, '\x13\x11\x15\x17\n', 7, 0)
(224.45387268066406, 152.55653381347656, 236.7419891357422, 158.43553161621094, '\x13\x11\x15\x19\n', 8, 0)
(276.93701171875, 210.7005157470703, 280.4111328125, 216.5795135498047, '\x14\n', 9, 0)
(276.93701171875, 242.6503448486328, 289.2251281738281, 250.1333465576172, '\x15\n\x16\n', 10, 0)
(297.90374755859375, 170.46168518066406, 301.37786865234375, 176.34068298339844, '\x17\n', 11, 0)
(308.1867370605469, 192.11805725097656, 341.0408630371094, 206.28538513183594, '\x18\n\x19\n', 12, 0)
(337.5667419433594, 232.48976135253906, 369.75335693359375, 241.4432830810547, '\x1a\n\x1b\n', 13, 0)
(256.3710021972656, 185.83351135253906, 307.3619689941406, 191.71250915527344, '0RQR3ODQH \x0b:7$\x0c\n', 14, 0)
(244.75238037109375, 272.593017578125, 291.0601501464844, 278.4720458984375, '7KHRUHWLFDO OLPLW\n', 15, 0)
(243.5507354736328, 259.2249755859375, 363.983154296875, 271.38714599609375, '0LQLPHUURU\n1HW/LQHV \x0b9RWH\x0c\n', 16, 0)
(292.9620056152344, 158.98631286621094, 369.6877136230469, 166.20130920410156, '%UHLPDQ\nV :DYHIRUPV\n', 17, 0)
(212.70199584960938, 221.7484130859375, 219.91241455078125, 235.344482421875, 'εJ\n', 18, 0)
(263.0480041503906, 300.7003173828125, 342.70135498046875, 307.9153137207031, '1XPEHU RI SDUDPHWHUV\n', 19, 0)
(131.7760009765625, 335.55450439453125, 442.636474609375, 390.23638916015625, 'Figure 6: Breiman waveforms: Generalization error ϵg averaged over 11 tests\nversus number of parameters. Error bars on the number of weights generated\nby NetLines and MonoPlane are not visible at the scale of the ﬁgure. 1: linear dis-\ncrimination; 2: perceptron; 3: backpropagation; 4: genetic algorithm; 5: quadratic\ndiscrimination; 6: Parzen’s kernel; 7: K-NN; 8: constraint (Gascuel, 1995).\n', 20, 0)
(131.7760009765625, 415.3909606933594, 442.615966796875, 438.9668273925781, 'formance to deteriorate. The TONs vote reduces the variance but does not\ndecrease the average ϵg.\n', 21, 0)
(131.76983642578125, 452.07977294921875, 442.6634216308594, 555.188720703125, '5.3.2 Fisher’s Iris Plants Database.\nIn this classic three-class problem, one\nhas to determine the class of iris plants based on the values of N = 4 real-\nvalued attributes. The database of D = 150 patterns contains 50 examples\nof each class. Networks were trained with P = 149 patterns, and the gener-\nalization error is the mean value of all the 150 leave-one-out possible tests.\nResults of ϵg are displayed as a function of the number of weights in Figure 7.\nError bars are available for only our own results. In this difﬁcult problem,\nthe vote of the three possible TONs trained with the three possible class\nsequences (see section 4) improves the generalization performance.\n', 22, 0)
(131.773193359375, 568.995361328125, 190.65492248535156, 580.2587280273438, '6 Conclusion\n', 23, 0)
(131.7760009765625, 591.8789672851562, 442.66326904296875, 660.6185302734375, 'We presented an incremental learning algorithm for classiﬁcation, which we\ncall NetLines. It generates small feedforward neural networks with a single\nhidden layer of binary units connected to a binary output neuron. NetLines\nallows for an automatic adaptation of the neural network to the complexity\nof the particular task. This is achieved by coupling an error-correcting strat-\negy for the successive addition of hidden neurons with Minimerror, a very\n', 24, 0)

page suivante
(133.38999938964844, 117.62150573730469, 444.2142333984375, 128.47036743164062, '1026\nJ. Manuel Torres Moreno and Mirta B. Gordon\n', 0, 0)
(194.55160522460938, 286.77252197265625, 318.2175598144531, 298.53350830078125, '\x14\x13\n\x14\x13\x13\n\x13\x11\x13\x13\n', 1, 0)
(194.55160522460938, 259.90234375, 206.8397216796875, 265.7813720703125, '\x13\x11\x13\x15\n', 2, 0)
(194.55160522460938, 233.0321807861328, 206.8397216796875, 238.9111785888672, '\x13\x11\x13\x17\n', 3, 0)
(194.55160522460938, 206.29615783691406, 206.8397216796875, 212.17515563964844, '\x13\x11\x13\x19\n', 4, 0)
(194.55160522460938, 179.4259796142578, 206.8397216796875, 185.3049774169922, '\x13\x11\x13\x1b\n', 5, 0)
(194.55160522460938, 152.55580139160156, 206.8397216796875, 158.43479919433594, '\x13\x11\x14\x13\n', 6, 0)
(247.83599853515625, 242.9204864501953, 261.4579772949219, 248.7994842529297, '\x14\n\x15\n', 7, 0)
(286.43072509765625, 227.4131622314453, 295.2461242675781, 233.2921600341797, '\x16\x0f\x17\n', 8, 0)
(316.0780944824219, 211.10414123535156, 319.5522155761719, 216.98313903808594, '\x18\n', 9, 0)
(316.0780944824219, 242.9204864501953, 319.5522155761719, 248.7994842529297, '\x19\n', 10, 0)
(247.16799926757812, 270.46148681640625, 290.80810546875, 276.34051513671875, '1HW/LQHV \x0bYRWH\x0c\n', 11, 0)
(222.32838439941406, 212.71083068847656, 267.0437927246094, 218.58982849121094, '1HW/LQHV \x0b:7$\x0c\n', 12, 0)
(290.0350036621094, 160.18931579589844, 338.9950866699219, 167.40431213378906, ',5,6 GDWDEDVH\n', 13, 0)
(176.65699768066406, 221.7484130859375, 183.86741638183594, 235.344482421875, 'εJ\n', 14, 0)
(243.82899475097656, 302.8393249511719, 310.14190673828125, 310.0543212890625, '1XPEHU RI ZHLJKWV\n', 15, 0)
(133.38999938964844, 335.55450439453125, 444.221923828125, 379.27813720703125, 'Figure 7: Iris database: Generalization error ϵg versus number of parameters.\n1: offset, 2: backpropagation (Martinez & Est`eve, 1992); 4,5: backpropagation\n(Verma & Mulawka, 1995); 3,6: gradient-descent orthogonalized training (Verma\n& Mulawka, 1995).\n', 16, 0)
(133.38999938964844, 408.5659484863281, 444.27734375, 660.623779296875, 'efﬁcient perceptron training algorithm. Learning is fast not only because\nit reduces the problem to that of training single perceptrons, but mainly\nbecause there is no longer a need for the usual preliminary tests required to\ndetermine the correct architecture for the particular application. Theorems\nvalid for binary as well as for real-valued inputs guarantee the existence of\na solution with a bounded number of hidden neurons obeying the growth\nstrategy.\nThe networks are composed of binary hidden units whose states consti-\ntute a faithful encoding of the input patterns. They implement a mapping\nfrom the input space to a discrete H-dimensional hidden space, H being\nthe number of hidden neurons. Thus, each pattern is labeled with a binary\nword of H bits. This encoding may be seen as a compression of the pattern’s\ninformation. The hidden neurons deﬁne linear boundaries, or portions of\nboundaries, between classes in input space. The network’s output may be\ngiven a probabilistic interpretation based on the distance of the patterns to\nthese boundaries.\nTests on several benchmarks showed that the networks generated by our\nincremental strategy are small, in spite of the fact that the hidden neurons\nare appended until error-free learning is reached. Even when the networks\nobtained with NetLines are larger than those used by other algorithms, its\ngeneralization error remains among the smallest values reported. In noisy\nor difﬁcult problems, it may be useful to stop the network’s growth before\n', 17, 0)

page suivante
(131.7760009765625, 117.62150573730469, 442.60028076171875, 128.47036743164062, 'Classiﬁcation Tasks with Binary Units\n1027\n', 0, 0)
(131.7760009765625, 145.053955078125, 442.6539306640625, 271.08056640625, 'the condition of zero training errors is reached. This decreases overﬁtting, as\nsmaller networks (with less parameters) are thus generated. However, the\nprediction quality (measured by the generalization error) of the classiﬁers\ngenerated with NetLines is not improved by early stopping.\nOur results were obtained without cross-validation or any data manip-\nulation like boosting, bagging, or arcing (Breiman, 1994; Drucker, Schapire,\n& Simard, 1993). Those costly procedures combine results of very large\nnumbers of classiﬁers, with the aim of improving the generalization perfor-\nmance through the reduction of the variance. Because NetLines is a stable\nclassiﬁer, presenting small variance, we do not expect that such techniques\nwould signiﬁcantly improve our results.\n', 1, 0)
(131.7760009765625, 282.572021484375, 174.88905334472656, 293.83538818359375, 'Appendix\n', 2, 0)
(131.759765625, 305.45294189453125, 442.654052734375, 460.7409973144531, 'In this appendix we exhibit a particular solution to the learning strategy of\nNetLines. This solution is built in such a way that the cardinal of a convex\nsubset of well-learned patterns, Lh, grows monotonically upon the addition\nof hidden units. Because this cardinal cannot be larger than the total number\nof training patterns, the algorithm must stop with a ﬁnite number of hidden\nunits.\nSuppose that h hidden units have already been included and that the\noutput neuron still makes classiﬁcation errors on patterns of the training set,\ncalled training errors. Among these wrongly learned patterns, let ν be the\none closest to the hyperplane normal to ⃗wh, called hyperplane-h hereafter.\nWe deﬁne Lh as the subset of (correctly learned) patterns lying closer to\nhyperplane-h than ⃗ξν. Patterns in Lh have 0 < γh < |γ ν\nh |. The subset Lh and\nat least pattern ν are well learned if the next hidden unit, h+1, has weights:\n', 3, 0)
(155.67022705078125, 461.8748474121094, 442.6177062988281, 483.3415222167969, '⃗wh+1 = τ ν\nh ⃗wh − (1 − ϵh)τ ν\nh ( ⃗wh · ⃗ξν)ˆe0,\n(A.1)\n', 4, 0)
(131.76370239257812, 482.11895751953125, 442.61419677734375, 505.02899169921875, 'where ˆe0 ≡ (1, 0, . . . , 0). The conditions that both Lh and pattern ν have\npositive stabilities (are correctly learned) impose that\n', 5, 0)
(155.67514038085938, 520.5609130859375, 210.44161987304688, 542.480224609375, '0 < ϵh < min\nµ∈Lh\n', 6, 0)
(213.21200561523438, 511.8974609375, 251.7381591796875, 534.4098510742188, '|γ ν\nh | − γ µ\nh\n', 7, 0)
(224.781005859375, 520.560791015625, 442.62091064453125, 548.4545288085938, '|γ ν\nh |\n.\n(A.2)\n', 8, 0)
(131.7694549560547, 546.7201538085938, 442.61151123046875, 570.5037841796875, 'The following weights between the hidden units and the output will give\nthe correct output to pattern ν and to the patterns of Lh:\n', 9, 0)
(164.6026153564453, 577.3214721679688, 442.6112365722656, 595.5715942382812, 'W0(h + 1) = W0(h) + τ ν\n(A.3)\n', 10, 0)
(166.14706420898438, 592.2129516601562, 442.6221008300781, 610.0166015625, 'Wi(h + 1) = Wi(h) for 1 ≤ i ≤ h\n(A.4)\n', 11, 0)
(155.68304443359375, 606.2134399414062, 442.62139892578125, 624.4625854492188, 'Wh+1(h + 1) = −τ ν.\n(A.5)\n', 12, 0)
(131.76153564453125, 626.2499389648438, 442.61199951171875, 660.6173706054688, 'Thus, card(Lh+1) ≥ card(Lh) + 1. As the number of patterns in Lh increases\nmonotonically with h, convergence is guaranteed with less than P hidden\nunits.\n', 13, 0)

page suivante
(133.38999938964844, 117.62150573730469, 444.2142333984375, 128.47036743164062, '1028\nJ. Manuel Torres Moreno and Mirta B. Gordon\n', 0, 0)
(133.38999938964844, 145.08233642578125, 214.88360595703125, 156.34568786621094, 'Acknowledgments\n', 1, 0)
(133.38999938964844, 167.96795654296875, 444.23577880859375, 190.87799072265625, 'J.M. thanks Consejo Nacional de Ciencia y Tecnolog´ıa and Universidad\nAut´onoma Metropolitana, M´exico, for ﬁnancial support (grant 65659).\n', 2, 0)
(133.3900146484375, 210.33990478515625, 180.7055206298828, 221.60325622558594, 'References\n', 3, 0)
(133.38999938964844, 233.1875457763672, 444.2464294433594, 660.449951171875, 'Alpaydin, E. A. I. (1990). Neural models of supervised and unsupervised learning. Un-\npublished doctoral dissertation, Ecole Polytechnique F´ed´erale de Lausanne,\nSwitzerland.\nBiehl, M., & Opper, M. (1991). Tilinglike learning in the parity machine. Physical\nReview A, 44, 6888.\nBottou, L., & Vapnik, V. (1992). Local learning algorithms. Neural Computation,\n4(6), 888–900.\nBreiman, L. (1994). Bagging predictors (Tech. Rep. No. 421). Berkeley: Department\nof Statistics, University of California at Berkeley.\nBreiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classiﬁcation\nand regression trees. Monterey, CA: Wadsworth and Brooks/Cole.\nDenker,J.,Schwartz,D.,Wittner,B.,Solla,S.,Howard,R.,Jackel,L.,&Hopﬁeld,J.\n(1987). Large automatic learning, rule extraction, and generalization. Complex\nSystems, 1, 877–922.\nDepenau, J. (1995). Automated design of neural network architecture for classiﬁcation.\nUnpublished doctoral dissertation, Computer Science Department, Aarhus\nUniversity.\nDrucker, H., Schapire, R., & Simard, P. (1993). Improving performance in neu-\nral networks using a boosting algorithm. In S. J. Hanson, J. D. Cowan, &\nC. L. Giles (Eds.), Advances in neural information processing systems, 5 (pp. 42–\n49). San Mateo, CA: Morgan Kaufmann.\nFahlman, S. E., & Lebiere, C. (1990). The cascade-correlation learning architec-\nture. In D. S. Touretzky (Ed.), Advances in neural information processing systems,\n2 (pp. 524–532). San Mateo: Morgan Kaufmann.\nFarrell, K. R., & Mammone, R. J. (1994). Speaker recognition using neural tree\nnetworks. In J. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in Neural\nInformation Processing Systems, 6 (pp. 1035–1042). San Mateo, CA: Morgan\nKaufmann.\nFrean, M. (1990). The Upstart algorithm: A method for constructing and training\nfeedforward neural networks. Neural Computation, 2(2), 198–209.\nFrean, M. (1992). A “thermal” perceptron learning rule. Neural Computation, 4(6),\n946–957.\nFriedman, J. H. (1996). On bias, variance, 0/1-loss, and the curse-of-dimensionality\n(Tech. Rep.) Stanford, CA: Department of Statistics, Stanford University.\nFritzke, B. (1994). Supervised learning with growing cell structures. In\nJ. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in neural informa-\ntion processing systems, 6 (pp. 255–262). San Mateo, CA: Morgan Kaufmann.\nGallant, S. I. (1986). Optimal linear discriminants. In Proc. 8th. Conf. Pattern\nRecognition, Oct. 28–31, Paris, vol. 4.\n', 4, 0)

page suivante
(131.7760009765625, 117.62150573730469, 442.60028076171875, 128.47036743164062, 'Classiﬁcation Tasks with Binary Units\n1029\n', 0, 0)
(131.7760009765625, 145.51560974121094, 442.62896728515625, 649.4857788085938, 'Gascuel, O. (1995). Symenu. Collective Paper (Gascuel O. Coordinator) (Tech. Rep.).\n5`emes Journ´ees Nationales du PRC-IA Teknea, Nancy.\nGeman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks and the\nbias/variance dilemma. Neural Computation, 4(1), 1–58.\nGoodman, R. M., Smyth, P., Higgins, C. M., & Miller, J. W. (1992). Rule-based\nneural networks for classiﬁcation and probability estimation. Neural Compu-\ntation, 4(6), 781–804.\nGordon, M. B. (1996). A convergence theorem for incremental learning with real-\nvalued inputs. In IEEE International Conference on Neural Networks, pp. 381–\n386.\nGordon, M. B., & Berchier, D. (1993). Minimerror: A perceptron learning rule\nthat ﬁnds the optimal weights. In M. Verleysen (Ed.), European Symposium on\nArtiﬁcial Neural Networks (pp. 105–110). Brussels: D Facto.\nGordon, M. B., & Grempel, D. (1995). Optimal learning with a temperature\ndependent algorithm. Europhysics Letters, 29(3), 257–262.\nGordon, M. B., Peretto, P., & Berchier, D. (1993). Learning algorithms for percep-\ntrons from statistical physics. Journal of Physics I (France), 3, 377–387.\nGorman, R. P., & Sejnowski, T. J. (1988). Analysis of hidden units in a layered\nnetwork trained to classify sonar targets. Neural Networks, 1, 75–89.\nGyorgyi, G., & Tishby, N. (1990). Statistical theory of learning a rule. In\nW. K. Theumann & R. Koeberle (Eds.), Neural networks and spin glasses. Sin-\ngapore: World Scientiﬁc.\nHoehfeld, M., & Fahlman, S. (1991). Learning with limited numerical precision using\nthe cascade correlation algorithm (Tech. Rep. No. CMU-CS-91-130). Pittsburgh:\nCarnegie Mellon University.\nKnerr, S., Personnaz, L., & Dreyfus, G. (1990). Single-layer learning revisited: A\nstepwise procedure for building and training a neural network. In J. H´erault\n& F. Fogelman (Eds.), Neurocomputing, algorithms, architectures and applications\n(pp. 41–50). Berlin: Springer-Verlag.\nMarchand, M., Golea, M., & Ruj´an, P. (1990). A convergence theorem for sequen-\ntial learning in two-layer perceptrons. Europhysics Letters, 11, 487–492.\nMartinez, D., & Est`eve, D. (1992). The offset algorithm: Building and learning\nmethod for multilayer neural networks. Europhysics Letters, 18, 95–100.\nM´ezard, M., & Nadal, J.-P. (1989). Learning in feedforward layered networks:\nThe Tiling algorithm. J. Phys. A: Math. and Gen., 22, 2191–2203.\nMukhopadhyay, S., Roy, A., Kim, L. S., & Govil, S. (1993). A polynomial time al-\ngorithm for generating neural networks for pattern classiﬁcation: Its stability\nproperties and some test results. Neural Computation, 5(2), 317–330.\nNadal, J.-P. (1989). Study of a growth algorithm for a feedforward neural net-\nwork. Int. J. Neur. Syst., 1, 55–59.\nPrechelt, L. (1994). PROBEN1—A set of benchmarks and benchmarking rules for neu-\nral network training algorithms (Tech. Rep. No. 21/94). University of Karlsruhe,\nFaculty of Informatics.\nRafﬁn, B., & Gordon, M. B. (1995). Learning and generalization with Minimerror,\na temperature dependent learning algorithm. Neural Computation, 7(6), 1206–\n1224.\n', 1, 0)

page suivante
(133.38999938964844, 117.62150573730469, 444.2142333984375, 128.47036743164062, '1030\nJ. Manuel Torres Moreno and Mirta B. Gordon\n', 0, 0)
(133.3899383544922, 145.51560974121094, 444.2330322265625, 441.2790222167969, 'Reilly, D. E, Cooper, L. N., & Elbaum, C. (1982). A neural model for category\nlearning. Biological Cybernetics, 45, 35–41.\nRoy, A., Kim, L., & Mukhopadhyay, S. (1993). A polynomial time algorithm\nfor the construction and training of a class of multilayer perceptron. Neural\nNetworks, 6(1), 535–545.\nSirat, J. A., & Nadal, J.-P. (1990). Neural trees: A new tool for classiﬁcation.\nNetwork, 1, 423–438.\nSolla, S. A. (1989). Learning and generalization in layered neural networks: The\ncontiguity problem. In L. Personnaz & G. Dreyfus (Eds.), Neural Networks\nfrom Models to Applications. Paris: I.D.S.E.T.\nTorres Moreno, J.-M., & Gordon, M. B. (1995). An evolutive architecture coupled\nwith optimal perceptron learning for classiﬁcation. In M. Verleysen (Ed.),\nEuropean Symposium on Artiﬁcial Neural Networks. Brussels: D Facto.\nTorres Moreno, J.-M., & Gordon, M. B. (1998). Characterization of the sonar\nsignals benchmark. Neural Proc. Letters, 7(1), 1–4.\nTrhun, S. B., et al. (1991). The monk’s problems: A performance comparison of different\nlearning algorithms (Tech. Rep. No. CMU-CS-91-197). Pittsburgh: Carnegie\nMellon University.\nVapnik, V. (1992). Principles of risk minimization for learning theory. In\nJ. E. Moody, S. J. Hanson, & R. P. Lippmann (Eds.), Advances in neural informa-\ntion processing systems, 4 (pp. 831–838). San Mateo, CA: Morgan Kaufmann.\nVerma, B. K., & Mulawka, J. J. (1995). A new algorithm for feedforward neu-\nral networks. In M. Verleysen (Ed.), European Symposium on Artiﬁcial Neural\nNetworks (pp. 359–364). Brussels: D Facto.\nWolberg, W. H., & Mangasarian, O. L. (1990). Multisurface method of pattern\nseparation for medical diagnosis applied to breast cytology. In Proceedings of\nthe National Academy of Sciences, USA, 87, 9193–9196.\n', 1, 0)
(133.38999938964844, 458.27081298828125, 331.8348693847656, 467.9145202636719, 'Received February 13, 1997; accepted September 4, 1997.\n', 2, 0)

page suivante
(46.69200134277344, 57.68895721435547, 168.38751220703125, 67.68895721435547, 'This article has been cited by:\n', 0, 0)
(49.50200271606445, 83.86195373535156, 555.313720703125, 132.86195373535156, '1. C. Citterio, A. Pelagotti, V. Piuri, L. Rocca. 1999. Function approximation-fast-convergence neural approach based on spectral\nanalysis. IEEE Transactions on Neural Networks 10, 725-740. [CrossRef]\n2. Andrea Pelagotti, Vincenzo Piuri. 1997. Entropic Analysis and Incremental Synthesis of Multilayered Feedforward Neural\nNetworks. International Journal of Neural Systems 08, 647-659. [CrossRef]\n', 1, 0)

page suivante
