(84.13899993896484, 86.16465759277344, 507.73944091796875, 122.76204681396484, 'A New Annotated Portuguese/Spanish Corpus for the Multi-Sentence\nCompression Task\n', 0, 0)
(109.45901489257812, 138.5689697265625, 482.4148254394531, 166.51971435546875, 'Elvys Linhares Pontes1, Juan-Manuel Torres-Moreno1,2, St´ephane Huet1,\nAndr´ea Carneiro Linhares3\n', 1, 0)
(143.08302307128906, 165.142578125, 448.79150390625, 225.6155242919922, '1CERI/LIA, Universit´e d’Avignon et des Pays de Vaucluse, Avignon, France\n2 ´Ecole Polytechnique de Montr´eal, Montr´eal, Canada\n3Universidade Federal do Cear´a, Sobral-CE, Brasil\n{elvys.linhares-pontes, juan-manuel.torres, stephane.huet}@univ-avignon.fr\nandrea.linhares@ufc.br\n', 2, 0)
(52.15800476074219, 240.6399383544922, 539.71484375, 286.5645446777344, 'Abstract\nMulti-sentence compression aims to generate a short and informative compression from several source sentences that deal with the same\ntopic. In this work, we present a new corpus for the Multi-Sentence Compression (MSC) task in Portuguese and Spanish. We also\nprovide on this corpus a comparison of two state-of-the-art MSC systems.\n', 3, 0)
(52.15800476074219, 297.3552551269531, 349.7221374511719, 309.02056884765625, 'Keywords: Annotated Corpus, Multi-Sentence Compression, Multilingual Corpus.\n', 4, 0)
(126.6820068359375, 322.02203369140625, 212.48448181152344, 337.57574462890625, '1.\nIntroduction\n', 5, 0)
(52.15800476074219, 340.97357177734375, 287.0064697265625, 677.2213134765625, 'Among the various applications of Natural Language Pro-\ncessing, Automatic Text Summarization (ATS) aims at\nsummarizing one or more texts automatically. Summariza-\ntion systems identify relevant data and create a summary\nfrom key information. The (Multi-)Sentence Compression\ntask can be seen as a subproblem of ATS with the objective\nto generate a shorter, informative and correct sentence from\nsource sentence(s).\nIn many cases, state-of-the-art NLP systems are evaluated\nwith experiments restrained to the English language, in part\nbecause there are a lot of available English resources for\nmost NLP tasks. As regards Multi-Sentence Compression\n(MSC), the available resources are unfortunately limited; to\nour knowledge, only one dataset is freely available and it is\nconﬁned to the French language (Boudin and Morin, 2013).\nIn this work, we present a new annotated corpus in the Por-\ntuguese and Spanish languages for the MSC task. Using\nthis corpus, we evaluate two state-of-the-art systems and\nshow that the use of several languages leads to more miti-\ngated results on the superiority of one system than the use\nof the French corpus alone.\nThe remainder of this paper is organized as follows. In Sec-\ntion 2, we characterize MSC with respect to related tasks\nfrom the perspective of the available corpora. Section 3\ndescribes the creation and the features of our corpus. In\nSection 4 we analyze the results achieved by state-of-the-\nart methods using our dataset. Finally, conclusions are set\nout in Section 5.\n', 6, 0)
(123.54400634765625, 689.2767944335938, 215.62295532226562, 704.8305053710938, '2.\nRelated Work\n', 7, 0)
(52.15800476074219, 708.2283935546875, 287.0064392089844, 791.9642944335938, 'Sentence Compression (SC) aims at producing a reduced\ngrammatically correct sentence from a source sentence. SC\ncan be used in the context of the abstractive summarization\nof documents, the generation of article titles or the simpli-\nﬁcation of complex sentences, using diverse methods (opti-\nmization, syntactic structure, deletion of words and/or gen-\neration of sentences). The corpora for SC can be divided\n', 8, 0)
(304.8659973144531, 324.29339599609375, 539.7144165039062, 791.965087890625, 'in two categories: deletion-based and summarization-based\nSC.\nIn the case of SC by deletion of words, sentences are\ncompressed by removing irrelevant words (Filippova et al.,\n2015; Ive and Yvon, 2016). Knight and Marcu (2002) de-\nveloped a SC corpus by aligning abstracts and sentences\nextracted from the Ziff-Davis corpus, which is a collec-\ntion of newspaper articles announcing computer products.\nClarke and Lapata (2008) provided two manually created\ntwo-reference corpora for deletion-based compression. Fil-\nippova and Altun (2013), and Filippova et al. (2015) ex-\ntracted and released deletion-based compressions by align-\ning news headlines to the ﬁrst sentences. Finally, Ive and\nYvon (2016) developed an English-French parallel corpus\nfor the compression and simpliﬁcation tasks.\nSC by generations of sentences analyzes a whole sentence\nand generates a new shorter sentence with the core infor-\nmation of the source sentence (Rush et al., 2015; Gan-\nitkevitch et al., 2011; Cohn and Lapata, 2008; Toutanova\net al., 2016). Ganitkevitch et al. (2011) created a corpus\nof compression paraphrases composed of parallel English-\nEnglish sentences obtained from multiple reference transla-\ntions. Rush et al. (2015) produced compression pairs made\nup of the headline of each article and its ﬁrst sentence; they\nreleased their code to extract data from the annotated Gi-\ngaword (Graff et al., 2011). Cohn and Lapata (2008) and\nToutanova et al. (2016) describe two manually created ab-\nstractive compression corpora that are publicly available.\nThe dataset presented in Cohn and Lapata (2008) comprises\na single-reference sentence pairs for abstractive summary,\nwhile the corpus developed by Toutanova et al. (2016) has\nmultiple references for short paragraph compressions.\nMulti-Sentence Compression (MSC), also known as Multi-\nSentence Fusion, is a variation of SC. MSC aims at ana-\nlyzing a cluster of similar sentences to generate a new sen-\ntence, which is shorter than the average length of source\nsentences and has the key information of the cluster (Barzi-\nlay and McKeown, 2005; Filippova, 2010). MSC enables\nsummarization and question-answering systems to gener-\n', 9, 0)
(284.0, 800.989990234375, 310.68798828125, 817.47802734375, '3192\n', 10, 0)

page suivante
(102.87300109863281, 68.36791229248047, 467.1322021484375, 87.30728149414062, 'Characteristics\nFrench\nPortuguese\nSpanish\n', 0, 0)
(232.3090057373047, 81.28025817871094, 489.0006408691406, 92.94554138183594, 'Source\nReference\nSource\nReference\nSource\nReference\n', 1, 0)
(102.87300109863281, 93.03558349609375, 481.12750244140625, 105.04051971435547, '#tokens\n20,224\n2,362\n17,998\n1,425\n30,588\n3,694\n', 2, 0)
(102.87300109863281, 105.38958740234375, 477.3918762207031, 117.39452362060547, '#vocabulary (tokens)\n2,867\n636\n2,438\n533\n4,390\n881\n', 3, 0)
(102.87300109863281, 117.74359130859375, 477.3918762207031, 129.74851989746094, '#sentences\n618\n120\n544\n80\n800\n160\n', 4, 0)
(102.87300109863281, 130.0965576171875, 478.6375732421875, 142.1014862060547, 'avg. sentence length (tokens)\n33.0\n19.7\n33.1\n17.8\n38.2\n23.1\n', 5, 0)
(102.87300109863281, 142.4505615234375, 482.78643798828125, 154.4554901123047, 'type-token ratio\n38.8%\n50.1%\n33.7%\n67.9%\n35.2%\n43.4%\n', 6, 0)
(102.87300109863281, 154.8045654296875, 478.6375732421875, 166.8094940185547, 'sentence similarity [0,1]\n0.46\n0.67\n0.51\n0.59\n0.47\n0.64\n', 7, 0)
(229.90499877929688, 176.54254150390625, 361.96917724609375, 188.54747009277344, 'Table 1: Statistics of the corpora.\n', 8, 0)
(52.15800476074219, 208.5875244140625, 287.0078125, 519.4713745117188, 'ate outputs combining fully formed sentences from one or\nseveral documents. Various corpora have been developed\nfor MSC and are composed of clusters of similar sentences\nfrom different source news in English, French, Spanish or\nVietnamese languages (Barzilay and McKeown, 2005; Fil-\nippova, 2010; Boudin and Morin, 2013; Thadani and McK-\neown, 2013; Luong et al., 2015). Filippova’s corpus as\nwell as Boudin and Morin’s contain clusters of similar sen-\ntences, each cluster composed of at least 7 or 8 sentences,\nwhereas the datasets introduced in (McKeown et al., 2010)\nand (Luong et al., 2015) have only a pair of source sen-\ntences per cluster. McKeown et al. (2010) collected 300\nEnglish sentence pairs taken from newswire clusters using\nAmazon’s Mechanical Turk. Likewise, the dataset built by\nLuong et al. (2015) contains 250 Vietnamese sentences di-\nvided into 115 groups of similar sentences with 2 sentences\nper group. Thadani and McKeown (2013) presented an En-\nglish corpus with 1,858 clusters having between 2 and 4\nsentences; this dataset was built using automatic methods\nfrom annotations made for the DUC1 and TAC2 evalua-\ntions. The corpora presented in (McKeown et al., 2010),\n(Boudin and Morin, 2013) and (Luong et al., 2015) are pub-\nlicly available, but among these three datasets only the sec-\nond one is more suited to multi-document summarization\nor question-answering tasks because the documents to ana-\nlyze are usually composed of many similar sentences.\n', 9, 0)
(108.82099914550781, 527.5569458007812, 230.34559631347656, 543.1106567382812, '3.\nDataset Description\n', 10, 0)
(52.15800094604492, 545.0914306640625, 287.0125732421875, 628.827392578125, 'We introduce a novel annotated corpus collected from Por-\ntuguese and Spanish Google News.3 This corpus is com-\nposed of clusters of similar sentences along with reference\ncompressions for each cluster. The data are described in the\nfollowing subsections. Table 1 summarizes the characteris-\ntics of the corpus and Table 2 shows a small example of our\nPortuguese dataset.\n', 11, 0)
(52.15800476074219, 636.0112915039062, 287.0119934082031, 723.6123657226562, '3.1.\nSource Sentences\nIn keeping with the methodology introduced by Filippova\n(2010), we collected links from Google News in Spanish\nand Portuguese between July and September 2016. These\nlinks redirect international news sites in Spanish (La Jor-\nnada, Milenio, El Economista, BBC Mundo, El Colom-\nbiano, El Pa´ıs, CNN en espa˜nol, etc.) and in Portuguese\n', 12, 0)
(52.157989501953125, 735.5687255859375, 287.0082702636719, 793.0090942382812, '1http://duc.nist.gov\n2http://www.nist.gov/tac\n3The Spanish and Portuguese MSC datasets are freely avail-\nable, under GPL license on the DOI website: http://dev.\ntermwatch.es/˜fresa/CORPUS/MSF2/.\n', 13, 0)
(304.8659362792969, 208.5875244140625, 539.7181396484375, 411.8755187988281, '(G1, Uol Not´ıcias, Estad˜ao, O Globo, etc.). Each cluster is\ncomposed of related sentences describing a speciﬁc event\nand was chosen among the ﬁrst sentence from different ar-\nticles about Science, Sports, Economy, Health, Business,\nTechnology, Accidents/Catastrophes, General Information\nand other subjects. During the collection period, sentences\nwere gathered among news threads that had at least 8 dif-\nferent sources. The source sentences of each cluster were\nmanually selected so that they best describe the news, while\nsentences dealing with less relevant information were dis-\ncarded. Each source sentence is composed of at least 8 to-\nkens and a verb. In order to ensure the variability of source\nsentences inside a cluster, we removed all duplicated sen-\ntences, by assuming that sentences were too similar when\nthe cosine similarity4 computed from one-hot vectors was\nhigher that 0.8. We used the TreeTagger system5 to tag the\nsource sentences with Parts-of-Speech.\n', 14, 0)
(304.865966796875, 419.823486328125, 539.7144165039062, 603.4083862304688, '3.2.\nReference Compressions\nLike in (Filippova, 2010; Boudin and Morin, 2013), ref-\nerence compressions are edited by human annotators, all\nnative Portuguese or Spanish speakers, who analyzed the\nmost relevant facts of a cluster and generated a condensed\nsentence of this cluster. We suggested that the annotators\nshould use the same vocabulary and n-grams as the source\nsentences and only select the most relevant information\nabout the topic. We also recommended that they should\ngenerate compressions that are shorter than the length aver-\nage of the source sentences. The following sections provide\ndetails about the Portuguese and Spanish parts of the cor-\npus and, as a matter of comparison, brieﬂy recalls the main\ncharacteristics of the French corpus built by Boudin and\nMorin.\n', 15, 0)
(304.865966796875, 612.3128051757812, 539.7144165039062, 732.3843994140625, '3.2.1.\nPortuguese Dataset\nThe Portuguese corpus is composed of 40 clusters. Each\ncluster has at least 10 similar sentences by topic and 2 refer-\nence compressions made by 2 human annotators. This cor-\npus contains 17,998 tokens and has a vocabulary of 2,438\ntokens. Source sentences have an average of 33.1 tokens\nper sentence with a standard deviation of 9.9 tokens. The\nType-Token Ratio (TTR) indicates the reuse of tokens in\nthe cluster and is deﬁned by the number of unique tokens\ndivided by the number of tokens in each cluster; the lower\n', 16, 0)
(304.8660583496094, 745.49267578125, 539.7169799804688, 768.3035278320312, '4The cosine similarity between two vectors u and v associated\nwith two sentences is deﬁned by\nu·v\n', 17, 0)
(304.8659973144531, 757.4990234375, 539.7213745117188, 793.0090942382812, '||u|| ||v|| in the [0,1] range.\n5Website:\nhttp://www.cis.uni-muenchen.de/\n˜schmid/tools/TreeTagger/\n', 18, 0)
(284.0, 800.989990234375, 310.68798828125, 817.47802734375, '3193\n', 19, 0)

page suivante
(60.660980224609375, 68.36791229248047, 531.2155151367188, 176.77146911621094, 'Source sentences :\nA Tesla fez uma oferta de compra `a empresa de servic¸os de energia solar SolarCity por mais de 2300 milh˜oes de euros.\nA Tesla Motors , fabricante de carros el´etricos , anunciou aquisic¸˜ao da SolarCity por US$ 2,6 bilh˜oes .\nA fabricante de carros el´etricos e baterias Tesla Motors disse nesta segunda-feira ( 1 ) que chegou a um acordo com a\nSolarCity para comprar a instaladora de pain´eis solares por US$ 2,6 bilh˜oes , em um grande passo do bilion´ario Elon\nMusk para oferecer aos consumidores um neg´ocio totalmente especializado em energia limpa , informou a Reuters .\nReference compressions :\nA Tesla Motors anunciou acordo para comprar a SolarCity por US$ 2,6 bilh˜oes .\nA fabricante Tesla Motors vai adquirir a instaladora de pain´eis solares da SolarCity .\n', 0, 0)
(194.125, 186.50555419921875, 397.7505798339844, 198.51048278808594, 'Table 2: Small example of our Portuguese dataset.\n', 1, 0)
(52.15800476074219, 228.5125732421875, 287.0064392089844, 336.15960693359375, 'the TTR, the greater the reuse of tokens in the cluster. The\nsentence similarity represents the average cosine similarity\nof the sentences in a cluster. Using these metrics, references\nhave an average length of 17.8 tokens and a standard devi-\nation of 1.5 tokens, while the Portuguese source corpus has\na TTR of 33.7%. The Portuguese annotators generated the\ncompressions with a TTR of 67.9% and a sentence similar-\nity of 0.59. Finally, the average compression ratio between\nthe reference and source sentences is 54%.\n', 2, 0)
(52.15800476074219, 345.5150146484375, 287.0063781738281, 489.6354675292969, '3.2.2.\nSpanish Dataset\nThe Spanish part is also composed of 40 clusters. It has\n30,588 tokens and a vocabulary of 4,390 tokens. Each clus-\nter has 20 similar sentences on the same topic and 4 refer-\nence compressions made by 4 human annotators. Source\nsentences have an average of 38.2 tokens per sentence with\na standard deviation of 10.7 tokens and an average TTR of\n35.2%. Reference compressions contain the same vocab-\nulary as source sentences while keeping an average size of\n23.1 tokens, a standard deviation of 2.4 tokens and a TTR of\n43.4%. The sentence similarity between the compressions\nis 0.64. The average compression rate is 61%.\n', 3, 0)
(52.15800476074219, 498.9908752441406, 287.0063781738281, 583.3353881835938, '3.2.3.\nFrench Dataset\nWe used in the following experiments the French corpus\ndeveloped by Boudin and Morin (2013). This corpus also\nhas 40 clusters composed of 618 sentences (33 tokens on\naverage). The clusters are composed of 15 sentences on\naverage and the TTR of the corpus is 38.8%. Reference\ncompressions have a compression rate of 60%.\n', 4, 0)
(94.91700744628906, 594.1968994140625, 244.24942016601562, 609.7506103515625, '4.\nExperimental Evaluation\n', 5, 0)
(52.15800857543945, 612.5874633789062, 287.0064392089844, 791.9644165039062, 'We used our corpus to provide a more thorough evalua-\ntion of state-of-the-art approaches for MSC than the study\non the French corpus alone. We tested on our dataset a\nsimple baseline, as well as (Filippova, 2010) and (Boudin\nand Morin, 2013) methods. Filippova modeled clusters of\nsimilar sentences as Word Graphs based on the cohesion\nof tokens and their Part-of-Speech (PoS). Inspired by the\ngood results of the Filippova’s method, Boudin and Morin\nused the TextRank method as a re-rank method to analyze\nthe sentences generated by Filippova’s method in order to\nproduce well punctuated and hopefully more informative\ncompressions. The baseline system creates a Word Graph\n(WG) like Filippova’s method, but this time all arcs have\nthe same weight. Then, the system generates a compres-\nsion represented by the shortest path in the WG that has\n', 6, 0)
(304.8659973144531, 228.512451171875, 539.7142944335938, 252.5122528076172, 'at least 8 tokens. Algorithms were implemented using the\nPython programming language and the takahe6 library.\n', 7, 0)
(304.86602783203125, 261.9403991699219, 479.3426818847656, 276.19793701171875, '4.1.\nAutomatic and Manual Metrics\n', 8, 0)
(304.86602783203125, 278.99151611328125, 539.7145385742188, 435.03240966796875, 'The most important features of MSC are informativeness\nand grammaticality. Informativeness is the percentage of\nthe main information retained in the compression, while\ngrammaticality analyzes whether a sentence is correct or\nnot.\nReferences are assumed to contain the most important\ninformation.\nThus we calculated informativeness scores\nbased on the common information between the output of\nthe MSC system and the references using ROUGE (Lin,\n2004).\nIn particular, we used the f-measure metrics\nROUGE-1, ROUGE-2 and ROUGE-SU4. Like in Boudin\nand Morin (Boudin and Morin, 2013), ROUGE metrics are\ncalculated using stop words removal and stemming.7\n', 9, 0)
(304.86602783203125, 435.55645751953125, 539.7144775390625, 591.0232543945312, 'We also led a manual evaluation with 4 native speakers\nfor each language. The native speakers of each language\njudged the compression in two aspects: informativeness\nand grammaticality. In the same way as (Filippova, 2010;\nBoudin and Morin, 2013), the native speakers evaluated the\ngrammaticality in a 3-point scale: 0 point for an ungram-\nmatical compression, 1 point for compression with minor\nmistakes; and 2 points for a correct compression. The in-\nformativeness evaluation process is similar for grammati-\ncality: 0 point if the compression is not related to the main\ntopic, 1 point if the compression misses some relevant in-\nformation and 2 points if the compression conveys the gist\nof the main event.\n', 10, 0)
(304.86602783203125, 600.4901733398438, 479.9344177246094, 614.7477416992188, '4.2.\nResults with Automatic Metrics\n', 11, 0)
(304.8659973144531, 617.5413208007812, 539.71630859375, 665.4122924804688, 'Table 3 shows f-score ROUGE scores for the French, Por-\ntuguese and Spanish datasets.8 Boudin and Morin’s system\ngenerated better compressions with higher ROUGE scores\nthan Filippova’s and the baseline for all datasets.\n', 12, 0)
(304.86602783203125, 680.857666015625, 539.7175903320312, 791.6845703125, '6Website:\nhttp://www.florianboudin.org/\npublications.html\n7http://snowball.tartarus.org/\n8Although we used the same system and data as (Boudin and\nMorin, 2013) for the French corpus, we were not able to repro-\nduce exactly their results. The ROUGE scores given in their arti-\ncle are close to ours for their system: 0.6568 (ROUGE-1), 0.4414\n(ROUGE-2) and 0.4344 (ROUGE-SU4), but using Filippova’s\nsystem we measured higher scores than them: 0.5744 (ROUGE-\n1), 0.3921 (ROUGE-2) and 0.3700 (ROUGE-SU4).\n', 13, 0)
(284.0, 800.989990234375, 310.68798828125, 817.47802734375, '3194\n', 14, 0)

page suivante
(60.8466796875, 69.38036346435547, 495.29754638671875, 87.94093322753906, 'Method\nFrench\nPortuguese\nSpanish\n', 0, 0)
(175.60076904296875, 81.09624481201172, 534.9363403320312, 93.79835510253906, 'RG-1\nRG-2\nRG-SU4\nRG-1\nRG-2\nRG-SU4\nRG-1\nRG-2\nRG-SU4\n', 1, 0)
(60.8466796875, 93.55465698242188, 530.7376708984375, 105.31948852539062, 'Baseline\n0.3681\n0.1904\n0.1758\n0.3199\n0.1273\n0.1309\n0.2700\n0.0990\n0.0984\n', 2, 0)
(60.8466796875, 105.31011199951172, 530.7333984375, 118.01222229003906, 'Filippova (2010)\n0.6384\n0.4423\n0.4297\n0.5388\n0.2971\n0.2938\n0.5004\n0.2983\n0.2847\n', 3, 0)
(60.8466796875, 117.41606903076172, 530.7333984375, 130.11817932128906, 'Boudin and Morin (2013)\n0.6674\n0.4672\n0.4602\n0.5532\n0.3029\n0.2868\n0.5140\n0.2960\n0.2801\n', 4, 0)
(52.15800094604492, 139.2425537109375, 539.7177734375, 151.2474822998047, 'Table 3: ROUGE f-scores measured on the French, Portuguese and Spanish datasets. The best ROUGE results are in bold.\n', 5, 0)
(52.15800094604492, 181.24957275390625, 287.0064392089844, 576.12744140625, 'Table 4 provides statistics on the length and the compres-\nsion ratio of the sentences generated by the systems. The\nbaseline system output the shortest compressions, which\ntranslated into the worst ROUGE scores.\nFor the three\ntested datasets, Filippova’s method generated shorter com-\npressions with a smaller standard deviation than Boudin\nand Morin’s system. Let us note that for this last system\nthe lengths of the outputs are less regular across the three\nlanguages.\nThe Portuguese and Spanish languages derive from Latin\nand are closely related languages. However, they differ in\nmany details of their grammar and lexicon. Moreover, the\ndatasets produced for the three languages are unlike accord-\ning to several features. First, our corpus contains a smaller\n(Portuguese corpus) and a larger (Spanish corpus) dataset\nin terms of sentences than the original French corpus. Be-\nsides, the compression rates of the three datasets (see Sec-\ntion 3.) indicates that the Portuguese source sentences have\nmore irrelevant tokens. The sentence similarity (Table 1,\nlast line) describes the variability of sentences in the source\nsentences and in the references, and reﬂects here that the\nsentences are slightly more diverse for the Portuguese cor-\npus. It can be noticed that the references are more similar\ntoo each other than source sentences since they only retain\nthe main information. Finally, the French corpus has a TTR\nof 38.8% whereas the Portuguese and Spanish datasets have\nTTRs of 33.7% and 35.2%, respectively.\nThe baseline system generated the shortest compression be-\ncause all arcs of the WG have the same weights. However,\nthis system analyzes neither the grammaticality nor the\nmost used n-grams in the clusters. Consequently, the base-\nline system generated compressions with the worst ROUGE\nscores.\n', 6, 0)
(52.15800094604492, 584.2273559570312, 287.00640869140625, 791.9644775390625, '4.3.\nHuman Evaluation\nROUGE only analyzes the overlapping between the candi-\ndate compression and the references. Since this analysis is\nnot reliable enough, we led a further manual evaluation to\nstudy the informativeness and grammaticality of compres-\nsions, as described in Section 4.1.. Given the poor results\nof the baseline with ROUGE, we only analyzed the Filip-\npova’s and Boudin and Morin’s methods (Table 5).\nWe measured inter-rater agreement on the judgments we\ncollected, obtaining values of Fleiss’ kappa of 0.418, of\n0.305 and 0.364 for French, Portuguese and Spanish re-\nspectively.\nThese results show that human evaluation is\nrather subjective. Questioning evaluators on how they pro-\nceed to rate sentences reveals that they often made their\nchoice by comparing outputs for a given cluster. As the\ndifferences of the grammaticality and the informativeness\nscores for the methods are not statistically signiﬁcant, we\n', 7, 0)
(304.865966796875, 181.24951171875, 539.7144165039062, 444.771484375, 'move our investigation on the average and standard de-\nviation of the results. Both methods generated compres-\nsions of good quality (scores higher than 1) for all datasets,\nespecially for the French and the Portuguese parts where\nscores above 1.5 for grammaticality and above 1.2 for in-\nformativeness were obtained. Filippova’s method gener-\nated more correct compressions (except for the Portuguese\ncorpus where both methods obtained almost the same re-\nsults), which shows that the re-ranking step tends to mod-\nerately deteriorate grammaticality.\nBy contrast, Boudin\nand Morin’s method consistently improves informative-\nness, which validates the interest of integrating the anal-\nysis of key phrases inside candidate compressions. This re-\nranking method combines the cohesion score of Filippova\nand the relevance of key phrases9 to generate more infor-\nmative compression. This method selects the path of Word\nGraph that has relevant key phrases even if this path has a\nlower cohesion quality.\nAll in all, Boudin and Morin’s method generated more in-\nformative but also longer compressions than Filippova’s,\nCR showing a relative increase of 18% between both sys-\ntems (Table 4).\n', 8, 0)
(337.1589660644531, 456.6969909667969, 507.4249572753906, 472.2507019042969, '5.\nConclusion and Future Work\n', 9, 0)
(304.865966796875, 475.5865478515625, 539.71435546875, 679.7904052734375, 'Multi-Sentence Compression aims to generate a short infor-\nmative text summary from several sentences with related\nand redundant information. This task can be used in the\ndomain of multi-document summarization or question an-\nswering to provide more informative and concise texts.\nIn this paper, we presented a new annotated corpus in\ntwo languages that extends the French data made available\nin (Boudin and Morin, 2013). We also compared two state-\nof-the art systems on this new dataset. We hope this cor-\npus will help the NLP community to develop and validate\nmulti-language methods for multi-sentence compression.\nIn order to extend the multi-language resources to more di-\nverse languages, we plan to create a similar MSC dataset\nfor Arabic. We also want to use our corpus to test other\ncompetitive MSC systems, such as the one based on integer\nlinear programming we introduced in (Linhares Pontes et\nal., 2016).\n', 10, 0)
(365.0679626464844, 691.7159423828125, 479.5151062011719, 707.2696533203125, '6.\nAcknowledgments\n', 11, 0)
(304.865966796875, 710.60546875, 539.7142333984375, 734.56640625, 'This work was partially ﬁnanced by the European Project\nCHISTERA-AMIS ANR-15-CHR2-0001.\n', 12, 0)
(304.86602783203125, 746.9566650390625, 539.7149658203125, 791.6845703125, '9Key phrases are multi-word phrases composed of the syn-\ntactic pattern (ADJ)∗(NPP|NC)+(ADJ)∗, which ADJ are\nadjectives, NPP are proper nouns and NC are common nouns.\nFrench, Portuguese and Spanish have similar syntactic patterns.\n', 13, 0)
(284.0, 800.989990234375, 310.68798828125, 817.47802734375, '3195\n', 14, 0)

page suivante
(116.8219985961914, 68.36791229248047, 455.5262145996094, 87.30728149414062, 'Method\nFrench\nPortuguese\nSpanish\n', 0, 0)
(239.03700256347656, 80.68255615234375, 472.56689453125, 92.68749237060547, 'Length\nCR\nLength\nCR\nLength\nCR\n', 1, 0)
(116.8219985961914, 93.03558349609375, 475.0514831542969, 111.80512237548828, 'Baseline\n9.6 ± 1.5\n29%\n9.5 ± 1.2\n29%\n9.1 ± 0.3\n24%\n', 2, 0)
(116.8219985961914, 105.38958740234375, 475.0517883300781, 124.15912628173828, 'Filippova (2010)\n16.9 ± 5.1\n51%\n17.3 ± 5.3\n52%\n16.5 ± 6.4\n43%\n', 3, 0)
(116.8219985961914, 117.74359130859375, 475.0517883300781, 136.5131378173828, 'Boudin and Morin (2013)\n19.7 ± 6.9\n59%\n22.9 ± 6.3\n69%\n23.4 ± 8.4\n61%\n', 4, 0)
(82.89299774169922, 139.4815673828125, 508.9833068847656, 151.4864959716797, 'Table 4: Length (average and standard deviation of tokens) and compression ratio (CR) of system outputs.\n', 5, 0)
(65.34400177001953, 171.9648895263672, 489.84521484375, 190.90328979492188, 'Method\nFrench\nPortuguese\nSpanish\n', 6, 0)
(190.18899536132812, 183.91990661621094, 512.7105712890625, 196.88125610351562, 'Gram.\nInfo.\nGram.\nInfo.\nGram.\nInfo.\n', 7, 0)
(65.34400177001953, 196.27391052246094, 526.5306396484375, 215.40211486816406, 'Filippova (2010)\n1.65 ± 0.58\n1.25 ± 0.76\n1.61 ± 0.64\n1.51 ± 0.66\n1.51 ± 0.69\n1.02 ± 0.72\n', 8, 0)
(65.34400177001953, 208.62693786621094, 526.5306396484375, 227.75514221191406, 'Boudin and Morin (2013)\n1.56 ± 0.62\n1.48 ± 0.68\n1.66 ± 0.62\n1.70 ± 0.59\n1.30 ± 0.76\n1.16 ± 0.82\n', 9, 0)
(52.15800094604492, 230.72454833984375, 539.7174682617188, 254.6844940185547, 'Table 5:\nManual evaluation of compressions (ratings are expressed on a scale of 0 to 2). All results are statistically\nequivalent.\n', 10, 0)
(90.32699584960938, 282.4150390625, 248.8409881591797, 297.96875, '7.\nBibliographical References\n', 11, 0)
(52.157989501953125, 302.37255859375, 287.0119934082031, 791.9642944335938, 'Barzilay, R. and McKeown, K. R. (2005). Sentence fusion\nfor multidocument news summarization. Computational\nLinguistics, 31(3):297–328, September.\nBoudin, F. and Morin, E. (2013). Keyphrase extraction\nfor N-best reranking in multi-sentence compression. In\nNAACL, pages 298–305.\nClarke, J. and Lapata, M. (2008). Global inference for\nsentence compression: An integer linear programming\napproach.\nJournal of Artiﬁcial Intelligence Research\n(JAIR, 31:399–429.\nCohn, T. and Lapata, M. (2008). Sentence compression\nbeyond word deletion. In COLING, pages 137–144.\nFilippova, K. and Altun, Y. (2013). Overcoming the lack of\nparallel data in sentence compression. In EMNLP, pages\n1481–1491.\nFilippova, K., Alfonseca, E., Colmenares, C. A., Kaiser,\nL., and Vinyals, O. (2015). Sentence compression by\ndeletion with LSTMs. In EMNLP, pages 360–368.\nFilippova, K. (2010). Multi-sentence compression: Find-\ning shortest paths in word graphs. In COLING, pages\n322–330.\nGanitkevitch, J., Callison-Burch, C., Napoles, C., and\nDurme, B. V. (2011). Learning sentential paraphrases\nfrom bilingual parallel corpora for text-to-text genera-\ntion. In EMNLP, pages 1168–1179.\nIve, J. and Yvon, F. (2016). Parallel sentence compression.\nIn COLING, Technical Papers, page 1503–1513.\nKnight, K. and Marcu, D. (2002). Summarization beyond\nsentence extraction: A probabilistic approach to sentence\ncompression. Artiﬁcial Intelligence, 139(1):91–107.\nLin, C.-Y. (2004). ROUGE: A Package for Automatic\nEvaluation of Summaries. In Workshop Text Summariza-\ntion Branches Out (ACL’04), pages 74–81.\nLinhares Pontes, E., Gouveia da Silva, T., Linhares,\nA. C., Torres-Moreno, J.-M., and Huet, S.\n(2016).\nM´etodos de otimizac¸˜ao combinat´oria aplicados ao prob-\nlema de compress˜ao multifrases. In Anais do XLVIII\nSimp´osio Brasileiro de Pesquisa Operacional (SBPO),\npages 2278–2289.\nLuong, A. V., Tran, N. T., Ung, V. G., and Nghiem, M. Q.\n', 12, 0)
(304.865966796875, 284.68634033203125, 539.719970703125, 501.9021911621094, '(2015). Word graph-based multi-sentence compression:\nRe-ranking candidates using frequent words. In Sev-\nenth International Conference on Knowledge and Sys-\ntems Engineering (KSE), pages 55–60.\nMcKeown, K., Rosenthal, S., Thadani, K., and Moore, C.\n(2010). Time-efﬁcient creation of an accurate sentence\nfusion corpus. In HLT-NAACL, pages 317–320.\nRush, A. M., Chopra, S., and Weston, J. (2015). A neural\nattention model for abstractive sentence summarization.\nIn EMNLP, pages 379–389.\nThadani, K. and McKeown, K. (2013). Supervised sen-\ntence fusion with single-stage inference. In Sixth Inter-\nnational Joint Conference on Natural Language Process-\ning, IJCNLP, pages 1410–1418.\nToutanova, K., Brockett, C., Tran, K. M., and Amershi,\nS. (2016). A dataset and evaluation metrics for abstrac-\ntive compression of sentences and short paragraphs. In\nEMNLP, pages 340–350.\n', 13, 0)
(332.364990234375, 511.53570556640625, 512.218994140625, 527.0894165039062, '8.\nLanguage Resource References\n', 14, 0)
(304.8659973144531, 529.3492431640625, 539.718994140625, 625.5331420898438, 'Boudin,\nFlorian\nand\nMorin,\nEmmanuel.\n(2013).\nKeyphrase Extraction for N-best Reranking in Multi-\nSentence Compression. NAACL (2013). Available on\nhttps://github.com/boudinﬂ/lina-msc.\nGraff, David and Cieri, Christopher and Kong, Junbo and\nChen, Ke and Maeda, Kazuaki. (2011). English Gi-\ngaword. Linguistic Data Consortium, 5th, ISLRN 911-\n942-430-413-0.\n', 15, 0)
(284.0, 800.989990234375, 310.68798828125, 817.47802734375, '3196\n', 16, 0)

page suivante
