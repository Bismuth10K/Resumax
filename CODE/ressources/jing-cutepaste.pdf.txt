(192.9600067138672, 91.56861877441406, 489.88775634765625, 110.18301391601562, 'Cut and Paste Based Text Summarization \n', 0, 0)
(218.39999389648438, 122.37117767333984, 455.4835510253906, 135.17758178710938, 'Hongyan \nJing and Kathleen \nR. McKeown \n', 1, 0)
(256.32000732421875, 135.33120727539062, 429.091552734375, 148.13760375976562, 'Department \nof Computer Science \n', 2, 0)
(289.20001220703125, 148.05117797851562, 395.19140625, 160.85757446289062, 'Columbia University \n', 3, 0)
(272.1600036621094, 161.25119018554688, 411.6828308105469, 174.05758666992188, 'New York, NY 10027, USA \n', 4, 0)
(266.1600036621094, 174.21121215820312, 412.96136474609375, 187.01760864257812, 'hjing, kathyQcs.columbia.edu \n', 5, 0)
(194.39999389648438, 230.42593383789062, 243.7506561279297, 244.71913146972656, 'Abstract \n', 6, 0)
(107.04000091552734, 246.21121215820312, 334.47235107421875, 281.5776062011719, 'We present a cut and paste based text summa- \nrizer, which uses operations derived from an anal- \nysis of human written abstracts. \nThe summarizer \n', 7, 0)
(107.04000091552734, 279.81121826171875, 335.6065979003906, 371.5776062011719, 'edits extracted sentences, using reduction to remove \ninessential phrases and combination to merge re- \nsuiting phrases together as coherent sentences. Our \nwork includes a statistically based sentence decom- \nposition program that identifies where the phrases of \na summary originate in the original document, pro- \nducing an aligned corpus of summaries and articles \nwhich we used to develop the summarizer. \n', 8, 0)
(108.0, 380.4259338378906, 198.92921447753906, 394.7191467285156, '1 \nIntroduction \n', 9, 0)
(107.04000091552734, 396.6911926269531, 334.8841247558594, 554.9375610351562, 'There is a big gap between the summaries produced \nby current automatic summarizers and the abstracts \nwritten by human professionals. Certainly one fac- \ntor contributing to this gap is that automatic sys- \ntems can not always correctly identify the important \ntopics of an article. Another factor, however, which \nhas received little attention, is that automatic sum- \nmarizers have poor text generation techniques. Most \nautomatic summarizers rely on extracting key sen- \ntences or paragraphs from an article to produce a \nsummary. Since the extracted sentences are discon- \nnected in the original article, when they are strung \ntogether, the resulting summary can be inconcise, \nincoherent, and sometimes even misleading. \n', 10, 0)
(117.36000061035156, 553.6511840820312, 334.15521240234375, 566.4575805664062, 'We present a cut and paste based text sum- \n', 11, 0)
(107.5199966430664, 564.93115234375, 334.8914489746094, 600.2975463867188, 'marization technique, aimed at reducing the gap \nbetween automatically generated summaries and \nhuman-written abstracts. \nRather than focusing \n', 12, 0)
(107.5199966430664, 598.7711791992188, 334.5770263671875, 633.4175415039062, 'on how to identify key sentences, as do other re- \nsearchers, we study how to generate the text of a \nsummary once key sentences have been extracted. \n', 13, 0)
(117.5999984741211, 632.3711547851562, 334.4751281738281, 645.1775512695312, 'The main idea of cut and paste summarization \n', 14, 0)
(107.76000213623047, 643.4111938476562, 334.4471130371094, 667.257568359375, 'is to reuse the text in an article to generate the \nsummary. \nHowever, instead of simply extracting \n', 15, 0)
(107.76000213623047, 665.4911499023438, 334.542724609375, 711.4175415039062, 'sentences as current summarizers do, the cut and \npaste system will "smooth" the extracted sentences \nby editing them. Such edits mainly involve cutting \nphrases and pasting them together in novel ways. \n', 16, 0)
(117.83999633789062, 710.3711547851562, 334.44757080078125, 735.2791137695312, 'The key features of this work are: \n(1) The identification of cutting and past- \n', 17, 0)
(350.6400146484375, 229.94589233398438, 578.5137329101562, 322.1376037597656, 'ing operations. We identified six operations that \ncan be used alone or together to transform extracted \nsentences into sentences in human-written abstracts. \nThe operations were identified based on manual and \nautomatic comparison of human-written abstracts \nand the original articles. Examples include sentence \nreduction, sentence combination, syntactic transfor- \nmation, and lexical paraphrasing. \n', 18, 0)
(361.20001220703125, 323.305908203125, 578.8003540039062, 337.59912109375, '(2) Development of an automatic system to \n', 19, 0)
(350.3999938964844, 334.5859069824219, 578.9149169921875, 392.9375915527344, 'perform cut and paste operations. Two opera- \ntions - sentence reduction and sentence combination \n- are most effective in transforming extracted sen- \ntences into summary sentences that are as concise \nand coherent as in human-written abstracts. \nWe \n', 20, 0)
(350.6400146484375, 391.17120361328125, 578.4169921875, 449.3376159667969, 'implemented a sentence reduction module that re- \nmoves extraneous phrases from extracted sentences, \nand a sentence combination module that merges the \nextracted sentences or the reduced forms resulting \nfrom sentence reduction. \nOur sentence reduction \n', 21, 0)
(350.6400146484375, 447.5711975097656, 578.7163696289062, 549.4175415039062, 'model determines what to cut based on multiple \nsources of information, including syntactic knowl- \nedge, context, and statistics learned from corpus \nanalysis. It improves the conciseness of extracted \nsentences, making them concise and on target. Our \nsentence combination module implements combina- \ntion rules that were identified by observing examples \nwritten by human professionals. It improves the co- \nherence of extracted sentences. \n', 22, 0)
(361.20001220703125, 550.5858764648438, 575.300537109375, 564.8790893554688, '(3) Decomposing human-wrltten summary \n', 23, 0)
(350.8800048828125, 561.6259155273438, 578.7938842773438, 686.6975708007812, 'sentences. The cut and paste technique we propose \nhere is a new computational model which we based \non analysis of human-written abstracts. To do this \nanalysis, we developed an automatic system that can \nmatch a phrase in a human-written abstract to the \ncorresponding phrase in the article, identifying its \nmost likely location. This decomposition program \nallows us to analyze the construction of sentences \nin a human-written abstract. Its results have been \nused to train and test the sentence reduction and \nsentence combination module. \n', 24, 0)
(360.9599914550781, 688.0512084960938, 578.0989990234375, 700.8576049804688, 'In Section 2, we discuss the cut and paste tech- \n', 25, 0)
(351.1199951171875, 699.0911865234375, 578.5811157226562, 734.2175903320312, 'nique in general, from both a professional and com- \nputational perspective. We also describe the six cut \nand paste operations. In Section 3, we describe the \n', 26, 0)
(331.44000244140625, 745.4544067382812, 352.3340759277344, 759.3345336914062, '178 \n', 27, 0)

page suivante
(83.04000091552734, 86.13118743896484, 309.5411376953125, 164.93759155273438, 'system architecture. The major components of the \nsystem, including sentence reduction, sentence com- \nbination, decomposition, and sentence selection, are \ndescribed in Section 4. The evaluation results are \nshown in Section 5. Related work is discussed in \nSection 6. Finally, we conclude and discuss future \nwork. \n', 0, 0)
(338.8800048828125, 87.0912094116211, 539.2964477539062, 99.89761352539062, 'Document sentence: \nWhen it arrives some- \n', 1, 0)
(338.3999938964844, 97.89119720458984, 539.8155517578125, 177.17758178710938, "time next year in new TV sets, the V-chip will \ngive parents a new and potentially revolution- \nary device to block out programs they don't \nwant their children to see. \nSummary sentence: The V-chip will give par- \nents a device to block out programs they don't \nwant their children to see. \n", 2, 0)
(83.76000213623047, 184.8153839111328, 278.9095153808594, 199.4409942626953, '2 \nCut and paste in summarization \n', 3, 0)
(83.27999877929688, 207.61537170410156, 253.16273498535156, 233.2809600830078, '2.1 \nRelated work in professional \nsummarizing \n', 4, 0)
(83.04000091552734, 240.45120239257812, 309.4461975097656, 275.8175964355469, 'Professionals take two opposite positions on whether \na summary should be produced by cutting and past- \ning the original text. \nOne school of scholars is \n', 5, 0)
(83.04000091552734, 274.29119873046875, 309.5260925292969, 287.09759521484375, 'opposed; "(use) your own words... \nDo not keep \n', 6, 0)
(82.80000305175781, 285.5711975097656, 309.5709533691406, 320.6976013183594, 'too close to the words before you", states an early \nbook on abstracting for American high school stu- \ndents (Thurber, 1924). \nAnother study, however, \n', 7, 0)
(82.55999755859375, 319.17120361328125, 310.3170166015625, 443.8175964355469, 'shows that professional abstractors actually rely on \ncutting and pasting to produce summaries: "Their \nprofessional role tells abstractors to avoid inventing \nanything. They follow the author as closely as pos- \nsible and reintegrate the most important points of \na document in a shorter text" (Endres-Niggemeyer \net al., 1998). Some studies are somewhere in be- \ntween: "summary language may or may not follow \nthat of author\'s" (Fidel, 1986). Other guidelines or \nbooks on abstracting (ANSI, 1997; Cremmins, 1982) \ndo not discuss the issue. \n', 8, 0)
(92.87999725341797, 446.3712158203125, 308.9327697753906, 459.1776123046875, 'Our cut and paste based summarization is a com- \n', 9, 0)
(82.55999755859375, 457.41119384765625, 308.79449462890625, 481.4975891113281, 'putational model; we make no claim that humans \nuse the same cut and paste operations. \n', 10, 0)
(82.80000305175781, 494.6553649902344, 236.36839294433594, 509.28094482421875, '2.2 \nCut and paste operations \n', 11, 0)
(82.08000183105469, 516.211181640625, 309.1199951171875, 651.8975830078125, 'We manually analyzed 30 articles and their corre- \nsponding human-written summaries; the articles and \ntheir summaries come from different domains ( 15 \ngeneral news reports, 5 from the medical domain, \n10 from the legal domain) and the summaries were \nwritten by professionals from different organizations. \nWe found that reusing article text for summarization \nis almost universal in the corpus we studied. We de- \nfined six operations that can be used alone, sequen- \ntially, or simultaneously to transform selected sen- \ntences from an article into the corresponding sum- \nmary sentences in its human-written abstract: \n', 12, 0)
(92.87999725341797, 653.0553588867188, 205.0762176513672, 667.6809692382812, '(1) sentence reduction \n', 13, 0)
(92.63999938964844, 668.8511962890625, 308.3987731933594, 681.6575927734375, 'Remove extraneous phrases from a selected sen- \n', 14, 0)
(82.31999969482422, 679.8911743164062, 242.19505310058594, 692.6975708007812, 'tence, as in the following example 1: \n', 15, 0)
(93.5999984741211, 709.8911743164062, 308.5793151855469, 722.6975708007812, '1 All the examples in this section were produced by human \n', 16, 0)
(82.31999969482422, 719.0111694335938, 132.19566345214844, 731.8175659179688, 'professionals \n', 17, 0)
(335.0400085449219, 183.57119750976562, 551.7826538085938, 196.37759399414062, 'The deleted material can be at any granularity: a \n', 18, 0)
(325.44000244140625, 194.85122680664062, 552.2548217773438, 218.69760131835938, 'word, a phrase, or a clause. Multiple components \ncan be removed. \n', 19, 0)
(335.2799987792969, 222.25538635253906, 552.0674438476562, 247.25759887695312, '(2) sentence combination \nMerge material from several sentences. It can be \n', 20, 0)
(325.44000244140625, 245.73123168945312, 551.3363647460938, 281.0976257324219, 'used together with sentence reduction, as illustrated \nin the following example, which also uses paraphras- \ning: \n', 21, 0)
(337.9200134277344, 289.17120361328125, 539.84375, 447.4176025390625, 'Text Sentence 1: But it also raises serious \nquestions about the privacy of such highly \npersonal information wafting about the digital \nworld. \nText Sentence 2: The issue thus fits squarely \ninto the broader debate about privacy and se- \ncurity on the internet, whether it involves pro- \ntecting credit card number or keeping children \nfrom offensive information. \nSummary sentence: But it also raises the is- \nsue of privacy of such personal information \nand this issue hits the head on the nail in the \nbroader debate about privacy and security on \nthe internet. \n', 22, 0)
(334.79998779296875, 451.1210632324219, 554.8320922851562, 477.043212890625, '(3) syntactic transformation \nIn both sentence reduction and combination, syn- \n', 23, 0)
(325.20001220703125, 476.0928039550781, 554.2703857421875, 510.4031982421875, 'tactic transformations may be involved. For exam- \nple, the position of the subject in a sentence may be \nmoved from the end to the front. \n', 24, 0)
(335.2799987792969, 513.2810668945312, 555.7217407226562, 539.4432373046875, '(4) lexical paraphrasing \nReplace phrases with their paraphrases. For in- \n', 25, 0)
(324.9599914550781, 537.3311767578125, 552.4674072265625, 583.737548828125, 'stance, the summaries substituted point out with \nnote, and fits squarely into with a more picturesque \ndescription hits the head on the nail in the previous \nexamples. \n', 26, 0)
(334.79998779296875, 587.2953491210938, 552.0256958007812, 612.5375366210938, '(5) generalization or specification \nReplace phrases or clauses with more general or \n', 27, 0)
(324.9599914550781, 611.0111694335938, 552.2742309570312, 623.8175659179688, 'specific descriptions. \nExamples of generalization \n', 28, 0)
(324.9599914550781, 622.0512084960938, 436.58245849609375, 634.8576049804688, 'and specification include: \n', 29, 0)
(338.1600036621094, 644.8511962890625, 539.3615112304688, 657.6575927734375, 'Generalization: \n"a proposed new law that \n', 30, 0)
(338.6400146484375, 656.6421508789062, 538.592041015625, 668.6567993164062, 'would require \nWeb publishers \nto \nobtain \n', 31, 0)
(337.44000244140625, 667.9221801757812, 539.5501708984375, 713.3375854492188, 'parental consent before collecting personal in- \nformation from children" --+ "legislation to \nprotect children\'s privacy on-line" \nSpecification: \n"the White House\'s top drug \n', 32, 0)
(338.6400146484375, 712.5621337890625, 539.5813598632812, 724.5767822265625, 'official" ~ \n"Gen. Barry R. McCaffrey, the \n', 33, 0)
(340.0799865722656, 723.3621215820312, 482.2083435058594, 735.3767700195312, 'White House\'s top drug official" \n', 34, 0)
(303.6000061035156, 742.7808227539062, 325.01080322265625, 757.3219604492188, '179 \n', 35, 0)

page suivante
(182.63999938964844, 185.99830627441406, 250.3941650390625, 204.82083129882812, "p . . . . .  \n,_e_ yr _', \n- \n", 0, 0)
(174.9600067138672, 207.89999389648438, 238.58799743652344, 224.3879852294922, 'I , Co-reference ~, \n', 1, 0)
(174.9600067138672, 223.6782989501953, 234.8994598388672, 229.76133728027344, 'I .\n.\n.\n.\n.\n.\n.\n.\n.\n \nI \n', 2, 0)
(178.8000030517578, 236.8257598876953, 225.0489044189453, 250.27247619628906, ",]WordNet'l \n", 3, 0)
(160.0800018310547, 263.9457702636719, 255.8117218017578, 277.3924865722656, '~ned \nie~ \n-~ \n', 4, 0)
(290.3999938964844, 88.02577209472656, 375.0661315917969, 101.47249603271484, 'Input~icle . \nI\n~\n \n', 5, 0)
(278.1600036621094, 120.42573547363281, 391.44403076171875, 133.87245178222656, 'I Sentenc i extractiÂ°nl \n) \n', 6, 0)
(279.1199951171875, 154.26576232910156, 377.2341613769531, 167.7124786376953, 'extracteikey sentenc~ \n', 7, 0)
(267.8399963378906, 198.4257354736328, 384.9576110839844, 211.87245178222656, 'Cut and paste based generation \n', 8, 0)
(278.6400146484375, 225.0657501220703, 364.3240661621094, 238.51246643066406, '[ Sentence reduction ] \n', 9, 0)
(278.6400146484375, 252.4257354736328, 367.992919921875, 265.8724670410156, 'I Sentence combinatio~ \n', 10, 0)
(290.8800048828125, 312.90576171875, 356.20074462890625, 326.35247802734375, 'Output summary \n', 11, 0)
(272.8800048828125, 337.3857727050781, 403.3717956542969, 350.8324890136719, 'Figure 1: System architecture \n', 12, 0)
(113.5199966430664, 366.04705810546875, 330.4433288574219, 390.6724853515625, '(6) reordering \nChange the order of extracted sentences. For in- \n', 13, 0)
(103.19999694824219, 388.5057678222656, 330.3503112792969, 413.4725036621094, 'stance, place an ending sentence in an article at the \nbeginning of an abstract. \n', 14, 0)
(113.27999877929688, 411.7857666015625, 330.43927001953125, 425.23248291015625, 'In human-written abstracts, there are, of course, \n', 15, 0)
(103.44000244140625, 423.0657653808594, 330.74163818359375, 503.4725036621094, 'sentences that are not based on cut and paste, but \ncompletely written from scratch. We used our de- \ncomposition program to automatically analyze 300 \nhuman-written abstracts, and found that 19% of sen- \ntences in the abstracts were written from scratch. \nThere are also other cut and paste operations not \nlisted here due to their infrequent occurrence. \n', 16, 0)
(103.91999816894531, 513.7857666015625, 234.11656188964844, 527.2324829101562, '3 \nSystem \narchitecture \n', 17, 0)
(103.44000244140625, 529.86572265625, 331.5583190917969, 632.5924682617188, 'The architecture of our cut and paste based text \nsummarization system is shown in Figure 1. Input \nto the system is a single document from any domain. \nIn the first stage, extraction, key sentences in the ar- \nticle are identified, as in most current summarizers. \nIn the second stage, cut and paste based generation, a \nsentence reduction module and a sentence combina- \ntion module implement the operations we observed \nin human-written abstracts. \n', 18, 0)
(114.23999786376953, 630.90576171875, 331.4136047363281, 644.3524780273438, 'The cut and paste based component receives as \n', 19, 0)
(104.4000015258789, 641.9457397460938, 331.89361572265625, 711.0724487304688, 'input not only the extracted key sentences, but also \nthe original article. This component can be ported \nto other single-document summarizers to serve as \nthe generation component, since most current sum- \nmarizers extract key sentences - exactly what the \nextraction module in our system does. \n', 20, 0)
(115.19999694824219, 709.3857421875, 331.3304138183594, 722.8324584960938, 'Other resources and tools in the summarization \n', 21, 0)
(104.87999725341797, 720.4257202148438, 331.1427917480469, 733.8724365234375, 'system include a corpus of articles and their human- \n', 22, 0)
(346.55999755859375, 364.7457580566406, 574.924072265625, 434.35247802734375, 'written abstracts, the automatic decomposition pro- \ngram, a syntactic parser, a co-reference resolution \nsystem, the WordNet lexical database, and a large- \nscale lexicon we combined from multiple resources. \nThe components in dotted lines are existing tools or \nresources; all the others were developed by ourselves. \n', 23, 0)
(347.0400085449219, 442.0257873535156, 469.6563720703125, 455.4725036621094, '4 \nMajor \ncomponents \n', 24, 0)
(346.55999755859375, 455.46575927734375, 576.2744140625, 502.2724914550781, 'The main focus of our work is on decomposition of \nsummaries, sentence reduction, and sentence com- \nbination. We also describe the sentence extraction \nmodule, although it is not the main focus of our \n', 25, 0)
(347.0400085449219, 506.3983459472656, 360.1087646484375, 512.4813842773438, 'work. \n', 26, 0)
(347.0400085449219, 516.90576171875, 539.78466796875, 541.6324462890625, '4.1 \nDecomposition of human-written \nsummary sentences \n', 27, 0)
(347.0400085449219, 541.3857421875, 575.1766357421875, 610.9924926757812, 'The decomposition program, see (Jing and McKe- \nown, 1999) for details, is used to analyze the con- \nstruction of sentences in human-written abstracts. \nThe results from decomposition are used to build \nthe training and testing corpora for sentence reduc- \ntion and sentence combination. \n', 28, 0)
(357.3599853515625, 608.5857543945312, 575.1959838867188, 622.032470703125, 'The decomposition program answers three ques- \n', 29, 0)
(347.5199890136719, 619.86572265625, 576.216796875, 688.9924926757812, 'tions about a sentence in a human-written abstract: \n(1) Is the sentence constructed by cutting and past- \ning phrases from the input article? (2) If so, what \nphrases in the sentence come from the original arti- \ncle? (3) Where in the article do these phrases come \nfrom? \n', 30, 0)
(357.8399963378906, 686.105712890625, 575.034423828125, 699.5524291992188, 'We used a Hidden Markov Model (Baum, 1972) \n', 31, 0)
(348.0, 697.3857421875, 575.2172241210938, 710.8324584960938, 'solution to the decomposition problem. \nWe first \n', 32, 0)
(348.0, 708.4257202148438, 575.6531982421875, 732.9124755859375, 'mathematically formulated the problem, reducing it \nto a problem of finding, for each word in a summary \n', 33, 0)
(327.1199951171875, 745.6943969726562, 348.1761169433594, 759.5745239257812, '180 \n', 34, 0)

page suivante
(111.12000274658203, 81.82391357421875, 509.8494567871094, 125.4281005859375, 'Summary sentence: \n(F0:S1 arthur b sackler vice president for law and public policy of time warner inc ) \n(FI:S-1 and) (F2:S0 a member of the direct marketing association told ) (F3:$2 the com- \nmunications subcommittee \nof the senate commerce committee ) (F4:S-1 that legislation ) \n', 0, 0)
(111.5999984741211, 123.10394287109375, 507.8459777832031, 146.30810546875, "(F5:Slto protect ) (F6:$4 children' s ) (F7:$4 privacy ) (F8:$4 online ) (F9:S0 could destroy \nthe spontaneous nature that makes the internet unique ) \n", 1, 0)
(111.36000061035156, 149.50390625, 509.9339599609375, 266.30810546875, "Source document sentences: \nSentence 0: a proposed new law that would require web publishers to obtain parental consent before \ncollecting personal information from children (F9 could destroy the spontaneous nature that \nmakes the internet unique ) (F2 a member of the direct marketing association told) a \nsenate panel thursday \nSentence 1:(F0 arthur b sackler vice president for law and public policy of time warner \ninc ) said the association supported efforts (F5 to protect ) children online but he urged lawmakers \nto find some middle ground that also allows for interactivity on the internet \nSentence 2: for example a child's e-mail address is necessary in order to respond to inquiries such \nas updates on mark mcguire's and sammy sosa's home run figures this year or updates of an online \nmagazine sackler said in testimony to (F3 the communications subcommittee \nof the senate \n", 2, 0)
(111.36000061035156, 263.50390625, 509.12939453125, 296.8701477050781, "commerce committee ) \nSentence 4: the subcommittee is considering the (F6 children's ) (F8 online ) (F7 privacy ) \nprotection act which was drafted on the recommendation of the federal trade commission \n", 3, 0)
(197.27999877929688, 307.1839294433594, 440.24017333984375, 319.670166015625, 'Figure 2: Sample output of the decomposition program \n', 4, 0)
(84.23999786376953, 334.783935546875, 311.6022033691406, 414.71014404296875, 'sentence, a document position that it most likely \ncomes from. The position of a word in a document \nis uniquely identified by the position of the sentence \nwhere the word appears, and the position of the word \nwithin the sentence. Based on the observation of cut \nand paste practice by humans, we produced a set of \ngeneral heuristic rules. \nSample heuristic rules in- \n', 5, 0)
(84.23999786376953, 413.263916015625, 310.8110046386719, 470.3901672363281, 'clude: two adjacent words in a summary sentence \nare most likely to come from two adjacent words in \nthe original document; adjacent words in a summary \nsentence are not very likely to come from sentences \nthat are far apart in the original document. \nWe \n', 6, 0)
(84.23999786376953, 468.94390869140625, 310.5108642578125, 515.2702026367188, 'use these heuristic rules to create a Hidden Markov \nModel. The Viterbi algorithm (Viterbi, 1967) is used \nto efficiently find the most likely document position \nfor each word in the summary sentence. \n', 7, 0)
(94.55999755859375, 514.303955078125, 310.5858459472656, 526.7902221679688, 'Figure 2 shows sample output of the program. \n', 8, 0)
(84.0, 525.5839233398438, 310.66351318359375, 671.97119140625, 'For the given summary sentence, the program cor- \nrectly identified that the sentence was combined \nfrom four sentences in the input article. It also di- \nvided the summary sentence into phrases and pin- \npointed the exact document origin of each phrase. \nA phrase in the summary sentence is annotated as \n(FNUM:SNUM actual-text), where FNUM is the se- \nquential number of the phrase and SNUM is the \nnumber of the document sentence where the phrase \ncomes from. SNUM = -1 means that the compo- \nnent does not come from the original document. The \nphrases in the document sentences are annotated as \n(FNUM actual-text). \n', 9, 0)
(84.4800033569336, 677.743896484375, 204.4337615966797, 690.2301635742188, '4.2 \nSentence reduction \n', 10, 0)
(84.0, 692.1439208984375, 311.45184326171875, 726.710205078125, 'The task of the sentence reduction module, de- \nscribed in detail in (Jing, 2000), is to remove extra- \nneous phrases from extracted sentences. The goal of \n', 11, 0)
(326.6400146484375, 334.783935546875, 553.6468505859375, 414.9311828613281, 'reduction is to "reduce without major loss"; that is, \nwe want to remove as many extraneous phrases as \npossible from an extracted sentence so that it can be \nconcise, but without detracting from the main idea \nthat the sentence conveys. Ideally, we want to re- \nmove a phrase from an extracted sentence only if it \nis irrelavant to the main topic. \n', 12, 0)
(336.7200012207031, 413.263916015625, 552.9212036132812, 425.7501525878906, 'Our reduction module makes decisions based on \n', 13, 0)
(326.6400146484375, 424.5439147949219, 460.34686279296875, 437.0301513671875, 'multiple sources of knowledge: \n', 14, 0)
(337.20001220703125, 436.0639343261719, 553.0899047851562, 448.5501708984375, '(1) Grammar \nchecking. In this step, we mark \n', 15, 0)
(326.6400146484375, 447.1039123535156, 554.8118896484375, 593.7501831054688, 'which components of a sentence or a phrase are \nobligatory to keep it grammatically correct. To do \nthis, we traverse the sentence parse tree, produced \nby the English Slot Grammar(ESG) parser devel- \noped at IBM (McCord, 1990), in top-down order \nand mark for each node in the parse tree, which \nof its children are obligatory. The main source of \nknowledge the system relies on in this step is a \nlarge-scale, reusable lexicon we combined from mul- \ntiple resources (Jing and McKeown, 1998). The lexi- \ncon contains subcategorizations for over 5,000 verbs. \nThis information is used to mark the obligatory ar- \nguments of verb phrases. \n', 16, 0)
(337.20001220703125, 592.783935546875, 552.3429565429688, 605.2702026367188, '(2) Context information. We use an extracted \n', 17, 0)
(326.6400146484375, 603.8239135742188, 555.2022705078125, 726.9501953125, "sentence's local context in the article to decide which \ncomponents in the sentence are likely to be most \nrelevant to the main topic. We link the words in the \nextracted sentence with words in its local context, \nif they are repetitions, morphologically related, or \nlinked with each other in WordNet through certain \ntype of lexical relation, such as synonymy, antonymy, \nor meronymy. Each word in the extracted sentence \ngets an importance score, based on the number of \nlinks it has with other words and the types of links. \nEach phrase in the sentence is then assigned a score \n", 18, 0)
(306.7200012207031, 739.4544067382812, 326.4801025390625, 753.3345336914062, '181 \n', 19, 0)

page suivante
(130.0800018310547, 86.6239013671875, 529.0975952148438, 109.43014526367188, 'Example 1: \nOriginal sentence \n: When it arrives sometime next year in new TV sets, the V-chip will give \n', 0, 0)
(129.60000610351562, 107.263916015625, 529.547119140625, 161.51016235351562, "parents a new and potentially revolutionary device to block out programs they don't \nwant their children to see. \nReduction program: The V-chip will give parents a new and potentially revolutionary device to \nblock out programs they don't want their children to see. \nProfessionals \n: The V-chip will give parents a device to block out programs they don't want \n", 1, 0)
(129.83999633789062, 159.34393310546875, 214.34915161132812, 171.83016967773438, 'their children to see. \n', 2, 0)
(129.60000610351562, 180.79872131347656, 528.9959106445312, 203.50994873046875, "Example 2: \nOriginal sentence \n: Sore and Hoffman's creation would allow broadcasters to insert \n", 3, 0)
(129.36000061035156, 201.34393310546875, 530.5289916992188, 255.59017944335938, "multiple ratings into a show, enabling the V-chip to filter out racy or violent material but leave \nunexceptional portions o.f a show alone. \nReduction Program: Som and Hoffman's creation would allow broadcasters to insert multiple rat- \nings into a show. \nProfessionals \n: Som and Hoffman's creation would allow broadcasters to insert multiple rat- \n", 4, 0)
(129.1199951171875, 253.4239501953125, 198.82061767578125, 265.9101867675781, 'ings into a show. \n', 5, 0)
(206.16000366210938, 276.2239074707031, 343.6124267578125, 288.71014404296875, 'Figure 3: Sample output of the \n', 6, 0)
(102.23999786376953, 304.0639343261719, 328.6615905761719, 339.11016845703125, 'by adding up the scores of its children nodes in the \nparse tree. This score indicates how important the \nphrase is to the main topic in discussion. \n', 7, 0)
(112.80000305175781, 338.3839111328125, 329.1578063964844, 350.8701477050781, '(3) Corpus evidence. The program uses a cor- \n', 8, 0)
(102.0, 349.6639099121094, 330.3222351074219, 496.3101501464844, 'pus of input articles and their corresponding reduced \nforms in human-written abstracts to learn which \ncomponents of a sentence or a phrase can be re- \nmoved and how likely they are to be removed by \nprofessionals. This corpus was created using the de- \ncomposition program. We compute three types of \nprobabilities from this corpus: the probability that \na phrase is removed; the probability that a phrase is \nreduced (i.e., the phrase is not removed as a whole, \nbut some components in the phrase are removed); \nand the probability that a phrase is unchanged at \nall (i.e., neither removed nor reduced). These cor- \npus probabilities help us capture human practice. \n', 9, 0)
(113.04000091552734, 495.34393310546875, 328.77581787109375, 508.3099365234375, '(4) Final decision. The final reduction decision \n', 10, 0)
(102.4800033569336, 506.6239013671875, 330.19921875, 597.3501586914062, 'is based on the results from all the earlier steps. A \nphrase is removed only if it is not grammatically \nobligatory, not the focus of the local context (indi- \ncated by a low context importance score), and has a \nreasonable probability of being removed by humans. \nThe phrases we remove from an extracted sentence \ninclude clauses, prepositional phrases, gerunds, and \nto-infinitives. \n', 11, 0)
(112.55999755859375, 596.3839111328125, 328.508056640625, 608.8701782226562, 'The result of sentence reduction is a shortened \n', 12, 0)
(102.72000122070312, 607.4239501953125, 329.8588562011719, 653.5101928710938, 'version of an extracted sentence 2. This shortened \ntext can be used directly as a summary, or it can \nbe fed to the sentence combination module to be \nmerged with other sentences. \n', 13, 0)
(113.27999877929688, 652.303955078125, 329.8794250488281, 664.7902221679688, 'Figure 3 shows two examples produced by the re- \n', 14, 0)
(102.95999908447266, 663.3439331054688, 330.6154479980469, 698.1502075195312, 'duction program. The corresponding sentences in \nhuman-written abstracts are also provided for com- \nparison. \n', 15, 0)
(114.4800033569336, 702.2239379882812, 331.5826110839844, 714.710205078125, '2It is actually also possible that the reduction program \n', 16, 0)
(103.44000244140625, 711.3439331054688, 330.4280090332031, 732.9501953125, 'decides no phrase in a sentence should be removed, thus the \nresult of reduction is the same as the input. \n', 17, 0)
(345.8399963378906, 275.9839172363281, 469.0401916503906, 288.47015380859375, 'sentence reduction program \n', 18, 0)
(345.3599853515625, 303.9187316894531, 481.1056213378906, 316.5499267578125, '4.3 \nSentence combination \n', 19, 0)
(345.6000061035156, 318.2239074707031, 573.323486328125, 387.11016845703125, 'To build the combination module, we first manu- \nally analyzed a corpus of combination examples pro- \nduced by human professionals, automatically cre- \nated by the decomposition program, and identified \na list of combination operations. Table 1 shows the \ncombination operations. \n', 20, 0)
(355.44000244140625, 386.1439208984375, 572.99169921875, 398.6301574707031, 'To implement a combination operation, we need \n', 21, 0)
(345.6000061035156, 397.4239196777344, 573.8024291992188, 499.1901550292969, 'to do two things: decide when to use which com- \nbination operation, and implement the combining \nactions. To decide when to use which operation, we \nanalyzed examples by humans and manually wrote \na set of rules. Two simple rules are shown in Fig- \nure 4. Sample outputs using these two simple rules \nare shown in Figure 5. We are currently exploring \nusing machine learning techniques to learn the com- \nbination rules from our corpus. \n', 22, 0)
(355.44000244140625, 498.2239074707031, 573.4882202148438, 510.71014404296875, 'The implementation of the combining actions in- \n', 23, 0)
(345.6000061035156, 509.50390625, 573.970947265625, 555.3501586914062, 'volves joining two parse trees, substituting a subtree \nwith another, or adding additional nodes. We im- \nplemented these actions using a formalism based on \nTree Adjoining Grammar (Joshi, 1987). \n', 24, 0)
(346.0799865722656, 561.438720703125, 466.823974609375, 574.0699462890625, '4.4 \nExtraction Module \n', 25, 0)
(345.8399963378906, 575.743896484375, 574.3195190429688, 699.3501586914062, 'The extraction module is the front end of the sum- \nmarization system and its role is to extract key sen- \ntences. Our method is primarily based on lexical re- \nlations. First, we link words in a sentence with other \nwords in the article through repetitions, morpholog- \nical relations, or one of the lexical relations encoded \nin WordNet, similar to step 2 in sentence reduction. \nAn importance score is computed for each word in a \nsentence based on the number of lexical links it has \nwith other words, the type of links, and the direc- \ntions of the links. \n', 26, 0)
(356.6400146484375, 697.9039306640625, 572.6387939453125, 710.3901977539062, 'After assigning a score to each word in a sentence, \n', 27, 0)
(346.55999755859375, 708.9439086914062, 573.9024658203125, 732.710205078125, 'we then compute a score for a sentence by adding up \nthe scores for each word. This score is then normal- \n', 28, 0)
(326.3999938964844, 746.4144287109375, 347.45611572265625, 760.2945556640625, '182 \n', 29, 0)

page suivante
(88.55999755859375, 81.66815948486328, 457.5666198730469, 94.63175964355469, 'Categories \nCombination Operations \n', 0, 0)
(88.55999755859375, 92.1439208984375, 306.9690856933594, 104.63015747070312, 'Add descriptions or names for people or organizations \n', 1, 0)
(88.55999755859375, 112.5439453125, 142.33895874023438, 125.03018188476562, 'Aggregations \n', 2, 0)
(87.83999633789062, 154.5439453125, 208.4696044921875, 167.03018188476562, 'Substitute incoherent phrases \n', 3, 0)
(88.55999755859375, 196.5439453125, 332.4552917480469, 209.03018188476562, 'Substitute phrases with more general or specific information \n', 4, 0)
(342.4800109863281, 92.6239013671875, 547.8486328125, 220.31015014648438, 'add description (see Figure 5) \nadd name \nextract common subjects or objects (see Figure 5) \nchange one sentence to a clause \nadd connectives (e.g., and or while) \nadd punctuations (e.g., ";") \nsubstitute dangling anaphora \nsubstitute dangling noun phrases \nsubstitute adverbs (e.g., here) \nremove connectives \nsubstitute with more general information \nsubstitute with more specific information \n', 5, 0)
(88.55999755859375, 218.3839111328125, 561.8582153320312, 230.87014770507812, 'Mixed operations \ncombination of any of above operations (see Figure 2) \n', 6, 0)
(243.60000610351562, 249.58392333984375, 388.707763671875, 262.0701599121094, 'Table 1: Combination operations \n', 7, 0)
(109.44000244140625, 270.78814697265625, 508.61712646484375, 390.47015380859375, 'Rule 1: \nIF: ((a person or an organization is mentioned the first time) and (the full name or the full descrip- \ntion of the person or the organization exists somewhere in the original article but is missing in the \nsummary)) \nTHEN" replace the phrase with the full name plus the full description \nRule 2: \nIF: ((two sentences are close to each other in the original article) and (their subjects refer to the \nsame entity) and (at least one of the sentences is the reduced form resulting from sentence reduc- \ntion)) \nTHEN: merge the two sentences by removing the subject in the second sentence, and then com- \nbining it with the first sentence using connective "and". \n', 8, 0)
(218.16000366210938, 401.02392578125, 414.28369140625, 413.5101623535156, 'Figure 4: Sample sentence combination rules \n', 9, 0)
(81.83999633789062, 428.1439208984375, 308.9401550292969, 462.9501647949219, 'ized over the number of words a sentence contains. \nThe sentences with high scores are considered im- \nportant. \n', 10, 0)
(91.91999816894531, 462.70391845703125, 308.1211853027344, 475.1901550292969, 'The extraction system selects sentences based on \n', 11, 0)
(81.83999633789062, 473.7439270019531, 307.5322570800781, 508.5501708984375, 'the importance computed as above, as well as other \nindicators, including sentence positions, cue phrases, \nand tf*idf scores. \n', 12, 0)
(82.08000183105469, 520.148193359375, 154.03659057617188, 533.11181640625, '5 \nEvaluation \n', 13, 0)
(81.83999633789062, 536.8638916015625, 308.234619140625, 571.190185546875, 'Our evaluation includes separate evaluations of each \nmodule and the final evaluations of the overall sys- \ntem. \n', 14, 0)
(91.68000030517578, 571.1838989257812, 307.96917724609375, 583.670166015625, 'We evaluated the decomposition program by two \n', 15, 0)
(81.36000061035156, 582.2239379882812, 307.84918212890625, 606.2301635742188, 'experiments, described in (Jing and McKeown, \n1999). \nIn the first experiment, we selected 50 \n', 16, 0)
(80.4000015258789, 604.783935546875, 307.6768798828125, 728.1502075195312, 'human-written abstracts, consisting of 305 sentences \nin total. A human subject then read the decomposi- \ntion results of these sentences to judge whether they \nare correct. 93.8% of the sentences were correctly \ndecomposed. In the second experiment, we tested \nthe system in a summary alignment task. We ran \nthe decomposition program to identify the source \ndocument sentences that were used to construct the \nsentences in human-written abstracts. Human sub- \njects were also asked to select the document sen- \ntences that are semantlc-equivalent to the sentences \n', 17, 0)
(324.239990234375, 428.6239318847656, 551.7298583984375, 564.4701538085938, 'in the abstracts. We compared the set of sentences \nidentified by the program with the set of sentences \nselected by the majority of human subjects, which is \nused as the gold standard in the computation of pre- \ncision and recall. The program achieved an average \n81.5% precision, 78.5% recall, and 79.1% f-measure \nfor 10 documents. The average performance of 14 \nhuman judges is 88.8% precision, 84.4% recall, and \n85.7% f-measure. Recently, we have also tested the \nsystem on legal documents (the headnotes used by \nWestlaw company), and the program works well on \nthose documents too. \n', 18, 0)
(333.8399963378906, 567.3439331054688, 550.61279296875, 579.8302001953125, 'The evaluation of sentence reduction (see (Jing, \n', 19, 0)
(324.0, 578.6239013671875, 551.4564819335938, 680.3901977539062, '2000) for details) used a corpus of 500 sentences and \ntheir reduced forms in human-written abstracts. 400 \nsentences were used to compute corpus probabili- \nties and 100 sentences were used for testing. The \nresults show that 81.3% of the reduction decisions \nmade by the system agreed with those of humans. \nThe humans reduced the length of the 500 sentences \nby 44.2% on average, and the system reduced the \nlength of the 100 test sentences by 32.7%. \n', 20, 0)
(333.6000061035156, 683.263916015625, 550.6595458984375, 695.7501831054688, 'The evaluation of sentence combination module \n', 21, 0)
(323.760009765625, 694.0639038085938, 551.167724609375, 728.6301879882812, 'is not as straightforward as that of decomposition \nor reduction since combination happens later in the \npipeline and it depends on the output from prior \n', 22, 0)
(302.8800048828125, 742.3344116210938, 323.61212158203125, 756.2145385742188, '183 \n', 23, 0)

page suivante
(133.44000244140625, 82.79666137695312, 533.3831176757812, 199.60269165039062, 'Example 1: add descriptions or names for people or organization \nOriginal document sentences: \n"We\'re trying to prove that there are big benefits to the patients by involving them more deeply in \ntheir treatment", said Paul Clayton, Chairman of the Department dealing with comput- \nerized medical information at Columbia. \n"The economic payoff from breaking into health care records is a lot less than for \nbanks", said Clayton at Columbia. \nCombined sentence: \n"The economic payoff from breaking into health care records is a lot less than for banks", said Paul \nClayton, Chairman of the Department dealing with computerized medical information at Columbia. \nProfessional: (the same) \n', 0, 0)
(133.1999969482422, 209.27664184570312, 534.5065307617188, 263.58624267578125, 'Example 2: extract common subjects \nOriginal document sentences: \nThe new measure is an echo of the original bad idea, blurred just enough to cloud prospects \nboth for enforcement and for court review. \nUnlike the 1996 act, this one applies only to commercial Web sites \n- thus sidestepping \n', 1, 0)
(132.72000122070312, 262.5268859863281, 535.1163330078125, 378.6427001953125, '1996 objections to the burden such regulations would pose for museums, libraries and freewheeling \nconversation deemed "indecent" by somebody somewhere. \nThe new version also replaces the vague "indecency" standard, to which the court objected, \nwith the better-defined one of material ruled "harmful to minors." \nCombined sentences: \nThe new measure is an echo of the original bad idea. \nThe new version applies only to commercial web sites and replaces the vague "indecency" standard \nwith the better-defined one of material ruled "harmful to minors." \nProfessional: \nWhile the new law replaces the "indecency" standard with "harmful to minors" and now only \napplies to commercial Web sites, the "new measure is an echo of the original bad idea." \n', 2, 0)
(203.52000427246094, 389.0366516113281, 479.2185363769531, 401.2027282714844, 'Figure 5: Sample output of the sentence combination program \n', 3, 0)
(105.5999984741211, 416.87664794921875, 333.5016174316406, 462.6427001953125, 'modules. To evaluate just the combination compo- \nnent, we assume that the system makes the same \nreduction decision as humans and the co-reference \nsystem has a perfect performance. \nThis involves \n', 4, 0)
(105.36000061035156, 461.5166320800781, 332.8708801269531, 507.28271484375, 'manual tagging of some examples to prepare for the \nevaluation; this preparation is in progress. The eval- \nuation of sentence combination will focus on the ac- \ncessment of combination rules. \n', 5, 0)
(115.44000244140625, 506.6366271972656, 332.69232177734375, 518.802734375, 'The overM1 system evMuation includes both in- \n', 6, 0)
(105.5999984741211, 517.9166870117188, 332.7662658691406, 574.7227783203125, 'trinsic and extrinsic evaluation. In the intrinsic evM- \nuation, we asked human subjects to compare the \nquality of extraction-based summaries and their re- \nvised versions produced by our sentence reduction \nand combination modules. \nWe selected 20 docu- \n', 7, 0)
(105.5999984741211, 573.836669921875, 332.9469299316406, 608.32275390625, 'ments; three different automatic summarizers were \nused to generate a summary for each document, pro- \nducing 60 summaries in total. \nThese summaries \n', 8, 0)
(105.5999984741211, 607.4366455078125, 333.66162109375, 730.4827270507812, 'are all extraction-based. We then ran our sentence \nreduction and sentence combination system to re- \nvise the summaries, producing a revised version for \neach summary. We presented human subjects with \nthe full documents, the extraction-based summaries, \nand their revised versions, and asked them to com- \npare the extraction-based summaries and their re- \nvised versions. The human subjects were asked to \nscore the conciseness of the summaries (extraction- \nbased or revised) based on a scale from 0 to 10 - \nthe higher the score, the more concise a summary is. \n', 9, 0)
(348.7200012207031, 416.6366271972656, 578.197021484375, 574.2427368164062, 'They were also asked to score the coherence of the \nsummaries based on a scale from 0 to 10. On aver- \nage, the extraction-based summaries have a score of \n4.2 for conciseness, while the revised summaries have \na score of 7.9 (an improvement of 88%). The average \nimprovement for the three systems are 78%, 105%, \nand 88% respectively. The revised summaries are \non average 41% shorter than the original extraction- \nbased summaries. For summary coherence, the aver- \nage score for the extraction-based summaries is 3.9, \nwhile the average score for the revised summaries is \n6.1 (an improvement of 56%). The average improve- \nment for the three systems are 69%, 57%, and 53% \nrespectively. \n', 10, 0)
(358.79998779296875, 574.7966918945312, 576.6553955078125, 586.9627685546875, 'We are preparing a task-based evaluation, in \n', 11, 0)
(348.9599914550781, 586.07666015625, 577.0186157226562, 642.8827514648438, "which we will use the data from the Summariza- \ntion EvMuation Conference (Mani et al., 1998) and \ncompare how our revised summaries can influence \nhumans' performance in tasks like text categoriza- \ntion and ad-hoc retrieval. \n", 12, 0)
(349.44000244140625, 656.1566772460938, 445.4216003417969, 668.32275390625, '6 \nRelated \nwork \n', 13, 0)
(349.20001220703125, 673.6766967773438, 576.9354248046875, 730.2427368164062, '(Mani et al., 1999) addressed the problem of revising \nsummaries to improve their quality. They suggested \nthree types of operations: elimination, aggregation, \nand smoothing. The goal of the elimination opera- \ntion is similar to that of the sentence reduction op- \n', 14, 0)
(329.760009765625, 744.5111694335938, 350.55780029296875, 758.0608520507812, '184 \n', 15, 0)

page suivante
(80.4000015258789, 84.43846893310547, 308.72003173828125, 253.32504272460938, 'eration in our system. The difference is that while \nelimination always removes parentheticals, sentence- \ninitial PPs and certain adverbial phrases for every \nextracted sentence, our sentence reduction module \naims to make reduction decisions according to each \ncase and removes a sentence component only if it \nconsiders it appropriate to do so. The goal of the \naggregation operation and the smoothing operation \nis similar to that of the sentence combination op- \neration in our system. However, the combination \noperations and combination rules that we derived \nfrom corpus analysis are significantly different from \nthose used in the above system, which mostly came \nfrom operations in traditional natural language gen- \neration. \n', 0, 0)
(81.36000061035156, 261.552001953125, 269.16119384765625, 278.1720275878906, '7 \nConclusions and future work \n', 1, 0)
(80.87999725341797, 279.5585021972656, 308.5079345703125, 450.1250305175781, 'This paper presents a novel architecture for text \nsummarization using cut and paste techniques ob- \nserved in human-written abstracts. In order to auto- \nmatically analyze a large quantity of human-written \nabstracts, we developed a decomposition program. \nThe automatic decomposition allows us to build \nlarge corpora for studying sentence reduction and \nsentence combination, which are two effective op- \nerations in cut and paste. We developed a sentence \nreduction module that makes reduction decisions us- \ning multiple sources of knowledge. We also investi- \ngated possible sentence combination operations and \nimplemented the combination module. A sentence \nextraction module was developed and used as the \nfront end of the summarization system. \n', 2, 0)
(91.19999694824219, 448.51849365234375, 307.76446533203125, 461.6450500488281, 'We are preparing the task-based evaluation of the \n', 3, 0)
(81.12000274658203, 459.7984924316406, 308.1399841308594, 517.5650024414062, 'overall system. We also plan to evaluate the porta- \nbility of the system by testing it on another corpus. \nWe will also extend the system to query-based sum- \nmarization and investigate whether the system can \nbe modified for multiple document summarization. \n', 4, 0)
(81.36000061035156, 525.7919921875, 182.10560607910156, 542.4119873046875, 'Acknowledgment \n', 5, 0)
(81.12000274658203, 544.2785034179688, 308.2077331542969, 601.8049926757812, 'We thank IBM for licensing us the ESG parser \nand the MITRE corporation for licensing us the co- \nreference resolution system. This material is based \nupon work supported by the National Science Foun- \ndation under Grant No. \nIRI 96-19124 and IRI \n', 6, 0)
(81.12000274658203, 599.718505859375, 308.824462890625, 646.2050170898438, '96-18797. Any opinions, findings, and conclusions \nor recommendations expressed in this material are \nthose of the authors and do not necessarily reflect \nthe views of the National Science Foundation. \n', 7, 0)
(81.36000061035156, 654.4320068359375, 146.49961853027344, 671.052001953125, 'References \n', 8, 0)
(81.36000061035156, 671.95849609375, 308.0289611816406, 685.0850219726562, 'ANSI. 1997. Guidelines for abstracts. Technical Re- \n', 9, 0)
(91.44000244140625, 683.2384643554688, 308.4901428222656, 706.925048828125, 'port Z39.14-1997, NISO Press, Bethesda, Mary- \nland. \n', 10, 0)
(81.83999633789062, 706.0385131835938, 308.5611877441406, 719.1650390625, 'L. Baum. 1972. An inequality and associated max- \n', 11, 0)
(91.44000244140625, 716.8385009765625, 309.5367431640625, 729.9650268554688, 'imization technique in statistical estimation of \n', 12, 0)
(332.6400146484375, 84.43846893310547, 550.5331420898438, 108.3863754272461, 'probabilistic functions of a markov process. In- \nequalities, (3):1-8. \n', 13, 0)
(323.0400085449219, 106.75847625732422, 550.1234130859375, 119.9063949584961, 'Edward T. Cremmins. 1982. The Art of Abstracting. \n', 14, 0)
(332.6400146484375, 117.79845428466797, 437.9700012207031, 130.92501831054688, 'ISI Press, Philadelphia. \n', 15, 0)
(322.79998779296875, 129.31846618652344, 550.1244506835938, 142.44503784179688, 'Brigitte Endres-Niggemeyer, Kai Haseloh, Jens \n', 16, 0)
(332.6400146484375, 140.59849548339844, 550.2711181640625, 164.76504516601562, 'Mfiller, Simone Peist, Irene Santini de Sigel, \nAlexander \nSigel, \nElisabeth \nWansorra, \nJan \n', 17, 0)
(332.8800048828125, 162.9185028076172, 548.8088989257812, 187.34640502929688, 'Wheeler, and Brfinja Wollny. 1998. Summarizing \nInformation. Springer, Berlin. \n', 18, 0)
(323.5199890136719, 185.71849060058594, 550.2581176757812, 198.84506225585938, 'Raya Fidel. 1986. Writing abstracts for free-text \n', 19, 0)
(332.8800048828125, 196.7584686279297, 550.0757446289062, 221.16506958007812, 'searching. Journal of Documentation, 42(1):11- \n21, March. \n', 20, 0)
(323.2799987792969, 219.55845642089844, 550.9199829101562, 232.68502807617188, 'Hongyan Jing and Kathleen R. McKeown. 1998. \n', 21, 0)
(333.1199951171875, 230.59849548339844, 550.9309692382812, 310.925048828125, 'Combining multiple, large-scale resources in a \nreusable lexicon for natural language generation. \nIn Proceedings of the 36th Annual Meeting of the \nAssociation for Computational Linguistics and the \n17th International Conference on Computational \nLinguistics, volume 1, pages 607-613, Universit6 \nde Montreal, Quebec, Canada, August. \n', 22, 0)
(323.5199890136719, 309.3184814453125, 552.159912109375, 322.4450378417969, 'Hongyan Jing and Kathleen R. McKeown. 1999. \n', 23, 0)
(333.1199951171875, 320.5984802246094, 551.814453125, 345.0263977050781, 'The decomposition of human-written summary \nsentences. \nIn Proceedings of the P2nd In- \n', 24, 0)
(333.3599853515625, 343.7056579589844, 552.2567138671875, 390.1250305175781, "ternational ACM SIGIR Conference on Re- \nsearch and Development in Information Re- \ntrieval(SIGIR'99), pages 129-136, University of \nBerkeley, CA, August. \n", 25, 0)
(323.5199890136719, 388.75848388671875, 550.4656372070312, 401.8850402832031, 'Hongyan Jing. 2000. Sentence reduction for au- \n', 26, 0)
(333.6000061035156, 399.7984924316406, 552.2089233398438, 424.4664001464844, 'tomatic text summarization. In Proceedings of \nANLP 2000. \n', 27, 0)
(323.5199890136719, 422.5984802246094, 549.2069702148438, 435.72503662109375, 'Aravind.K. Joshi. \n1987. Introduction to tree- \n', 28, 0)
(333.3599853515625, 433.87847900390625, 551.3954467773438, 469.5650329589844, 'adjoining grammars. In A. Manaster-Ramis, ed- \nitor, Mathematics of Language. John Benjamins, \nAmsterdam. \n', 29, 0)
(323.760009765625, 467.95849609375, 551.8512573242188, 481.0850524902344, 'Inderjeet Mani, David House, Gary Klein, Lynette \n', 30, 0)
(333.6000061035156, 479.2384948730469, 551.1832885742188, 525.9650268554688, 'Hirschman, Leo Obrst, Therese Firmin, Michael \nChrzanowski, and Beth Sundheim. 1998. The \nTIPSTER SUMMAC text summarization eval- \nuation final report. \nTechnical Report MTR \n', 31, 0)
(333.8399963378906, 524.1184692382812, 511.1165771484375, 537.2449951171875, '98W0000138, The MITRE Corporation. \n', 32, 0)
(324.0, 535.8784790039062, 551.2745361328125, 549.0050048828125, 'Inderjeet Mani, Barbara Gates, and Erie Bloedorn. \n', 33, 0)
(333.6000061035156, 546.678466796875, 551.4449462890625, 604.6849975585938, "1999. Improving summaries by revising them. In \nProceedings of the 37th Annual Meeting of the As- \nsociation for Computational Linguistics(A CL '99), \npages 558-565, University of Maryland, Mary- \nland, June. \n", 34, 0)
(324.239990234375, 603.0784912109375, 551.3021240234375, 616.2263793945312, 'Michael MeCord, 1990. English Slot Grammar. \n', 35, 0)
(333.6000061035156, 614.3584594726562, 358.2677307128906, 627.4849853515625, 'IBM. \n', 36, 0)
(324.0, 625.8784790039062, 552.1034545898438, 639.0263671875, 'Samuel Thurber, editor. 1924. Prgcis Writing for \n', 37, 0)
(333.8399963378906, 637.1585083007812, 549.6123046875, 661.5650024414062, 'American Schools. The Atlantic Monthly Press, \nINC., Boston. \n', 38, 0)
(324.239990234375, 659.718505859375, 552.655517578125, 672.8450317382812, 'A.J. Viterbi. 1967. Error bounds for convolution \n', 39, 0)
(333.8399963378906, 670.9984741210938, 552.1365966796875, 706.4663696289062, 'codes and an asymptotically optimal decoding al- \ngorithm. IEEE Transactions on Information The- \nory, 13:260-269. \n', 40, 0)
(302.3999938964844, 741.803955078125, 323.5704650878906, 756.6755981445312, '185 \n', 41, 0)

page suivante
