(63.757999420166016, 69.74044036865234, 548.244384765625, 110.40362548828125, 'Skill2vec: Machine Learning Approach for Determining the Relevant\nSkills from Job Description\n', 0, 0)
(85.06900787353516, 126.22401428222656, 523.0962524414062, 152.26229858398438, 'Le Van-Duyet\nme@duyet.net\nVo Minh Quan\n95.vominhquan@gmail.com\nDang Quang An\nan.dang1390@gmail.com\n', 1, 0)
(53.99998474121094, 172.9454345703125, 298.80078125, 304.1986389160156, 'Abstract— Unsupervise learned word embeddings have seen\ntremendous success in numerous Natural Language Processing\n(NLP) tasks in recent years. The main contribution of this\npaper is to develop a technique called Skill2vec, which applies\nmachine learning techniques in recruitment to enhance the\nsearch strategy to ﬁnd candidates possessing the appropriate\nskills. Skill2vec is a neural network architecture inspired by\nWord2vec, developed by Mikolov et al. in 2013. It transforms\nskills to new vector space, which has the characteristics of\ncalculation and presents skills relationships. We conducted an\nexperiment evaluation manually by a recruitment company’s\ndomain experts to demonstrate the effectiveness of our ap-\nproach.\n', 2, 0)
(130.33798217773438, 311.72662353515625, 222.4620361328125, 323.7315673828125, 'I. INTRODUCTION\n', 3, 0)
(53.99998092651367, 327.6046142578125, 298.80133056640625, 483.2024230957031, 'Recruiters in information technology domain have met the\nproblem ﬁnding appropriate candidates by their skills. In the\nresume, the candidate may describe one skill in different\nways or skills could be replaced by others. The recruiters\nmay not have the domain knowledge to know if one’s skills\nare ﬁt or not, so they can only ﬁnd ones with matched skills.\nIn order to cope with the problem, one should try to\nﬁnd the relatedness of skills. There are some approaches:\nbuilding a dictionary manually, ontology approach, natural\nlanguage processing methods, etc. In this study, we apply a\nword embedding method Word2Vec, using skills from online\njob post descriptions. We treat skills as terms, job posts as\ndocuments and ﬁnd the relatedness of these skills.\n', 4, 0)
(127.5579833984375, 490.98846435546875, 225.24118041992188, 502.993408203125, 'II. RELATED WORK\n', 5, 0)
(53.99998092651367, 506.866455078125, 298.8014221191406, 734.3264770507812, 'To ﬁnd relatedness of skills, Simon Hughes [4] from Dice\nproposed an approach using Latent Semantic Analysis with\nan assumption that skills are related to skills which occur\nin the same context, and here contexts are job posts. This\napproach will build a term-document matrix and use Singular\nValue Decomposition to reduce the dimensionality. The cons\nof this approach is that when we have new data coming, we\ncan not update the old term-document matrix, this leads to\ndifﬁculties in maintaining the model, as the change of trend\nin this domain is high.\nGoogle’s Data Scientists also face the same problems\nin Cloud Jobs API [7]. Their solution is to build a skill\nontology deﬁning around 50,000 skills in 30 job domain with\nrelationships such as is a, related to, etc. This approach can\nrepresent complicate relationships between skills and jobs,\nbut building such an ontology costs so much time and effort.\nTo overcome the problem of relevant term, [3] present a\nnew, effective log-based approach to relevant term extraction\nand term suggestion.\n', 6, 0)
(313.1999816894531, 172.383544921875, 558.0012817382812, 316.59857177734375, 'The goal of [2] is to develop an automated system that\ndiscovers additional names for an entity given just one of\nits names, using Latent semantic analysis (LSA) [1]. In the\nexample of authors, the city in India referred to as Bombay\nin some documents may be referred to as Mumbai in others\nbecause its name ofﬁcially changed from the former to the\nlatter in 1995.\n[5] is the introduction of an ontology-based skills man-\nagement system at Swiss Life and the lessons learned from\nthe utilisation of the methodology, present a methodology\nfor application-driven development of ontologies that is\ninstantiated by a case study.\n', 7, 0)
(354.3799743652344, 329.2486267089844, 516.8202514648438, 341.2535705566406, 'III. WORD2VEC ARCHITECTURE\n', 8, 0)
(313.199951171875, 348.5606384277344, 558.00146484375, 648.8954467773438, 'Word2Vec is a group of models proposed by Mikolov et al\nin 2013 [6]. It consists of 2 models: continuous bag-of-words\nand continuous Skip-gram, both are shallow neural networks\nthat try to learn distributed representations of words with\nthe target is to maximize the accuracy while minimizing the\ncomputational complexity. In the continuous bag-of-words\narchitecture, the model predicts the current word from a\nwindow of surrounding context words. On the other hand,\nSkip-gram model try to predict surrounding context words\nbased on the current word. In this work, we focus on Skip-\ngram model as it is known to be better with infrequent words\nand it also give slightly better result in our experiment.\nThe model consists of three layers: input layer, one hidden\nlayer and output layer. The input layer take a word encoded\nusing 1-of-V encoding (also known as one-hot encoding),\nwhere V is size of the dictionary. The word is then fed\nthrough the linear hidden layer to the output layer, which\nis a softmax classiﬁer. The output layer will try to predict\nwords within window size before and after current word.\nUsing stochastic gradient descent and back propagation, we\ntrain the model until it converges.\nThis model takes vector dimensionality and window size\nas parameters. The author found that increasing the window\nsize improves the quality of the word vector, and yet it\nincreases the computational complexity.\n', 9, 0)
(392.1729431152344, 661.5454711914062, 479.033447265625, 673.5504150390625, 'IV. METHODOLOGY\n', 10, 0)
(313.199951171875, 680.8574829101562, 454.1009216308594, 692.7528076171875, 'A. Data collecting and processing\n', 11, 0)
(313.199951171875, 698.4114990234375, 558.0014038085938, 734.326416015625, 'Choosing a universal data set for the model is extremely\nimportant, the data should be large enough and should have\nbalanced distributions over words (i.e. skills).\n', 12, 0)
(10.940000534057617, 219.47998046875, 37.619998931884766, 555.0, 'arXiv:1707.09751v3  [cs.CL]  9 Oct 2019\n', 13, 0)

page suivante
(54.0, 52.83154296875, 298.8013610839844, 88.74748992919922, 'There are two dataset we need to concern, one (1) is the\nstandard skills dictionary for the parser and another (2) is\nskills for training model; follow the ﬁgure 1.\n', 0, 0)
(88.18800354003906, 102.18353271484375, 284.4384765625, 128.5354461669922, 'Career\nwebsites\n(1) Standard skills\ncollect\ncleaning\n', 1, 0)
(88.18800354003906, 173.049560546875, 279.7268371582031, 198.19248962402344, 'Career\nwebsites\nJob descriptions\ncollect\n', 2, 0)
(231.74099731445312, 142.6375732421875, 275.1336364746094, 155.9674530029297, '(*) parser\n', 3, 0)
(222.62701416015625, 245.111572265625, 271.5234680175781, 269.071533203125, '(2) Training\ndata\n', 4, 0)
(93.00900268554688, 281.8956298828125, 259.791259765625, 291.4996032714844, 'Fig. 1.\nPipeline of data collecting and processing\n', 5, 0)
(54.0, 304.32257080078125, 298.80133056640625, 376.10345458984375, 'First, we collected and prepared a large dictionary of\nskills. With this dictionary, we can extract a set of skills\nfrom raw job descriptions. Skillss need to be cleaned into\nunique skills because there are many way to present a skill in\njob description (i.e. OOP or Object-oriented programming).\nFigure 2 brieﬂy depicts the concept of the cleaning process.\n', 6, 0)
(167.343994140625, 390.6655578613281, 185.45599365234375, 414.62548828125, 'Raw\nskill\n', 7, 0)
(152.47799682617188, 432.2345275878906, 200.31838989257812, 456.1954650878906, '(1) Remove\npunctuation\n', 8, 0)
(147.62698364257812, 474.7545471191406, 205.1709747314453, 498.7144775390625, '(2) Dictionary\nreplace\n', 9, 0)
(153.23599243164062, 517.2745361328125, 199.5620880126953, 541.2344970703125, '(3) Regular\nexpression\n', 10, 0)
(137.52499389648438, 559.7935791015625, 215.2731475830078, 583.7545166015625, '(4) Lemmatization,\nStemming\n', 11, 0)
(160.0760040283203, 603.3345947265625, 192.72344970703125, 627.2945556640625, 'Cleaned\nskill\n', 12, 0)
(122.3550033569336, 640.1185913085938, 230.44544982910156, 649.7225952148438, 'Fig. 2.\nPipeline cleaning skills\n', 13, 0)
(54.000003814697266, 662.5465087890625, 298.80133056640625, 734.3274536132812, 'After that, we had the dictionary of skills ready for\nparsing. We collected a huge number of job descriptions\nfrom Dice.com - one of the most popular career website\nabout Tech jobs in USA. From these job descriptions, we\nextract skills for each one by using our skills dictionary\n(1). Now, the dataset is presented by a list of collections\n', 14, 0)
(313.20001220703125, 52.83251953125, 558.0010375976562, 124.61351776123047, 'of skills based on job descriptions. After crawling, we got\na total of 5GB with more than 1,400,000 job descriptions.\nFrom these data, we extracted skills and performed as a\nlist of skills in the same context, the context here includes\nskills in the same job description. The dataset is published\nat https://github.com/duyetdev/skill2vec-dataset\n', 15, 0)
(323.1620178222656, 136.51361083984375, 479.85369873046875, 148.51853942871094, 'The data structure is shown in table I.\n', 16, 0)
(405.1390380859375, 155.74365234375, 466.0690612792969, 177.30264282226562, 'TABLE I\nDATA STRUCTURE\n', 17, 0)
(357.70001220703125, 189.56658935546875, 488.5892028808594, 199.17056274414062, 'Job description\nContext skills\n', 18, 0)
(375.9200134277344, 203.41461181640625, 513.4992065429688, 213.01858520507812, 'JD1\nJava, Spark, Hadoop, Python\n', 19, 0)
(375.9200134277344, 217.2625732421875, 487.4894104003906, 226.86654663085938, 'JD2\nPython, Hive\n', 20, 0)
(375.9200134277344, 231.110595703125, 498.4560546875, 240.71456909179688, 'JD3\nPython, Flask, SQL\n', 21, 0)
(377.3999938964844, 246.11428833007812, 471.15191650390625, 259.9344482421875, '· · ·\n· · ·\n', 22, 0)
(313.20001220703125, 277.9895324707031, 414.021484375, 289.8848876953125, 'B. Skill2vec architecture\n', 23, 0)
(313.20001220703125, 293.0775451660156, 558.0011596679688, 424.6343688964844, 'In this paper, for training the dataset, we used a neural\nnetwork inspired by Word2Vec model as mentioned above.\nHere we treated our skills as words in Word2Vec model.\nIn this study, with the documents contain only the skills,\nwe chose the maximum window size, implied that every\nskills in the same job description are related to each other.\nFor the vector dimensions, after some point, adding more\ndimensions provides diminishing improvements, so we chose\nthis parameter empirically. To honour the work of Word2Vec\nmodel as it holds a big part in our study, we name our model\nSkill2Vec. Figure 3 brieﬂy describes our Skill2Vec model.\n', 24, 0)
(385.5199890136719, 710.506591796875, 485.6802673339844, 720.110595703125, 'Fig. 3.\nSkill2vec architecture\n', 25, 0)

page suivante
(111.03500366210938, 52.83154296875, 241.7640838623047, 64.83647918701172, 'V. EXPERIMENTAL SETUP\n', 0, 0)
(54.000003814697266, 69.23956298828125, 298.8014831542969, 93.19951629638672, 'To evaluate our method, we have an expert team assesses\nthe result following these steps:\n', 1, 0)
(54.000003814697266, 95.60455322265625, 298.8014831542969, 181.79551696777344, '1) Pick 200 skills randomly from our dictionary.\n2) Our system will return top 5 ”nearest” skills for each.\n3) The expert team will check if these top 5 ”nearest”\nskills are relevant or not.\nThe experiment showed that 78% of skills returned by\nour model is truly relevant to the input skill. We present the\nexperimental results in table II\n', 2, 0)
(121.62200927734375, 191.65167236328125, 231.18756103515625, 213.21066284179688, 'TABLE II\nQUERY TOP 5 RELEVANT SKILLS\n', 3, 0)
(122.26000213623047, 226.78961181640625, 230.54132080078125, 236.39358520507812, 'Query skill\nTop relevant skills\n', 4, 0)
(127.1969985961914, 266.6396484375, 153.7613525390625, 276.2436218261719, 'HTML5\n', 5, 0)
(193.73500061035156, 240.63763427734375, 207.4595184326172, 250.24160766601562, 'css3\n', 6, 0)
(185.76499938964844, 254.08660888671875, 215.4296875, 263.6905822753906, 'bootstrap\n', 7, 0)
(185.66099548339844, 267.53662109375, 215.53282165527344, 277.1405944824219, 'front end\n', 8, 0)
(188.64599609375, 280.9856262207031, 212.54830932617188, 290.589599609375, 'angular\n', 9, 0)
(183.71299743652344, 294.43560791015625, 217.4822998046875, 304.0395812988281, 'responsive\n', 10, 0)
(132.50900268554688, 334.28662109375, 148.44920349121094, 343.8905944824219, 'OOP\n', 11, 0)
(191.96600341796875, 308.28363037109375, 209.22923278808594, 317.8876037597656, 'OOD\n', 12, 0)
(185.26300048828125, 321.7336120605469, 215.9319305419922, 331.33758544921875, 'Objective\n', 13, 0)
(193.69500732421875, 335.1826171875, 207.4992218017578, 344.7865905761719, 'Java\n', 14, 0)
(181.77999877929688, 348.63262939453125, 219.41477966308594, 358.2366027832031, 'Multithread\n', 15, 0)
(174.31199645996094, 362.0816345214844, 226.88275146484375, 371.68560791015625, 'Software Debug\n', 16, 0)
(127.86199951171875, 401.9326171875, 153.09532165527344, 411.5365905761719, 'Hadoop\n', 17, 0)
(195.281005859375, 375.92962646484375, 205.91311645507812, 385.5335998535156, 'Pig\n', 18, 0)
(193.00999450683594, 389.3796081542969, 208.18505859375, 398.98358154296875, 'Hive\n', 19, 0)
(189.97300720214844, 402.82861328125, 211.22129821777344, 412.4325866699219, 'HBase\n', 20, 0)
(185.9199981689453, 416.27862548828125, 215.27386474609375, 425.8825988769531, 'Big Data\n', 21, 0)
(191.3000030517578, 429.7276306152344, 209.8942413330078, 439.33160400390625, 'Spark\n', 22, 0)
(131.84800720214844, 469.57861328125, 149.1112518310547, 479.1825866699219, 'Scala\n', 23, 0)
(183.5970001220703, 443.5766296386719, 217.59744262695312, 453.18060302734375, 'Zookeeper\n', 24, 0)
(191.3000030517578, 457.025634765625, 209.8942413330078, 466.6296081542969, 'Spark\n', 25, 0)
(179.9429931640625, 470.4756164550781, 221.2520294189453, 480.07958984375, 'Data System\n', 26, 0)
(190.41099548339844, 483.92462158203125, 210.78256225585938, 493.5285949707031, 'Sqoop\n', 27, 0)
(185.76499938964844, 497.3746337890625, 215.4296875, 506.9786071777344, 'solrcloud\n', 28, 0)
(132.89199829101562, 537.224609375, 148.0670623779297, 546.82861328125, 'Hive\n', 29, 0)
(195.281005859375, 511.22265625, 205.91311645507812, 520.82666015625, 'Pig\n', 30, 0)
(190.41099548339844, 524.671630859375, 210.78257751464844, 534.275634765625, 'HDFS\n', 31, 0)
(187.97999572753906, 538.12158203125, 213.21331787109375, 547.7255859375, 'Hadoop\n', 32, 0)
(191.3000030517578, 551.5706176757812, 209.8942413330078, 561.1746215820312, 'Spark\n', 33, 0)
(189.531005859375, 565.0206298828125, 211.66397094726562, 574.6246337890625, 'Impala\n', 34, 0)
(132.3209991455078, 598.3175659179688, 220.4799346923828, 610.322509765625, 'VI. CONCLUSION\n', 35, 0)
(53.999996185302734, 614.7245483398438, 298.8013916015625, 734.3264770507812, 'In this paper, we developed a relationship network between\nskills in recruitment domain by using the neural net inspired\nby Word2vec model. We observed that it is possible to\ntrain high quality word vectors using very simple model\narchitectures due to lower cost of computation. Moreover,\nit is possible to compute very accurate high dimensional\nword vectors from a much larger dataset. Using Skip-gram\narchitecture and an advanced technique for preprocessing\ndata, the result seems to be impressive. The result of our\nwork can contribute to building the matching system between\n', 36, 0)
(313.20001220703125, 52.83154296875, 558.0023803710938, 148.52354431152344, 'candidates and job post. In the other hand, candidates can\nﬁnd the gap between the job post requirements and their\nability, so they can ﬁnd the suitable trainings.\nA direction we can follow in the future: adding domain\nin training model, for example: Between Python, Java, and\nR, in Data Science domain, Python and R are more relevant\nthan Java, however in Back End domain, Python and Java\nare more relevant than R.\n', 37, 0)
(407.705078125, 155.1986083984375, 463.5054931640625, 167.2035369873047, 'REFERENCES\n', 38, 0)
(313.2000732421875, 170.2916259765625, 558.0025024414062, 218.16160583496094, '[1]\nMichael W Berry and Ricardo D Fierro. “Low-rank\northogonal decompositions for information retrieval ap-\nplications”. In: Numerical linear algebra with applica-\ntions 3.4 (1996), pp. 301–327.\n', 39, 0)
(313.2001037597656, 218.1126708984375, 558.00146484375, 254.0276336669922, '[2]\nVinay Bhat et al. “Finding aliases on the web using\nlatent semantic analysis”. In: Data & Knowledge Engi-\nneering 49.2 (2004), pp. 129–143.\n', 40, 0)
(313.2001037597656, 253.97772216796875, 558.0010986328125, 313.80364990234375, '[3]\nChien-Kang Huang, Lee-Feng Chien, and Yen-Jen\nOyang. “Relevant term suggestion in interactive web\nsearch based on contextual information in query session\nlogs”. In: Journal of the Association for Information\nScience and Technology 54.7 (2003), pp. 638–649.\n', 41, 0)
(313.2001037597656, 313.7536926269531, 557.999755859375, 361.6246032714844, '[4]\nSimon Hughes. How We Data-Mine Related Tech Skills.\n2015. URL: http : / / insights . dice . com /\n2015/03/16/how-we-data-mine-related-\ntech-skills/ (visited on 09/12/2017).\n', 42, 0)
(313.2001953125, 361.57464599609375, 558.0012817382812, 409.4445495605469, '[5]\nThorsten Lau and York Sure. “Introducing ontology-\nbased skills management at a large insurance com-\npany”. In: Proceedings of the Modellierung. 2002,\npp. 123–134.\n', 43, 0)
(313.2002258300781, 409.39459228515625, 558.0013427734375, 469.2204895019531, '[6]\nTomas\nMikolov\net\nal.\n“Efﬁcient\nEstimation\nof\nWord Representations in Vector Space”. In: CoRR\nabs/1301.3781 (2013). URL: http://dblp.uni-\ntrier.de/db/journals/corr/corr1301.\nhtml#abs-1301-3781.\n', 44, 0)
(313.20025634765625, 469.1705322265625, 558.002197265625, 540.951416015625, '[7]\nChristian Posse. Cloud Jobs API: machine learning\ngoes to work on job search and discovery. 2016. URL:\nhttps: // cloud. google. com/ blog/big -\ndata/2016/11/cloud-jobs-api-machine-\nlearning-goes-to-work-on-job-search-\nand-discovery (visited on 03/03/2018).\n', 45, 0)

page suivante
