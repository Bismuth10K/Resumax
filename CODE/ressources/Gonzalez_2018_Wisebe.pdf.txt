(105.21900177001953, 52.88939666748047, 347.76580810546875, 85.17841339111328, 'WiSeBE: Window-Based Sentence\nBoundary Evaluation\n', 0, 0)
(58.91400146484375, 108.5795669555664, 393.5630798339844, 124.43266296386719, 'Carlos-Emiliano Gonz´alez-Gallardo1,2(B) and Juan-Manuel Torres-Moreno1,2\n', 1, 0)
(57.582000732421875, 132.29022216796875, 395.30450439453125, 201.8448486328125, '1 LIA - Universit´e d’Avignon et des Pays de Vaucluse, 339 chemin des Meinajaries,\n84140 Avignon, France\ncarlos-emiliano.gonzalez-gallardo@alumni.univ-avignon.fr,\njuan-manuel.torres@univ-avignon.fr\n2 D´epartement de GIGL, ´Ecole Polytechnique de Montr´eal,\nC.P. 6079, succ. Centre-ville, Montr´eal, Qu´ebec H3C 3A7, Canada\n', 2, 0)
(81.91744995117188, 228.5458984375, 371.0433044433594, 390.93994140625, 'Abstract. Sentence Boundary Detection (SBD) has been a major\nresearch topic since Automatic Speech Recognition transcripts have been\nused for further Natural Language Processing tasks like Part of Speech\nTagging, Question Answering or Automatic Summarization. But what\nabout evaluation? Do standard evaluation metrics like precision, recall,\nF-score or classiﬁcation error; and more important, evaluating an auto-\nmatic system against a unique reference is enough to conclude how well\na SBD system is performing given the ﬁnal application of the transcript?\nIn this paper we propose Window-based Sentence Boundary Evaluation\n(WiSeBE), a semi-supervised metric for evaluating Sentence Boundary\nDetection systems based on multi-reference (dis)agreement. We evalu-\nate and compare the performance of diﬀerent SBD systems over a set\nof Youtube transcripts using WiSeBE and standard metrics. This dou-\nble evaluation gives an understanding of how WiSeBE is a more reliable\nmetric for the SBD task.\n', 3, 0)
(81.91744995117188, 407.46282958984375, 307.7750244140625, 439.1672668457031, 'Keywords: Sentence Boundary Detection · Evaluation\nTranscripts · Human judgment\n', 4, 0)
(53.57699966430664, 450.2146911621094, 147.771240234375, 462.1817626953125, '1\nIntroduction\n', 5, 0)
(53.57491683959961, 471.56829833984375, 399.46258544921875, 603.2609252929688, 'The goal of Automatic Speech Recognition (ASR) is to transform spoken data\ninto a written representation, thus enabling natural human-machine interaction\n[33] with further Natural Language Processing (NLP) tasks. Machine transla-\ntion, question answering, semantic parsing, POS tagging, sentiment analysis and\nautomatic text summarization; originally developed to work with formal writ-\nten texts, can be applied over the transcripts made by ASR systems [2,25,31].\nHowever, before applying any of these NLP tasks a segmentation process called\nSentence Boundary Detection (SBD) should be performed over ASR transcripts\nto reach a minimal syntactic information in the text.\nTo measure the performance of a SBD system, the automatically segmented\ntranscript is evaluated against a single reference normally done by a human. But\n', 6, 0)
(53.57694625854492, 608.1136474609375, 304.4107360839844, 632.2485961914062, 'c\n⃝ Springer Nature Switzerland AG 2018\nI. Batyrshin et al. (Eds.): MICAI 2018, LNAI 11289, pp. 119–131, 2018.\nhttps://doi.org/10.1007/978-3-030-04497-8_10\n', 7, 0)

page suivante
(39.402000427246094, 31.01311492919922, 275.447021484375, 39.979515075683594, '120\nC.-E. Gonz´alez-Gallardo and J.-M. Torres-Moreno\n', 0, 0)
(39.4019775390625, 53.99528884887695, 385.2965393066406, 197.64012145996094, 'given a transcript, does it exist a unique reference? Or, is it possible that the\nsame transcript could be segmented in ﬁve diﬀerent ways by ﬁve diﬀerent people\nin the same conditions? If so, which one is correct; and more important, how\nto fairly evaluate the automatically segmented transcript? These questions are\nthe foundations of Window-based Sentence Boundary Evaluation (WiSeBE), a\nnew semi-supervised metric for evaluating SBD systems based on multi-reference\n(dis)agreement.\nThe rest of this article is organized as follows. In Sect. 2 we set the frame of\nSBD and how it is normally evaluated. WiSeBE is formally described in Sect. 3,\nfollowed by a multi-reference evaluation in Sect. 4. Further analysis of WiSeBE\nand discussion over the method and alternative multi-reference evaluation is\npresented in Sect. 5. Finally, Sect. 6 concludes the paper.\n', 1, 0)
(39.402000427246094, 218.06861877441406, 235.00538635253906, 230.03567504882812, '2\nSentence Boundary Detection\n', 2, 0)
(39.39886474609375, 241.28517150878906, 385.28851318359375, 384.93878173828125, 'Sentence Boundary Detection (SBD) has been a major research topic science\nASR moved to more general domains as conversational speech [17,24,26]. Per-\nformance of ASR systems has improved over the years with the inclusion and\ncombination of new Deep Neural Networks methods [5,9,33]. As a general rule,\nthe output of ASR systems lacks of any syntactic information such as capital-\nization and sentence boundaries, showing the interest of ASR systems to obtain\nthe correct sequence of words with almost no concern of the overall structure of\nthe document [8].\nSimilar to SBD is the Punctuation Marks Disambiguation (PMD) or Sentence\nBoundary Disambiguation. This task aims to segment a formal written text into\nwell formed sentences based on the existent punctuation marks [11,19,20,29]. In\nthis context a sentence is deﬁned (for English) by the Cambridge Dictionary1\n', 3, 0)
(39.402000427246094, 384.7542724609375, 51.06621170043945, 396.898681640625, 'as:\n', 4, 0)
(53.675418853759766, 406.66900634765625, 370.9593200683594, 442.7266540527344, '“a group of words, usually containing a verb, that expresses a thought in\nthe form of a statement, question, instruction, or exclamation and starts\nwith a capital letter when written”.\n', 5, 0)
(39.40096664428711, 452.4969787597656, 385.2665710449219, 488.55462646484375, 'PMD carries certain complications, some given the ambiguity of punctuation\nmarks within a sentence. A period can denote an acronym, an abbreviation, the\nend of the sentence or a combination of them as in the following example:\n', 6, 0)
(66.7612533569336, 498.324951171875, 357.9133605957031, 522.4215087890625, 'The U.S. president, Mr. Donald Trump, is meeting with the F.B.I.\ndirector Christopher A. Wray next Thursday at 8 p.m.\n', 7, 0)
(39.40196228027344, 532.2008056640625, 385.2955017089844, 580.2105712890625, 'However its diﬃculties, DPM proﬁts of morphological and lexical information\nto achieve a correct sentence segmentation. By contrast, segmenting an ASR\ntranscript should be done without any (or almost any) lexical information and\na ﬂurry deﬁnition of sentence.\n', 8, 0)
(42.13800048828125, 590.2371215820312, 188.2168731689453, 603.0106201171875, '1 https://dictionary.cambridge.org/.\n', 9, 0)

page suivante
(139.08599853515625, 31.01311492919922, 399.35009765625, 39.979515075683594, 'WiSeBE: Window-Based Sentence Boundary Evaluation\n121\n', 0, 0)
(53.577083587646484, 53.99528884887695, 399.4366760253906, 137.8705291748047, 'The obvious division in spoken language may be considered speaker utter-\nances. However, in a normal conversation or even in a monologue, the way ideas\nare organized diﬀers largely from written text. This diﬀerences, added to disﬂu-\nencies like revisions, repetitions, restarts, interruptions and hesitations make the\ndeﬁnition of a sentence unclear thus complicating the segmentation task [27].\nTable 1 exempliﬁes some of the diﬃculties that are present when working with\nspoken language.\n', 1, 0)
(128.53799438476562, 158.44424438476562, 324.40625, 167.41064453125, 'Table 1. Sentence Boundary Detection example\n', 2, 0)
(83.82599639892578, 181.00723266601562, 334.464599609375, 189.9736328125, 'Speech transcript\nSBD applied to transcript\n', 3, 0)
(83.82599639892578, 195.37124633789062, 221.54994201660156, 292.0093078613281, 'two two women can look out after\na kid so bad as a man and a\nwoman can so you can have a you\ncan have a mother and a father\nthat that still don’t do right with\nthe kid and you can have to men\nthat can so as long as the love\neach other as long as they love\neach other it doesn’t matter\n', 4, 0)
(229.67999267578125, 195.37124633789062, 365.9630126953125, 302.97161865234375, 'two // two women can look out\nafter a kid so bad as a man and a\nwoman can // so you can have a\n// you can have a mother and a\nfather that // that still don’t do\nright with the kid and you can\nhave to men that can // so as\nlong as the love each other // as\nlong as they love each other it\ndoesn’t matter//\n', 5, 0)
(53.57709503173828, 330.3672790527344, 399.37884521484375, 354.4727783203125, 'Stolcke and Shriberg [26] considered a set of linguistic structures as segments\nincluding the following list:\n', 6, 0)
(56.56488037109375, 363.1990051269531, 261.7027587890625, 422.46649169921875, '– Complete sentences\n– Stand-alone sentences\n– Disﬂuent sentences aborted in mid-utterance\n– Interjections\n– Back-channel responses.\n', 7, 0)
(53.57709503173828, 431.36407470703125, 399.4585876464844, 586.9697265625, 'In [17], Meteer and Iyer divided speaker utterances into segments, consisting\neach of a single independent clause. A segment was considered to begin either\nat the beginning of an utterance, or after the end of the preceding segment. Any\ndysﬂuency between the end of the previous segments and the begging of current\none was considered part of the current segments.\nRott and ˇCerva [23] aimed to summarize news delivered orally segmenting the\ntranscripts into “something that is similar to sentences”. They used a syntactic\nanalyzer to identify the phrases within the text.\nA wide study focused in unbalanced data for the SBD task was performed\nby Liu et al. [15]. During this study they followed the segmentation scheme pro-\nposed by the Linguistic Data Consortium2 on the Simple Metadata Annotation\nSpeciﬁcation V5.0 guideline (SimpleMDE V5.0) [27], dividing the transcripts in\nSemantic Units.\n', 8, 0)
(56.3129997253418, 590.2371215820312, 181.8534393310547, 603.0106201171875, '2 https://www.ldc.upenn.edu/.\n', 9, 0)

page suivante
(39.402000427246094, 31.01311492919922, 275.447021484375, 39.979515075683594, '122\nC.-E. Gonz´alez-Gallardo and J.-M. Torres-Moreno\n', 0, 0)
(39.401092529296875, 53.99528884887695, 385.2488098144531, 161.77479553222656, 'A Semantic Unit (SU) is considered to be an atomic element of the transcript\nthat manages to express a complete thought or idea on the part of the speaker\n[27]. Sometimes a SU corresponds to the equivalent of a sentence in written text,\nbut other times (the most part of them) a SU corresponds to a phrase or a single\nword.\nSUs seem to be an inclusive conception of a segment, they embrace diﬀerent\nprevious segment deﬁnitions and are ﬂexible enough to deal with the majority\nof spoken language troubles. For these reasons we will adopt SUs as our segment\ndeﬁnition.\n', 1, 0)
(39.401092529296875, 179.74832153320312, 219.85768127441406, 191.66358947753906, '2.1\nSentence Boundary Evaluation\n', 2, 0)
(39.393943786621094, 199.4453887939453, 385.28155517578125, 594.156494140625, 'SBD research has been focused on two diﬀerent aspects; features and methods.\nRegarding the features, some work focused on acoustic elements like pauses\nduration, fundamental frequencies, energy, rate of speech, volume change and\nspeaker turn [10,12,14].\nThe other kind of features used in SBD are textual or lexical features. They\nrely on the transcript content to extract features like bag-of-word, POS tags or\nword embeddings [7,12,16,18,23,26,30]. Mixture of acoustic and lexical features\nhave also been explored [1,13,14,32], which is advantageous when both audio\nsignal and transcript are available.\nWith respect to the methods used for SBD, they mostly rely on statisti-\ncal/neural machine translation [12,22], language models [8,15,18,26], conditional\nrandom ﬁelds [16,30] and deep neural networks [3,7,29].\nDespite their diﬀerences in features and/or methodology, almost all previous\ncited research share a common element; the evaluation methodology. Metrics as\nPrecision, Recall, F1-score, Classiﬁcation Error Rate and Slot Error Rate (SER)\nare used to evaluate the proposed system against one reference. As discussed\nin Sect. 1, further NLP tasks rely on the result of SBD, meaning that is crucial\nto have a good segmentation. But comparing the output of a system against a\nunique reference will provide a reliable score to decide if the system is good or\nbad?\nBohac et al. [1] compared the human ability to punctuate recognized spon-\ntaneous speech. They asked 10 people (correctors) to punctuate about 30 min of\nASR transcripts in Czech. For an average of 3,962 words, the punctuation marks\nplaced by correctors varied between 557 and 801; this means a diﬀerence of 244\nsegments for the same transcript. Over all correctors, the absolute consensus for\nperiod (.) was only 4.6% caused by the replacement of other punctuation marks\nas semicolons (;) and exclamation marks (!). These results are understandable if\nwe consider the diﬃculties presented previously in this section.\nTo our knowledge, the amount of studies that have tried to target the sentence\nboundary evaluation with a multi-reference approach is very small. In [1], Bohac\net al. evaluated the overall punctuation accuracy for Czech in a straightforward\nmulti-reference framework. They considered a period (.) valid if at least ﬁve of\ntheir 10 correctors agreed on its position.\n', 3, 0)

page suivante
(139.08599853515625, 31.01311492919922, 399.35009765625, 39.979515075683594, 'WiSeBE: Window-Based Sentence Boundary Evaluation\n123\n', 0, 0)
(53.57607650756836, 53.99528884887695, 399.4556579589844, 209.60121154785156, 'Kol´aˇr and Lamel [13] considered two independent references to evaluate their\nsystem and proposed two approaches. The ﬁst one was to calculate the SER for\neach of one the two available references and then compute their mean. They\nfound this approach to be very strict because for those boundaries where no\nagreement between references existed, the system was going to be partially wrong\neven the fact that it has correctly predicted the boundary. Their second app-\nroach tried to moderate the number of unjust penalizations. For this case, a\nclassiﬁcation was considered incorrect only if it didn’t match either of the two\nreferences.\nThese two examples exemplify the real need and some straightforward solu-\ntions for multi-reference evaluation metrics. However, we think that it is possible\nto consider in a more inclusive approach the similarities and diﬀerences that mul-\ntiple references could provide into a sentence boundary evaluation protocol.\n', 1, 0)
(53.57699966430664, 230.0207061767578, 347.50506591796875, 241.98776245117188, '3\nWindow-Based Sentence Boundary Evaluation\n', 2, 0)
(53.57699966430664, 253.2462615966797, 399.4342346191406, 338.61468505859375, 'Window-Based Sentence Boundary Evaluation (WiSeBE) is a semi-automatic\nmulti-reference sentence boundary evaluation protocol which considers the per-\nformance of a candidate segmentation over a set of segmentation references and\nthe agreement between those references.\nLet R = {R1, R2, ..., Rm} be the set of all available references given a tran-\nscript T = {t1, t2, ..., tn}, where tj is the jth word in the transcript; a reference\nRi is deﬁned as a binary vector in terms of the existent SU boundaries in T.\n', 3, 0)
(185.0387725830078, 348.8891906738281, 399.3808898925781, 368.1070251464844, 'Ri = {b1, b2, ..., bn}\n(1)\n', 4, 0)
(68.51797485351562, 366.81689453125, 94.00231170654297, 378.9613037109375, 'where\n', 5, 0)
(165.94223022460938, 370.1745300292969, 284.4503173828125, 407.3549499511719, 'bj =\n\x02\n1 if tj is a boundary\n0 otherwise\n', 6, 0)
(53.577972412109375, 405.61627197265625, 392.88262939453125, 419.25457763671875, 'Given a transcript T, the candidate segmentation CT is deﬁned similar to Ri.\n', 7, 0)
(183.78060913085938, 429.529296875, 399.3805847167969, 448.74713134765625, 'CT = {b1, b2, ..., bn}\n(2)\n', 8, 0)
(68.51766967773438, 447.45697021484375, 94.00200653076172, 459.60137939453125, 'where\n', 9, 0)
(165.94192504882812, 450.81463623046875, 284.4503173828125, 487.99505615234375, 'bj =\n\x02\n1 if tj is a boundary\n0 otherwise\n', 10, 0)
(53.577972412109375, 498.446044921875, 284.31683349609375, 510.361328125, '3.1\nGeneral Reference and Agreement Ratio\n', 11, 0)
(53.57769775390625, 518.1431274414062, 399.4162902832031, 562.759033203125, 'A General Reference (RG) is then constructed to calculate the agreement ratio\nbetween all references in. It is deﬁned by the boundary frequencies of each ref-\nerence Ri ∈ R.\n', 12, 0)
(181.9785919189453, 565.960205078125, 399.3813171386719, 585.1780395507812, 'RG = {d1, d2, ..., dn}\n(3)\n', 13, 0)
(68.51840209960938, 583.8968505859375, 94.00273895263672, 596.041259765625, 'where\n', 14, 0)

page suivante
(39.402000427246094, 31.01311492919922, 275.447021484375, 39.979515075683594, '124\nC.-E. Gonz´alez-Gallardo and J.-M. Torres-Moreno\n', 0, 0)
(134.82899475097656, 71.43724060058594, 154.71994018554688, 85.07554626464844, 'dj =\n', 1, 0)
(157.5, 63.413692474365234, 171.885986328125, 101.1213150024414, 'm\n\x03\n', 2, 0)
(158.23800659179688, 71.43724060058594, 385.2179870605469, 94.5884780883789, 'i=1\ntij\n∀tj ∈ T,\ndj = [0, m]\n(4)\n', 3, 0)
(39.40266418457031, 98.63511657714844, 385.2424011230469, 155.2121124267578, 'The Agreement Ratio (RGAR) is needed to get a numerical value of the dis-\ntribution of SU boundaries over R. A value of RGAR close to 0 means a low\nagreement between references in R, while RGAR = 1 means a perfect agreement\n(∀Ri ∈ R, Ri = Ri+1|i = 1, ..., m − 1) in R.\n', 4, 0)
(177.64920043945312, 156.44638061523438, 241.5670166015625, 175.64256286621094, 'RGAR = RGP B\n', 5, 0)
(217.18800354003906, 161.00526428222656, 385.21527099609375, 181.228271484375, 'RGHA\n(5)\n', 6, 0)
(39.40241241455078, 184.22508239746094, 385.25213623046875, 210.81468200683594, 'In the equation above, RGP B corresponds to the ponderated common boundaries\nof RG and RGHA to its hypothetical maximum agreement.\n', 7, 0)
(161.45956420898438, 225.5802764892578, 197.35293579101562, 240.21766662597656, 'RGP B =\n', 8, 0)
(200.12399291992188, 217.55667114257812, 214.50997924804688, 255.26431274414062, 'n\n\x03\n', 9, 0)
(200.42999267578125, 225.5802764892578, 385.2178955078125, 248.7313690185547, 'j=1\ndj [dj ≥ 2]\n(6)\n', 10, 0)
(147.65469360351562, 251.11444091796875, 227.39797973632812, 288.29486083984375, 'RGHA = m ×\n\x03\n', 11, 0)
(207.6840057373047, 258.6192932128906, 385.2197265625, 286.88409423828125, 'dj∈RG\n1 [dj ̸= 0]\n(7)\n', 12, 0)
(39.40289306640625, 299.63629150390625, 220.05178833007812, 311.55157470703125, '3.2\nWindow-Boundaries Reference\n', 13, 0)
(39.40289306640625, 319.3333740234375, 385.25946044921875, 368.8367004394531, 'In Sect. 2 we discussed about how disﬂuencies complicate SU segmentation. In a\nmulti-reference environment this causes disagreement between references around\na same SU boundary. The way WiSeBE handle disagreements produced by dis-\nﬂuencies is with a Window-boundaries Reference (RW ) deﬁned as:\n', 14, 0)
(164.11546325683594, 379.11126708984375, 385.2151794433594, 398.3291015625, 'RW = {w1, w2, ..., wp}\n(8)\n', 15, 0)
(39.40301513671875, 397.0389709472656, 385.23773193359375, 422.4462585449219, 'where each window wk considers one or more boundaries dj from RG with a\nwindow separation limit equal to RWl.\n', 16, 0)
(159.72349548339844, 432.91326904296875, 385.2148742675781, 452.131103515625, 'wk = {dj, dj+1, dj+2, ...}\n(9)\n', 17, 0)
(39.403076171875, 463.0222473144531, 112.16891479492188, 474.9375305175781, '3.3\nW iSeBE\n', 18, 0)
(39.402000427246094, 482.71929931640625, 383.6628723144531, 506.82470703125, 'WiSeBE is a normalized score dependent of (1) the performance of CT over RW\nand (2) the agreement between all references in R. It is deﬁned as:\n', 19, 0)
(109.04354858398438, 518.5842895507812, 385.2106628417969, 540.2950439453125, 'WiSeBE = F1RW × RGAR\nWiSeBE = [0, 1]\n(10)\n', 20, 0)
(39.402000427246094, 536.5209350585938, 385.22296142578125, 606.0491333007812, 'where F1RW corresponds to the harmonic mean of precision and recall of CT\nwith respect to RW (Eq. 11), while RGAR is the agreement ratio deﬁned in (5).\nRGAR can be interpreted as a scaling factor; a low value will penalize the overall\nWiSeBE score given the low agreement between references. By contrast, for a\nhigh agreement in R (RGAR ≈ 1), WiSeBE ≈ F1RW .\n', 21, 0)

page suivante
(139.08599853515625, 31.01311492919922, 399.35009765625, 39.979515075683594, 'WiSeBE: Window-Based Sentence Boundary Evaluation\n125\n', 0, 0)
(141.75900268554688, 63.83904266357422, 305.3870849609375, 87.86499786376953, 'F1RW = 2 × precisionRW × recallRW\n', 1, 0)
(200.4029998779297, 68.64714050292969, 399.381591796875, 90.11564636230469, 'precisionRW + recallRW\n(11)\n', 2, 0)
(105.67019653320312, 101.60514831542969, 171.4869384765625, 116.24266052246094, 'precisionRW =\n', 3, 0)
(175.45504760742188, 87.25101470947266, 185.97555541992188, 124.43143463134766, '\x04\n', 4, 0)
(185.9759979248047, 92.76713562011719, 343.337646484375, 113.47901153564453, 'bj∈CT 1\n[bj = 1, bj ∈ w\n∀w ∈ RW ]\n', 5, 0)
(217.54800415039062, 101.60478210449219, 399.381103515625, 140.10934448242188, '\x04\nbj∈CT 1\n[bj = 1]\n(12)\n', 6, 0)
(129.48330688476562, 133.65428161621094, 179.35293579101562, 148.2915496826172, 'recallRW =\n', 7, 0)
(183.321044921875, 120.20877838134766, 193.841552734375, 157.38919067382812, '\x04\n', 8, 0)
(193.8419952392578, 125.72514343261719, 319.5236511230469, 146.43714904785156, 'wk∈RW 1\n[wk ∋ b\n∀b ∈ CT ]\n', 9, 0)
(248.9219970703125, 133.65379333496094, 399.38116455078125, 152.6295623779297, 'p\n(13)\n', 10, 0)
(53.5775146484375, 154.02931213378906, 399.4211730957031, 215.4946746826172, 'Equations 12 and 13 describe precision and recall of CT with respect to RW .\nPrecision is the number of boundaries bj inside any window wk from RW divided\nby the total number of boundaries bj in CT . Recall corresponds to the number\nof windows w with at least one boundary b divided by the number of windows\nw in RW .\n', 11, 0)
(53.57699966430664, 228.32872009277344, 227.46751403808594, 240.2957763671875, '4\nEvaluating with W iSeBE\n', 12, 0)
(53.57600021362305, 248.5122528076172, 399.4674987792969, 368.2526550292969, 'To exemplify the WiSeBE score we evaluated and compared the performance\nof two diﬀerent SBD systems over a set of YouTube videos in a multi-reference\nenvironment. The ﬁrst system (S1) employs a Convolutional Neural Network to\ndetermine if the middle word of a sliding window corresponds to a SU bound-\nary or not [6]. The second approach (S2) by contrast, introduces a bidirectional\nRecurrent Neural Network model with attention mechanism for boundary detec-\ntion [28].\nIn a ﬁrst glance we performed the evaluation of the systems against each\none of the references independently. Then, we implemented a multi-reference\nevaluation with WiSeBE.\n', 13, 0)
(53.57499694824219, 383.1845703125, 118.24921417236328, 395.099853515625, '4.1\nDataset\n', 14, 0)
(53.57499313354492, 399.8301086425781, 399.47412109375, 603.2597045898438, 'We focused evaluation over a small but diversiﬁed dataset composed by 10\nYouTube videos in the English language in the news context. The selected videos\ncover diﬀerent topics like technology, human rights, terrorism and politics with\na length variation between 2 and 10 min. To encourage the diversity of content\nformat we included newscasts, interviews, reports and round tables.\nDuring the transcription phase we opted for a manual transcription process\nbecause we observed that using transcripts from an ASR system will diﬃcult\nin a large degree the manual segmentation process. The number of words per\ntranscript oscilate between 271 and 1,602 with a total number of 8,080.\nWe gave clear instructions to three evaluators (ref1, ref2, ref3) of how seg-\nmentation was needed to be perform, including the SU concept and how punctu-\nation marks were going to be taken into account. Periods (.), question marks (?),\nexclamation marks (!) and semicolons (;) were considered SU delimiters (bound-\naries) while colons (:) and commas (,) were considered as internal SU marks.\nThe number of segments per transcript and reference can be seen in Table 2. An\ninteresting remark is that ref3 assigns about 43% less boundaries than the mean\nof the other two references.\n', 15, 0)

page suivante
(39.402000427246094, 31.01311492919922, 275.447021484375, 39.979515075683594, '126\nC.-E. Gonz´alez-Gallardo and J.-M. Torres-Moreno\n', 0, 0)
(132.88568115234375, 53.17986297607422, 291.7101745605469, 62.146263122558594, 'Table 2. Manual dataset segmentation\n', 1, 0)
(97.29900360107422, 75.74321746826172, 327.33135986328125, 85.7086410522461, 'Reference v1\nv2\nv3\nv4\nv5\nv6\nv7\nv8\nv9\nv10 Total\n', 2, 0)
(97.29900360107422, 90.10710906982422, 319.89654541015625, 99.32545471191406, 'ref1\n38 42 17 11 55 87 109 72 55 16\n502\n', 3, 0)
(97.29900360107422, 103.87725067138672, 319.89654541015625, 113.09541320800781, 'ref2\n33 42 16 14 54 98\n92 65 51 20\n485\n', 4, 0)
(97.29900360107422, 117.64720916748047, 319.896728515625, 126.86531066894531, 'ref3\n23 20 10\n6 39 39\n76 30 29\n9\n281\n', 5, 0)
(39.402000427246094, 153.98941040039062, 118.84576416015625, 165.90467834472656, '4.2\nEvaluation\n', 6, 0)
(39.402000427246094, 172.84959411621094, 385.2457275390625, 220.8593292236328, 'We ran both systems (S1 & S2) over the manually transcribed videos obtaining\nthe number of boundaries shown in Table 3. In general, it can be seen that S1\npredicts 27% more segments than S2. This diﬀerence can aﬀect the performance\nof S1, increasing its probabilities of false positives.\n', 7, 0)
(126.61199951171875, 241.24423217773438, 297.9831848144531, 250.21063232421875, 'Table 3. Automatic dataset segmentation\n', 8, 0)
(99.91799926757812, 263.7982482910156, 324.72137451171875, 273.7635498046875, 'System v1\nv2\nv3\nv4\nv5\nv6\nv7\nv8\nv9\nv10 Total\n', 9, 0)
(99.91799926757812, 278.1622314453125, 317.2862243652344, 287.1286315917969, 'S1\n53 38 15 13 54 108 106 70 71 11\n539\n', 10, 0)
(99.91799926757812, 291.9322509765625, 317.28643798828125, 300.8986511230469, 'S2\n38 37 12 11 36\n92\n86 46 53 13\n424\n', 11, 0)
(39.401031494140625, 328.0992736816406, 385.2777404785156, 604.0064697265625, 'Table 4 condenses the performance of both systems evaluated against each\none of the references independently. If we focus on F1 scores, performance of both\nsystems varies depending of the reference. For ref1, S1 was better in 5 occasions\nwith respect of S2; S1 was better in 2 occasions only for ref2; S1 overperformed\nS2 in 3 occasions concerning ref3 and in 4 occasions for mean (bold).\nAlso from Table 4 we can observe that ref1 has a bigger similarity to S1 in\n5 occasions compared to other two references, while ref2 is more similar to S2\nin 7 transcripts (underline).\nAfter computing the mean F1 scores over the transcripts, it can be concluded\nthat in average S2 had a better performance segmenting the dataset compared\nto S1, obtaining a F1 score equal to 0.510. But... What about the complexity of\nthe dataset? Regardless all references have been considered, nor agreement or\ndisagreement between them has been taken into account.\nAll values related to the WiSeBE score are displayed in Table 5. The Agree-\nment Ratio (RGAR) between references oscillates between 0.525 for v8 and 0.767\nfor v5. The lower the RGAR, the bigger the penalization WiSeBE will give to\nthe ﬁnal score. A good example is S2 for transcript v4 where F1RW reaches a\nvalue of 0.800, but after considering RGAR the WiSeBE score falls to 0.462.\nIt is feasible to think that if all references are taken into account at the same\ntime during evaluation (F1RW ), the score will be bigger compared to an average\nof independent evaluations (F1mean); however this is not always true. That is\nthe case of S1 in v10, which present a slight decrease for F1RW compared to\nF1mean.\n', 12, 0)

page suivante
(139.08599853515625, 31.01311492919922, 399.35009765625, 39.979515075683594, 'WiSeBE: Window-Based Sentence Boundary Evaluation\n127\n', 0, 0)
(127.75515747070312, 53.17986297607422, 325.1728210449219, 62.146263122558594, 'Table 4. Independent multi-reference evaluation\n', 1, 0)
(57.66299819946289, 75.53668212890625, 347.0943298339844, 83.0113754272461, 'Transcript\nSystem ref1\nref2\nref3\nMean\n', 2, 0)
(129.63600158691406, 86.4715576171875, 383.8412780761719, 93.44535064697266, 'P\nR\nF1\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\n', 3, 0)
(57.66299819946289, 98.41461181640625, 392.86053466796875, 105.88936614990234, 'v1\nS1\n0.396 0.553 0.462 0.377 0.606 0.465 0.264 0.609\n0.368 0.346\n0.589\n0.432\n', 4, 0)
(102.29399871826172, 109.34269714355469, 395.32952880859375, 116.3234634399414, 'S2\n0.474 0.474 0.474 0.474 0.545 0.507 0.368 0.6087 0.459 0.439\n0.543\n0.480\n', 5, 0)
(57.66299819946289, 120.68272399902344, 395.32940673828125, 128.16433715820312, 'v2\nS1\n0.605 0.548 0.575 0.711 0.643 0.675 0.368 0.700\n0.483 0.561\n0.630\n0.578\n', 6, 0)
(102.29399871826172, 131.62469482421875, 392.86053466796875, 138.59849548339844, 'S2\n0.595 0.524 0.557 0.676 0.595 0.633 0.351 0.650\n0.456 0.541\n0.590\n0.549\n', 7, 0)
(57.66299819946289, 142.96466064453125, 392.86053466796875, 150.43923950195312, 'v3\nS1\n0.333 0.294 0.313 0.267 0.250 0.258 0.200 0.300\n0.240 0.267\n0.281\n0.270\n', 8, 0)
(102.29399871826172, 153.89271545410156, 395.32940673828125, 160.8734893798828, 'S2\n0.417 0.294 0.345 0.417 0.313 0.357 0.250 0.300\n0.273 0.361\n0.302\n0.325\n', 9, 0)
(57.66299819946289, 165.23959350585938, 392.86053466796875, 172.71435546875, 'v4\nS1\n0.615 0.571 0.593 0.462 0.545 0.500 0.308 0.667\n0.421 0.462\n0.595\n0.505\n', 10, 0)
(102.29399871826172, 176.1767120361328, 395.32940673828125, 183.15748596191406, 'S2\n0.909 0.714 0.800 0.818 0.818 0.818 0.455 0.833\n0.588 0.727\n0.789\n0.735\n', 11, 0)
(57.66299819946289, 187.50770568847656, 395.32940673828125, 194.9892578125, 'v5\nS1\n0.630 0.618 0.624 0.593 0.593 0.593 0.481 0.667\n0.560 0.568\n0.626\n0.592\n', 12, 0)
(102.29399871826172, 198.45867919921875, 392.86053466796875, 205.43247985839844, 'S2\n0.667 0.436 0.527 0.611 0.407 0.489 0.500 0.462\n0.480 0.593\n0.435\n0.499\n', 13, 0)
(57.66299819946289, 209.78269958496094, 392.8606262207031, 217.26437377929688, 'v6\nS1\n0.491 0.541 0.515 0.454 0.563 0.503 0.213 0.590\n0.313 0.386\n0.565\n0.443\n', 14, 0)
(102.29399871826172, 220.72669982910156, 395.32940673828125, 227.7074737548828, 'S2\n0.500 0.469 0.484 0.522 0.552 0.536 0.250 0.590\n0.351 0.4234 0.537\n0.457\n', 15, 0)
(57.66299819946289, 232.0666961669922, 392.8606262207031, 239.54824829101562, 'v7\nS1\n0.594 0.578 0.586 0.462 0.533 0.495 0.406 0.566\n0.473 0.487\n0.559\n0.518\n', 16, 0)
(102.29399871826172, 243.00169372558594, 395.3293151855469, 249.9824676513672, 'S2\n0.663 0.523 0.585 0.558 0.522 0.539 0.465 0.526\n0.494 0.562\n0.524\n0.539\n', 17, 0)
(57.66299819946289, 254.34869384765625, 392.86053466796875, 261.8233642578125, 'v8\nS1\n0.443 0.477 0.459 0.514 0.500 0.507 0.229 0.533\n0.320 0.395\n0.503\n0.429\n', 18, 0)
(102.29399871826172, 265.2765808105469, 395.32940673828125, 272.2573547363281, 'S2\n0.609 0.431 0.505 0.652 0.417 0.508 0.370 0.567\n0.447 0.543\n0.471\n0.487\n', 19, 0)
(57.66299819946289, 276.6166076660156, 395.3292236328125, 284.0983581542969, 'v9\nS1\n0.437 0.564 0.492 0.451 0.627 0.525 0.254 0.621\n0.360 0.380\n0.603\n0.459\n', 20, 0)
(102.29399871826172, 287.5516052246094, 392.8607177734375, 294.5323791503906, 'S2\n0.623 0.600 0.611 0.585 0.608 0.596 0.321 0.586\n0.414 0.509\n0.598\n0.541\n', 21, 0)
(57.66299819946289, 298.8916015625, 395.32940673828125, 306.37335205078125, 'v10\nS1\n0.818 0.450 0.581 0.818 0.450 0.581 0.455 0.556\n0.500 0.697\n0.523\n0.582\n', 22, 0)
(102.29399871826172, 309.83367919921875, 392.8606262207031, 316.8074645996094, 'S2\n0.692 0.450 0.545 0.615 0.500 0.552 0.308 0.444\n0.364 0.538\n0.4645 0.487\n', 23, 0)
(57.66299819946289, 321.1736755371094, 392.85748291015625, 328.1474609375, 'Mean scores S1\n—\n0.520\n—\n0.510\n—\n0.404\n—\n0.481\n', 24, 0)
(102.29399871826172, 332.1106872558594, 395.3260803222656, 339.0914611816406, 'S2\n—\n0.543\n—\n0.554\n—\n0.433\n—\n0.510\n', 25, 0)
(53.577789306640625, 362.6051940917969, 399.4053649902344, 398.6628112792969, 'An important remark is the behavior of S1 and S2 concerning v6. If evalu-\nated without considering any (dis)agreement between references (F1mean), S2\noverperforms S1; this is inverted once the systems are evaluated with WiSeBE.\n', 26, 0)
(53.57699966430664, 413.5577087402344, 135.0940399169922, 425.5247802734375, '5\nDiscussion\n', 27, 0)
(53.57699966430664, 434.2493896484375, 259.05780029296875, 448.6575927734375, '5.1\nRGA R and Fleiss’ Kappa correlation\n', 28, 0)
(53.576568603515625, 451.182861328125, 399.4092102050781, 547.018798828125, 'In Sect. 3 we described the WiSeBE score and how it relies on the RGAR value\nto scale the performance of CT over RW . RGAR can intuitively be consider an\nagreement value over all elements of R. To test this hypothesis, we computed\nthe Pearson correlation coeﬃcient (PCC) [21] between RGAR and the Fleiss’\nKappa [4] of each video in the dataset (κR).\nA linear correlation between RGAR and κR can be observed in Table 6. This\nis conﬁrmed by a PCC value equal to 0.890, which means a very strong positive\nlinear correlation between them.\n', 29, 0)
(53.57704162597656, 562.2293090820312, 185.75186157226562, 575.638671875, '5.2\nF 1mean vs. W iSeBE\n', 30, 0)
(53.577049255371094, 579.1627807617188, 399.4066467285156, 603.2597045898438, 'Results form Table 5 may give an idea that WiSeBE is just an scaled F1mean.\nWhile it is true that they show a linear correlation, WiSeBE may produce a\n', 31, 0)

page suivante
(39.402000427246094, 31.01311492919922, 275.447021484375, 39.979515075683594, '128\nC.-E. Gonz´alez-Gallardo and J.-M. Torres-Moreno\n', 0, 0)
(150.92160034179688, 53.17986297607422, 273.6698303222656, 62.146263122558594, 'Table 5. WiSeBE evaluation\n', 1, 0)
(98.01899719238281, 73.99724578857422, 326.1201477050781, 84.30733489990234, 'Transcript\nSystem F1mean F1RW\nRGAR\nWiSeBE\n', 2, 0)
(98.01899719238281, 88.37023162841797, 309.3184509277344, 97.58845520019531, 'v1\nS1\n0.432\n0.495\n0.691\n0.342\n', 3, 0)
(153.41400146484375, 101.73522186279297, 312.4964599609375, 110.70162200927734, 'S2\n0.480\n0.513\n0.354\n', 4, 0)
(98.01899719238281, 115.50524139404297, 312.4960632324219, 124.72340393066406, 'v2\nS1\n0.578\n0.659\n0.688\n0.453\n', 5, 0)
(153.41400146484375, 128.8701171875, 309.31884765625, 137.83651733398438, 'S2\n0.549\n0.595\n0.409\n', 6, 0)
(98.01899719238281, 142.6402587890625, 309.3184509277344, 151.85841369628906, 'v3\nS1\n0.270\n0.303\n0.684\n0.207\n', 7, 0)
(153.41400146484375, 156.01422119140625, 312.4964599609375, 164.98062133789062, 'S2\n0.325\n0.400\n0.274\n', 8, 0)
(98.01899719238281, 169.78414916992188, 309.3184509277344, 179.00242614746094, 'v4\nS1\n0.505\n0.593\n0.578\n0.342\n', 9, 0)
(153.41400146484375, 183.14913940429688, 312.4964599609375, 192.11553955078125, 'S2\n0.735\n0.800\n0.462\n', 10, 0)
(98.01899719238281, 196.91925048828125, 312.4960632324219, 206.13743591308594, 'v5\nS1\n0.592\n0.614\n0.767\n0.471\n', 11, 0)
(153.41400146484375, 210.29324340820312, 309.31884765625, 219.2596435546875, 'S2\n0.499\n0.500\n0.383\n', 12, 0)
(98.01899719238281, 224.05422973632812, 312.49615478515625, 233.2724151611328, 'v6\nS1\n0.443\n0.550\n0.541\n0.298\n', 13, 0)
(153.41400146484375, 237.42822265625, 309.3187561035156, 246.39462280273438, 'S2\n0.457\n0.535\n0.289\n', 14, 0)
(98.01899719238281, 251.19815063476562, 309.3184509277344, 260.41644287109375, 'v7\nS1\n0.518\n0.592\n0.617\n0.366\n', 15, 0)
(153.41400146484375, 264.563232421875, 312.4964599609375, 273.5296325683594, 'S2\n0.539\n0.606\n0.374\n', 16, 0)
(98.01899719238281, 278.3332214355469, 309.3184509277344, 287.55133056640625, 'v8\nS1\n0.429\n0.494\n0.525\n0.259\n', 17, 0)
(153.41400146484375, 291.7071228027344, 312.4964599609375, 300.67352294921875, 'S2\n0.487\n0.508\n0.267\n', 18, 0)
(98.01899719238281, 305.4682312011719, 309.3184509277344, 314.6864013671875, 'v9\nS1\n0.459\n0.569\n0.604\n0.344\n', 19, 0)
(153.41400146484375, 318.84222412109375, 312.4964599609375, 327.8086242675781, 'S2\n0.541\n0.667\n0.403\n', 20, 0)
(98.01899719238281, 332.61212158203125, 312.4960632324219, 341.8304443359375, 'v10\nS1\n0.582\n0.581\n0.619\n0.359\n', 21, 0)
(153.41400146484375, 345.9771423339844, 309.31884765625, 354.94354248046875, 'S2\n0.487\n0.545\n0.338\n', 22, 0)
(98.01899719238281, 359.7472229003906, 309.3176574707031, 368.713623046875, 'Mean scores S1\n0.481\n0.545\n0.631\n0.344\n', 23, 0)
(153.41400146484375, 373.1212463378906, 312.4964599609375, 382.087646484375, 'S2\n0.510\n0.567\n0.355\n', 24, 0)
(139.9409942626953, 398.95123291015625, 284.6650390625, 407.9176330566406, 'Table 6. Agreement within dataset\n', 25, 0)
(58.617000579833984, 421.2986755371094, 358.6465759277344, 428.7733459472656, 'Agreement metric\nv1\nv2\nv3\nv4\nv5\nv6\nv7\nv8\nv9\nv10\n', 26, 0)
(58.617000579833984, 433.232666015625, 366.0033264160156, 442.3994140625, 'RGAR\n0.691\n0.688\n0.684\n0.578\n0.767\n0.541\n0.617\n0.525\n0.604\n0.619\n', 27, 0)
(58.617000579833984, 444.57257080078125, 366.0033264160156, 452.0472717285156, 'κR\n0.776\n0.697\n0.757\n0.696\n0.839\n0.630\n0.743\n0.655\n0.704\n0.718\n', 28, 0)
(39.401153564453125, 479.20928955078125, 385.2627868652344, 551.1317138671875, 'diﬀerent system ranking than F1mean given the integral multi-reference principle\nit follows. However, what we consider the most proﬁtable about WiSeBE is the\ntwofold inclusion of all available references it performs. First, the construction of\nRW to provide a more inclusive reference against to whom be evaluated and then,\nthe computation of RGAR, which scales the result depending of the agreement\nbetween references.\n', 29, 0)

page suivante
(139.08599853515625, 31.01311492919922, 399.35009765625, 39.979515075683594, 'WiSeBE: Window-Based Sentence Boundary Evaluation\n129\n', 0, 0)
(53.57699966430664, 54.68272399902344, 143.38967895507812, 66.6497802734375, '6\nConclusions\n', 1, 0)
(53.575984954833984, 77.89915466308594, 399.4256591796875, 209.6007843017578, 'In this paper we presented WiSeBE, a semi-automatic multi-reference sentence\nboundary evaluation protocol based on the necessity of having a more reliable\nway for evaluating the SBD task. We showed how WiSeBE is an inclusive metric\nwhich not only evaluates the performance of a system against all references, but\nalso takes into account the agreement between them. According to your point\nof view, this inclusivity is very important given the diﬃculties that are present\nwhen working with spoken language and the possible disagreements that a task\nlike SBD could provoke.\nWiSeBE shows to be correlated with standard SBD metrics, however we\nwant to measure its correlation with extrinsic evaluations techniques like auto-\nmatic summarization and machine translation.\n', 2, 0)
(53.576995849609375, 223.29824829101562, 399.3741760253906, 276.09600830078125, 'Acknowledgments. We would like to acknowledge the support of CHIST-ERA for\nfunding this work through the Access Multilingual Information opinionS (AMIS),\n(France - Europe) project.\nWe also like to acknowledge the support given by the Prof. Hanifa Boucheneb from\nVERIFORM Laboratory (´Ecole Polytechnique de Montr´eal).\n', 3, 0)
(53.57699966430664, 296.773681640625, 116.49549102783203, 308.7407531738281, 'References\n', 4, 0)
(58.185001373291016, 321.9292297363281, 399.36273193359375, 407.60498046875, '1. Bohac, M., Blavka, K., Kucharova, M., Skodova, S.: Post-processing of the recog-\nnized speech for web presentation of large audio archive. In: 2012 35th International\nConference on Telecommunications and Signal Processing (TSP), pp. 441–445.\nIEEE (2012)\n2. Brum, H., Araujo, F., Kepler, F.: Sentiment analysis for Brazilian portuguese over\na skewed class corpora. In: Silva, J., Ribeiro, R., Quaresma, P., Adami, A., Branco,\nA. (eds.) PROPOR 2016. LNCS (LNAI), vol. 9727, pp. 134–138. Springer, Cham\n(2016). https://doi.org/10.1007/978-3-319-41552-9 14\n', 5, 0)
(58.1842041015625, 409.59857177734375, 399.3404846191406, 593.9082641601562, '3. Che, X., Wang, C., Yang, H., Meinel, C.: Punctuation prediction for unsegmented\ntranscript based on word vector. In: LREC (2016)\n4. Fleiss, J.L.: Measuring nominal scale agreement among many raters. Psychol. Bull.\n76(5), 378 (1971)\n5. Fohr, D., Mella, O., Illina, I.: New paradigm in speech recognition: deep neural net-\nworks. In: IEEE International Conference on Information Systems and Economic\nIntelligence (2017)\n6. Gonz´alez-Gallardo, C.E., Hajjem, M., SanJuan, E., Torres-Moreno, J.M.: Tran-\nscripts informativeness study: an approach based on automatic summarization. In:\nConf´erence en Recherche d’Information et Applications (CORIA), Rennes, France,\nMay (2018)\n7. Gonz´alez-Gallardo, C.E., Torres-Moreno, J.M.: Sentence boundary detection for\nFrench with subword-level information vectors and convolutional neural networks.\narXiv preprint arXiv:1802.04559 (2018)\n8. Gotoh, Y., Renals, S.: Sentence boundary detection in broadcast speech transcripts.\nIn: ASR2000-Automatic Speech Recognition: Challenges for the new Millenium\nISCA Tutorial and Research Workshop (ITRW) (2000)\n', 6, 0)

page suivante
(39.402000427246094, 31.01311492919922, 275.447021484375, 39.979515075683594, '130\nC.-E. Gonz´alez-Gallardo and J.-M. Torres-Moreno\n', 0, 0)
(39.40196990966797, 56.92424774169922, 385.2162170410156, 537.1361083984375, '9. Hinton, G., et al.: Deep neural networks for acoustic modeling in speech recogni-\ntion: the shared views of four research groups. IEEE Signal Process. Mag. 29(6),\n82–97 (2012)\n10. Jamil, N., Ramli, M.I., Seman, N.: Sentence boundary detection without speech\nrecognition: a case of an under-resourced language. J. Electr. Syst. 11(3), 308–318\n(2015)\n11. Kiss, T., Strunk, J.: Unsupervised multilingual sentence boundary detection. Com-\nput. Linguist. 32(4), 485–525 (2006)\n12. Klejch, O., Bell, P., Renals, S.: Punctuated transcription of multi-genre broadcasts\nusing acoustic and lexical approaches. In: 2016 IEEE Spoken Language Technology\nWorkshop (SLT), pp. 433–440. IEEE (2016)\n13. Kol´aˇr, J., Lamel, L.: Development and evaluation of automatic punctuation for\nFrench and english speech-to-text. In: Thirteenth Annual Conference of the Inter-\nnational Speech Communication Association (2012)\n14. Kol´aˇr, J., ˇSvec, J., Psutka, J.: Automatic punctuation annotation in Czech broad-\ncast news speech. In: SPECOM 2004 (2004)\n15. Liu, Y., Chawla, N.V., Harper, M.P., Shriberg, E., Stolcke, A.: A study in machine\nlearning from imbalanced data for sentence boundary detection in speech. Comput.\nSpeech Lang. 20(4), 468–494 (2006)\n16. Lu, W., Ng, H.T.: Better punctuation prediction with dynamic conditional ran-\ndom ﬁelds. In: Proceedings of the 2010 Conference on Empirical Methods in Natu-\nral Language Processing. pp. 177–186. Association for Computational Linguistics\n(2010)\n17. Meteer, M., Iyer, R.: Modeling conversational speech for speech recognition. In:\nConference on Empirical Methods in Natural Language Processing (1996)\n18. Mrozinski, J., Whittaker, E.W., Chatain, P., Furui, S.: Automatic sentence seg-\nmentation of speech for automatic summarization. In: 2006 IEEE International\nConference on Acoustics Speech and Signal Processing Proceedings, vol. 1, p. I.\nIEEE (2006)\n19. Palmer, D.D., Hearst, M.A.: Adaptive sentence boundary disambiguation. In: Pro-\nceedings of the Fourth Conference on Applied Natural Language Processing, pp.\n78–83. ANLC 1994. Association for Computational Linguistics, Stroudsburg, PA,\nUSA (1994)\n20. Palmer, D.D., Hearst, M.A.: Adaptive multilingual sentence boundary disambigua-\ntion. Comput. Linguist. 23(2), 241–267 (1997)\n21. Pearson, K.: Note on regression and inheritance in the case of two parents. Proc.\nR. Soc. Lond. 58, 240–242 (1895)\n22. Peitz, S., Freitag, M., Ney, H.: Better punctuation prediction with hierarchical\nphrase-based translation. In: Proceedings of the International Workshop on Spoken\nLanguage Translation (IWSLT), South Lake Tahoe, CA, USA (2014)\n23. Rott, M., ˇCerva, P.: Speech-to-text summarization using automatic phrase extrac-\ntion from recognized text. In: Sojka, P., Hor´ak, A., Kopeˇcek, I., Pala, K. (eds.) TSD\n2016. LNCS (LNAI), vol. 9924, pp. 101–108. Springer, Cham (2016). https://doi.\norg/10.1007/978-3-319-45510-5 12\n', 1, 0)
(39.40185546875, 539.1084594726562, 385.2017517089844, 602.8775024414062, '24. Shriberg, E., Stolcke, A.: Word predictability after hesitations: a corpus-based\nstudy. In: Proceedings of the Fourth International Conference on Spoken Language,\n1996. ICSLP 1996, vol. 3, pp. 1868–1871. IEEE (1996)\n25. Stevenson, M., Gaizauskas, R.: Experiments on sentence boundary detection. In:\nProceedings of the sixth conference on Applied natural language processing, pp.\n84–89. Association for Computational Linguistics (2000)\n', 2, 0)

page suivante
(139.08599853515625, 31.01311492919922, 399.35009765625, 39.979515075683594, 'WiSeBE: Window-Based Sentence Boundary Evaluation\n131\n', 0, 0)
(53.5770263671875, 56.92424774169922, 399.3680725097656, 120.6826400756836, '26. Stolcke, A., Shriberg, E.: Automatic linguistic segmentation of conversational\nspeech. In: Proceedings of the Fourth International Conference on Spoken Lan-\nguage, 1996. ICSLP 1996, vol. 2, pp. 1005–1008. IEEE (1996)\n27. Strassel, S.: Simple metadata annotation speciﬁcation v5. 0, linguistic data consor-\ntium (2003). http://www.ldc.upenn.edu/projects/MDE/Guidelines/SimpleMDE\nV5.0.pdf\n', 1, 0)
(53.575904846191406, 122.67858123779297, 399.4053649902344, 285.0726623535156, '28. Tilk, O., Alum¨ae, T.: Bidirectional recurrent neural network with attention mech-\nanism for punctuation restoration. In: Interspeech 2016 (2016)\n29. Treviso, M.V., Shulby, C.D., Aluisio, S.M.: Evaluating word embeddings for sen-\ntence boundary detection in speech transcripts. arXiv preprint arXiv:1708.04704\n(2017)\n30. Ueﬃng, N., Bisani, M., Vozila, P.: Improved models for automatic punctuation\nprediction for spoken and written text. In: Interspeech, pp. 3097–3101 (2013)\n31. Wang, W., Tur, G., Zheng, J., Ayan, N.F.: Automatic disﬂuency removal for\nimproving spoken language translation. In: 2010 IEEE International Conference on\nAcoustics Speech and Signal Processing (ICASSP), pp. 5214–5217. IEEE (2010)\n32. Xu, C., Xie, L., Huang, G., Xiao, X., Chng, E.S., Li, H.: A deep neural network\napproach for sentence boundary detection in broadcast news. In: Fifteenth Annual\nConference of the International Speech Communication Association (2014)\n33. Yu, D., Deng, L.: Automatic Speech Recognition. Springer, London (2015). https://\ndoi.org/10.1007/978-1-4471-5779-3\n', 2, 0)

page suivante
