(105.21900177001953, 52.88939666748047, 347.76580810546875, 85.17841339111328, 'WiSeBE: Window-Based Sentence\nBoundary Evaluation\n', 0, 0)
(58.91400146484375, 108.5795669555664, 393.5630798339844, 124.43266296386719, 'Carlos-Emiliano Gonz´alez-Gallardo1,2(B) and Juan-Manuel Torres-Moreno1,2\n', 1, 0)
(57.582000732421875, 132.29022216796875, 395.30450439453125, 201.8448486328125, '1 LIA - Universit´e d’Avignon et des Pays de Vaucluse, 339 chemin des Meinajaries,\n84140 Avignon, France\ncarlos-emiliano.gonzalez-gallardo@alumni.univ-avignon.fr,\njuan-manuel.torres@univ-avignon.fr\n2 D´epartement de GIGL, ´Ecole Polytechnique de Montr´eal,\nC.P. 6079, succ. Centre-ville, Montr´eal, Qu´ebec H3C 3A7, Canada\n', 2, 0)
(81.91744995117188, 228.5458984375, 371.0433044433594, 390.93994140625, 'Abstract. Sentence Boundary Detection (SBD) has been a major\nresearch topic since Automatic Speech Recognition transcripts have been\nused for further Natural Language Processing tasks like Part of Speech\nTagging, Question Answering or Automatic Summarization. But what\nabout evaluation? Do standard evaluation metrics like precision, recall,\nF-score or classiﬁcation error; and more important, evaluating an auto-\nmatic system against a unique reference is enough to conclude how well\na SBD system is performing given the ﬁnal application of the transcript?\nIn this paper we propose Window-based Sentence Boundary Evaluation\n(WiSeBE), a semi-supervised metric for evaluating Sentence Boundary\nDetection systems based on multi-reference (dis)agreement. We evalu-\nate and compare the performance of diﬀerent SBD systems over a set\nof Youtube transcripts using WiSeBE and standard metrics. This dou-\nble evaluation gives an understanding of how WiSeBE is a more reliable\nmetric for the SBD task.\n', 3, 0)
(81.91744995117188, 407.46282958984375, 307.7750244140625, 439.1672668457031, 'Keywords: Sentence Boundary Detection · Evaluation\nTranscripts · Human judgment\n', 4, 0)
(53.57699966430664, 450.2146911621094, 147.771240234375, 462.1817626953125, '1\nIntroduction\n', 5, 0)
(53.57491683959961, 471.56829833984375, 399.46258544921875, 603.2609252929688, 'The goal of Automatic Speech Recognition (ASR) is to transform spoken data\ninto a written representation, thus enabling natural human-machine interaction\n[33] with further Natural Language Processing (NLP) tasks. Machine transla-\ntion, question answering, semantic parsing, POS tagging, sentiment analysis and\nautomatic text summarization; originally developed to work with formal writ-\nten texts, can be applied over the transcripts made by ASR systems [2,25,31].\nHowever, before applying any of these NLP tasks a segmentation process called\nSentence Boundary Detection (SBD) should be performed over ASR transcripts\nto reach a minimal syntactic information in the text.\nTo measure the performance of a SBD system, the automatically segmented\ntranscript is evaluated against a single reference normally done by a human. But\n', 6, 0)
(53.57694625854492, 608.1136474609375, 304.4107360839844, 632.2485961914062, 'c\n⃝ Springer Nature Switzerland AG 2018\nI. Batyrshin et al. (Eds.): MICAI 2018, LNAI 11289, pp. 119–131, 2018.\nhttps://doi.org/10.1007/978-3-030-04497-8_10\n', 7, 0)
